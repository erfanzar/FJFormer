
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Erfan Zare Chavoshi">
      
      
      
        <link rel="prev" href="../generated-partition_utils-mesh_utils/">
      
      
        <link rel="next" href="../generated-utils/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.22">
    
    
      
        <title>T5x Partitioning - FJFormer</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.732c4fb1.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#partition_utilst5x_partitioning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="FJFormer" class="md-header__button md-logo" aria-label="FJFormer" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            FJFormer
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              T5x Partitioning
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/erfanzar/FJFormer" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="FJFormer" class="md-nav__button md-logo" aria-label="FJFormer" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    FJFormer
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/erfanzar/FJFormer" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Bits
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Bits
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-bits-bits/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bits
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-bits-calibration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Calibration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-bits-config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-bits-int_numerics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Int Numerics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-bits-no_numerics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    No Numerics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-bits-numerics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Numerics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-bits-q_dot_general/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Q Dot General
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-bits-q_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Q Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-bits-qk/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Qk
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-bits-stochastic_rounding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Stochastic Rounding
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Checkpoint
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Checkpoint
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-checkpoint-_load/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Load
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-checkpoint-streamer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Streamer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Func
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Func
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-func-_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Func
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-func-loss_func/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Loss Func
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Monitor
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Monitor
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-monitor-tracker/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tracker
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Optimizers
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Optimizers
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-optimizers-adafactor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adafactor
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-optimizers-adamw/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adamw
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-optimizers-lion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lion
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-optimizers-optimizer_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizer Utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-optimizers-rmsprop/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rmsprop
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Pallas Operations
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Pallas Operations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_1" >
        
          
          <label class="md-nav__link" for="__nav_7_1" id="__nav_7_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Efficient Attention
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_1">
            <span class="md-nav__icon md-icon"></span>
            Efficient Attention
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-pallas_operations-efficient_attention-efficient_attention/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Efficient Attention
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2" >
        
          
          <label class="md-nav__link" for="__nav_7_2" id="__nav_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Flash Attention
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_2">
            <span class="md-nav__icon md-icon"></span>
            Flash Attention
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_1" >
        
          
          <label class="md-nav__link" for="__nav_7_2_1" id="__nav_7_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gpu
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_2_1">
            <span class="md-nav__icon md-icon"></span>
            Gpu
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-pallas_operations-flash_attention-gpu-jax_flash_attn_gpu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Jax Flash Attn Gpu
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2_2" >
        
          
          <label class="md-nav__link" for="__nav_7_2_2" id="__nav_7_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tpu
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_2_2">
            <span class="md-nav__icon md-icon"></span>
            Tpu
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-pallas_operations-flash_attention-tpu-jax_flash_attn_tpu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Jax Flash Attn Tpu
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3" >
        
          
          <label class="md-nav__link" for="__nav_7_3" id="__nav_7_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Layer Norm
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_3">
            <span class="md-nav__icon md-icon"></span>
            Layer Norm
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_3_1" >
        
          
          <label class="md-nav__link" for="__nav_7_3_1" id="__nav_7_3_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gpu
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_3_1">
            <span class="md-nav__icon md-icon"></span>
            Gpu
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-pallas_operations-layer_norm-gpu-layer_norm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Layer Norm
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_4" >
        
          
          <label class="md-nav__link" for="__nav_7_4" id="__nav_7_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Ring Attention
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_4">
            <span class="md-nav__icon md-icon"></span>
            Ring Attention
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-pallas_operations-ring_attention-ring_attention/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ring Attention
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_5" >
        
          
          <label class="md-nav__link" for="__nav_7_5" id="__nav_7_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Rms Norm
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_5">
            <span class="md-nav__icon md-icon"></span>
            Rms Norm
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_5_1" >
        
          
          <label class="md-nav__link" for="__nav_7_5_1" id="__nav_7_5_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gpu
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_5_1">
            <span class="md-nav__icon md-icon"></span>
            Gpu
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-pallas_operations-rms_norm-gpu-rms_norm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rms Norm
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_6" >
        
          
          <label class="md-nav__link" for="__nav_7_6" id="__nav_7_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Softmax
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_6">
            <span class="md-nav__icon md-icon"></span>
            Softmax
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_6_1" >
        
          
          <label class="md-nav__link" for="__nav_7_6_1" id="__nav_7_6_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gpu
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_6_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_6_1">
            <span class="md-nav__icon md-icon"></span>
            Gpu
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-pallas_operations-softmax-gpu-softmax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Softmax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_7" >
        
          
          <label class="md-nav__link" for="__nav_7_7" id="__nav_7_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Splash Attention
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_7">
            <span class="md-nav__icon md-icon"></span>
            Splash Attention
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_7_1" >
        
          
          <label class="md-nav__link" for="__nav_7_7_1" id="__nav_7_7_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tpu
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_7_7_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_7_1">
            <span class="md-nav__icon md-icon"></span>
            Tpu
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-pallas_operations-splash_attention-tpu-splash_attention_kernel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Splash Attention Kernel
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-pallas_operations-splash_attention-tpu-splash_attention_mask/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Splash Attention Mask
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-pallas_operations-splash_attention-tpu-splash_attention_mask_info/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Splash Attention Mask Info
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" checked>
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Partition Utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Partition Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-partition_utils-mesh_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mesh Utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    T5x Partitioning
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    T5x Partitioning
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning" class="md-nav__link">
    <span class="md-ellipsis">
      t5x_partitioning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.AxisNames" class="md-nav__link">
    <span class="md-ellipsis">
      AxisNames
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner" class="md-nav__link">
    <span class="md-ellipsis">
      BasePartitioner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BasePartitioner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_mesh_size" class="md-nav__link">
    <span class="md-ellipsis">
      data_mesh_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_shard_id" class="md-nav__link">
    <span class="md-ellipsis">
      data_shard_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_shards" class="md-nav__link">
    <span class="md-ellipsis">
      data_shards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_data_layout" class="md-nav__link">
    <span class="md-ellipsis">
      get_data_layout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_local_chunk_info" class="md-nav__link">
    <span class="md-ellipsis">
      get_local_chunk_info
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_logical_axes" class="md-nav__link">
    <span class="md-ellipsis">
      get_logical_axes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_mesh_axes" class="md-nav__link">
    <span class="md-ellipsis">
      get_mesh_axes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.move_params_to_devices" class="md-nav__link">
    <span class="md-ellipsis">
      move_params_to_devices
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePjitPartitioner" class="md-nav__link">
    <span class="md-ellipsis">
      BasePjitPartitioner
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.DataLayout" class="md-nav__link">
    <span class="md-ellipsis">
      DataLayout
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.LocalChunker" class="md-nav__link">
    <span class="md-ellipsis">
      LocalChunker
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LocalChunker">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_local_chunk_info" class="md-nav__link">
    <span class="md-ellipsis">
      get_local_chunk_info
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_replica_id" class="md-nav__link">
    <span class="md-ellipsis">
      get_replica_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_shard_id" class="md-nav__link">
    <span class="md-ellipsis">
      get_shard_id
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner" class="md-nav__link">
    <span class="md-ellipsis">
      PjitPartitioner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PjitPartitioner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.logical_axis_rules" class="md-nav__link">
    <span class="md-ellipsis">
      logical_axis_rules
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.get_logical_axes" class="md-nav__link">
    <span class="md-ellipsis">
      get_logical_axes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.get_mesh_axes" class="md-nav__link">
    <span class="md-ellipsis">
      get_mesh_axes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.partition" class="md-nav__link">
    <span class="md-ellipsis">
      partition
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjittedFnWithContext" class="md-nav__link">
    <span class="md-ellipsis">
      PjittedFnWithContext
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.bounds_from_last_device" class="md-nav__link">
    <span class="md-ellipsis">
      bounds_from_last_device
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.default_mesh" class="md-nav__link">
    <span class="md-ellipsis">
      default_mesh
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.get_coords" class="md-nav__link">
    <span class="md-ellipsis">
      get_coords
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.get_cpu_mesh" class="md-nav__link">
    <span class="md-ellipsis">
      get_cpu_mesh
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.get_gpu_mesh" class="md-nav__link">
    <span class="md-ellipsis">
      get_gpu_mesh
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.get_mesh" class="md-nav__link">
    <span class="md-ellipsis">
      get_mesh
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.global_mesh_defined" class="md-nav__link">
    <span class="md-ellipsis">
      global_mesh_defined
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.standard_logical_axis_rules" class="md-nav__link">
    <span class="md-ellipsis">
      standard_logical_axis_rules
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.with_sharding_constraint" class="md-nav__link">
    <span class="md-ellipsis">
      with_sharding_constraint
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Xrapture
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            Xrapture
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-xrapture-implicit_array/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Implicit Array
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-xrapture-tracer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tracer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-xrapture-xrapture/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Xrapture
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning" class="md-nav__link">
    <span class="md-ellipsis">
      t5x_partitioning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.AxisNames" class="md-nav__link">
    <span class="md-ellipsis">
      AxisNames
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner" class="md-nav__link">
    <span class="md-ellipsis">
      BasePartitioner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BasePartitioner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_mesh_size" class="md-nav__link">
    <span class="md-ellipsis">
      data_mesh_size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_shard_id" class="md-nav__link">
    <span class="md-ellipsis">
      data_shard_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_shards" class="md-nav__link">
    <span class="md-ellipsis">
      data_shards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_data_layout" class="md-nav__link">
    <span class="md-ellipsis">
      get_data_layout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_local_chunk_info" class="md-nav__link">
    <span class="md-ellipsis">
      get_local_chunk_info
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_logical_axes" class="md-nav__link">
    <span class="md-ellipsis">
      get_logical_axes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_mesh_axes" class="md-nav__link">
    <span class="md-ellipsis">
      get_mesh_axes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.move_params_to_devices" class="md-nav__link">
    <span class="md-ellipsis">
      move_params_to_devices
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.BasePjitPartitioner" class="md-nav__link">
    <span class="md-ellipsis">
      BasePjitPartitioner
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.DataLayout" class="md-nav__link">
    <span class="md-ellipsis">
      DataLayout
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.LocalChunker" class="md-nav__link">
    <span class="md-ellipsis">
      LocalChunker
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LocalChunker">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_local_chunk_info" class="md-nav__link">
    <span class="md-ellipsis">
      get_local_chunk_info
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_replica_id" class="md-nav__link">
    <span class="md-ellipsis">
      get_replica_id
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_shard_id" class="md-nav__link">
    <span class="md-ellipsis">
      get_shard_id
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner" class="md-nav__link">
    <span class="md-ellipsis">
      PjitPartitioner
    </span>
  </a>
  
    <nav class="md-nav" aria-label="PjitPartitioner">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.logical_axis_rules" class="md-nav__link">
    <span class="md-ellipsis">
      logical_axis_rules
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.get_logical_axes" class="md-nav__link">
    <span class="md-ellipsis">
      get_logical_axes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.get_mesh_axes" class="md-nav__link">
    <span class="md-ellipsis">
      get_mesh_axes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.partition" class="md-nav__link">
    <span class="md-ellipsis">
      partition
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.PjittedFnWithContext" class="md-nav__link">
    <span class="md-ellipsis">
      PjittedFnWithContext
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.bounds_from_last_device" class="md-nav__link">
    <span class="md-ellipsis">
      bounds_from_last_device
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.default_mesh" class="md-nav__link">
    <span class="md-ellipsis">
      default_mesh
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.get_coords" class="md-nav__link">
    <span class="md-ellipsis">
      get_coords
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.get_cpu_mesh" class="md-nav__link">
    <span class="md-ellipsis">
      get_cpu_mesh
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.get_gpu_mesh" class="md-nav__link">
    <span class="md-ellipsis">
      get_gpu_mesh
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.get_mesh" class="md-nav__link">
    <span class="md-ellipsis">
      get_mesh
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.global_mesh_defined" class="md-nav__link">
    <span class="md-ellipsis">
      global_mesh_defined
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.standard_logical_axis_rules" class="md-nav__link">
    <span class="md-ellipsis">
      standard_logical_axis_rules
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.fjformer.partition_utils.t5x_partitioning.with_sharding_constraint" class="md-nav__link">
    <span class="md-ellipsis">
      with_sharding_constraint
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="partition_utilst5x_partitioning">partition_utils.t5x_partitioning</h1>


<div class="doc doc-object doc-module">



<a id="src.fjformer.partition_utils.t5x_partitioning"></a>
  <div class="doc doc-contents first">
  
      <p>Utilities for partitioning.</p>

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.AxisNames" class="doc doc-heading">
          <code>AxisNames</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code>tuple</code></p>

  
      <p>Tuple of strings specifying name for each axis.</p>
<p>We create a separate class for this so JAX's pytree utilities can distinguish
it from a tuple that should be treated as a pytree, instead treating it as a
leaf.</p>

            <details class="quote">
              <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AxisNames</span><span class="p">(</span><span class="nb">tuple</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tuple of strings specifying name for each axis.</span>

<span class="sd">    We create a separate class for this so JAX&#39;s pytree utilities can distinguish</span>
<span class="sd">    it from a tuple that should be treated as a pytree, instead treating it as a</span>
<span class="sd">    leaf.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="n">AxisNames</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;AxisNames</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.BasePartitioner" class="doc doc-heading">
          <code>BasePartitioner</code>


</h2>


  <div class="doc doc-contents ">

  
      <p>Interface for partitioning computations across hardware devices.</p>

            <details class="quote">
              <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">BasePartitioner</span><span class="p">(</span><span class="n">metaclass</span><span class="o">=</span><span class="n">abc</span><span class="o">.</span><span class="n">ABCMeta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Interface for partitioning computations across hardware devices.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_partitions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">model_parallel_submesh</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">params_on_devices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">ici_mesh_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">dcn_mesh_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Configures the partitioner.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_partitions: the number of partitions to use. Ignored if</span>
<span class="sd">            `model_parallel_submesh` is provided.</span>
<span class="sd">          model_parallel_submesh: 4-tuple that specifies the x,y,z,c submesh to use</span>
<span class="sd">            as the model-parallel device tile. This submesh is used for the larger</span>
<span class="sd">            of the two parameter dimensions, and, if 2-D activation sharding is</span>
<span class="sd">            enabled, for the model dimension of activations. The rest of the mesh is</span>
<span class="sd">            used for data parallelism and, if 2-D parameter sharding is enabled, the</span>
<span class="sd">            other parameter dimension.</span>
<span class="sd">          params_on_devices: whether to keep the params on devices, if False -</span>
<span class="sd">            params stay in the host memory. Note that some partitioners might ignore</span>
<span class="sd">            this setting, for example if they don&#39;t support storing all params on</span>
<span class="sd">            device memory.</span>
<span class="sd">          backend: get devices from the pinned backend, if specified. This is useful</span>
<span class="sd">            for explicitly specifying the devices other than relying on</span>
<span class="sd">            jax_platform_name.</span>
<span class="sd">          ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in</span>
<span class="sd">            each slice. The meaning of each mesh axis is defined by mesh_axis_names,</span>
<span class="sd">            so these two params must be the same length. If dcn_mesh_shape is</span>
<span class="sd">            present, the overall mesh is the product of ici_mesh_shape and</span>
<span class="sd">            dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with</span>
<span class="sd">            mesh_axis_names [&#39;replica&#39;, &#39;data&#39;, &#39;mdl&#39;] indicates 2-way replica</span>
<span class="sd">            parallelism, 3-way data parallelism, and 4-way model parallelism over 24</span>
<span class="sd">            devices. None, the default, is equivalent to a sequence of ones and</span>
<span class="sd">            means that the model is placed on a single device.</span>
<span class="sd">          dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over</span>
<span class="sd">            multiple slices. The overall mesh is the product of ici_mesh_shape and</span>
<span class="sd">            dcn_mesh_shape, and the meaning of each mesh axis is defined by</span>
<span class="sd">            mesh_axis_names, so these three params must be the same length.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">num_partitions</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">model_parallel_submesh</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;At least one of `num_partitions` or &#39;</span>
                             <span class="s1">&#39;`model_parallel_submesh` must be set.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">model_parallel_submesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_parallel_submesh</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="s1">&#39;`model_parallel_submesh` must be either None or a 4-tuple. Got&#39;</span>
                    <span class="s1">&#39; `model_parallel_submesh`=</span><span class="si">%r</span><span class="s1">. A ValueError will be raised&#39;</span>
                    <span class="s1">&#39; beginning March 1, 2022.&#39;</span>
                <span class="p">),</span>
                <span class="n">model_parallel_submesh</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">bool</span><span class="p">(</span><span class="n">num_partitions</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">bool</span><span class="p">(</span><span class="n">model_parallel_submesh</span><span class="p">):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                <span class="s1">&#39;At most one of `num_partitions` or `model_parallel_submesh` can be &#39;</span>
                <span class="s1">&#39;set. Got `num_partitions=</span><span class="si">%r</span><span class="s1">` and `model_parallel_submesh`=</span><span class="si">%r</span><span class="s1">. A &#39;</span>
                <span class="s1">&#39;ValueError will be raised beginning March 21, 2022.&#39;</span><span class="p">,</span>
                <span class="n">num_partitions</span><span class="p">,</span>
                <span class="n">model_parallel_submesh</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span> <span class="o">=</span> <span class="n">num_partitions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_parallel_submesh</span> <span class="o">=</span> <span class="n">model_parallel_submesh</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_params_on_devices</span> <span class="o">=</span> <span class="n">params_on_devices</span>
        <span class="k">if</span> <span class="n">ici_mesh_shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">dcn_mesh_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span> <span class="o">=</span> <span class="s1">&#39;data&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;replica&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span> <span class="o">=</span> <span class="n">backend</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ici_mesh_shape</span> <span class="o">=</span> <span class="n">ici_mesh_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dcn_mesh_shape</span> <span class="o">=</span> <span class="n">dcn_mesh_shape</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mesh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mesh</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">data_partition_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PartitionSpec</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">data_mesh_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Data mesh size.</span>

<span class="sd">        Data mesh size is defined as the number of global devices involved to</span>
<span class="sd">        carry out data parallel. Let&#39;s say we have a global mesh: (&#39;replica&#39;: 2,</span>
<span class="sd">        &#39;data&#39;: 4, &#39;model&#39;: 2), and axes &#39;replica&#39; and &#39;data&#39; are responsible for</span>
<span class="sd">        the data parallel, that means we have 2*4 = 8 devices involved - i.e., data</span>
<span class="sd">        mesh size is 8.</span>

<span class="sd">        Returns:</span>
<span class="sd">          the id of the shard for the axes being replicated among the devices used</span>
<span class="sd">          to shard the sharded_mesh_axes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data_submesh_sizes</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">]]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="k">else</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">data_mesh_size</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">data_submesh_sizes</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data_mesh_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">data_shards</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of data shards.</span>

<span class="sd">        Let&#39;s say we are dealing with 2 slices of df4x2 TPUs. In data pipeline</span>
<span class="sd">        we need prepare / send one data shard to each local host. This means, we</span>
<span class="sd">        need 4 shards since we have 4 local hosts. How to infer the number of hosts</span>
<span class="sd">        from mesh information? In this case, we have a global mesh: (&#39;replica&#39;: 2,</span>
<span class="sd">        &#39;data&#39;: 8, &#39;model&#39;: 2). Each local host (i.e., df2x2) has this local mesh:</span>
<span class="sd">        (&#39;replica&#39;: 1, &#39;data&#39;: 4, &#39;model&#39;: 2). By dividing global mesh with local</span>
<span class="sd">        mesh, we can get the count of hosts.</span>

<span class="sd">        Returns:</span>
<span class="sd">          Number of data shards. Each shard will be sent to one local host.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data_chunks</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_chunker</span><span class="o">.</span><span class="n">num_chunks</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">]]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="k">else</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_chunker</span><span class="o">.</span><span class="n">num_chunks</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">data_shards</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">data_chunks</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data_shards</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">data_shard_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Data shard id for the current host.</span>

<span class="sd">        Returns:</span>
<span class="sd">          Index of data shard that will be sent to the current local host.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_chunker</span><span class="o">.</span><span class="n">get_shard_id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_data_layout</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">host_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLayout</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns filled `DataLayout` based on the partitioned model layout.</span>

<span class="sd">        Args:</span>
<span class="sd">          batch_size: if set, indicates the requested batch size. The exception will</span>
<span class="sd">            be raised if this batch size is not compatible with the layout. If not</span>
<span class="sd">            set, the batch size is inferred from the layout.</span>
<span class="sd">          host_index: indicates the host index to use for the calculations, if not</span>
<span class="sd">            set - use JAX-provided one. Should be in [0, num_hosts) interval and the</span>
<span class="sd">            order should match the order of corresponding CPU devices in</span>
<span class="sd">            `jax.devices()`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          Filled `DataLayout` structure.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">host_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Explicit host_index is not yet implemented.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">DataLayout</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">shard_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">num_shards</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">is_first_host_in_replica_set</span><span class="o">=</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">process_index</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_mesh_size</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_mesh_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;Batch size (</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">) must be divisible by corresponding &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;data mesh size (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data_mesh_size</span><span class="si">}</span><span class="s1">).&#39;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_shards</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s1">&#39;Batch size (</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">) must be divisible by number of &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;data shards (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data_shards</span><span class="si">}</span><span class="s1">).&#39;</span>
            <span class="p">)</span>
        <span class="n">replica_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_chunker</span><span class="o">.</span><span class="n">get_replica_id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DataLayout</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span>
            <span class="n">shard_id</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_shard_id</span><span class="p">),</span>
            <span class="n">num_shards</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_shards</span><span class="p">),</span>
            <span class="n">is_first_host_in_replica_set</span><span class="o">=</span><span class="p">(</span><span class="n">replica_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_local_chunk_info</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
            <span class="n">mesh_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">LocalChunkInfo</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the local chunk info for a given array shape and sharded axes.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_chunker</span><span class="o">.</span><span class="n">get_local_chunk_info</span><span class="p">(</span><span class="n">global_shape</span><span class="p">,</span> <span class="n">mesh_axes</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">params_on_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params_on_devices</span>

    <span class="nd">@params_on_devices</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">params_on_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_params_on_devices</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">move_params_to_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span>
                               <span class="n">train_state_axes</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Moves the optimizer parameters to devices.&quot;&quot;&quot;</span>
        <span class="n">p_id_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="p">(</span>
            <span class="n">_id_fn</span><span class="p">,</span>
            <span class="n">in_axis_resources</span><span class="o">=</span><span class="p">(</span><span class="n">train_state_axes</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">out_axis_resources</span><span class="o">=</span><span class="p">(</span><span class="n">train_state_axes</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="n">donate_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
        <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">process_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">train_state</span> <span class="o">=</span> <span class="n">host_local_array_to_global_array</span><span class="p">(</span>
                <span class="n">train_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span> <span class="n">train_state_axes</span>
            <span class="p">)</span>
        <span class="n">train_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">p_id_fn</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">uint32</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">train_state</span>

    <span class="nd">@property</span>
    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">_local_chunker</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the chunker that matches the parameters of this partitioner.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">get_logical_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a copy of TrainState with Optional[AxisNames] as leaves.&quot;&quot;&quot;</span>
        <span class="c1"># By default, return None for the logical axes.</span>
        <span class="k">return</span> <span class="n">train_state</span><span class="o">.</span><span class="n">restore_state</span><span class="p">(</span>
            <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()))</span>

    <span class="k">def</span> <span class="nf">get_mesh_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">partition</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>  <span class="c1"># pylint: disable=g-bare-generic</span>
            <span class="n">in_axis_resources</span><span class="p">,</span>
            <span class="n">out_axis_resources</span><span class="p">,</span>
            <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
            <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">()</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PartitionedCallable</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">compile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partitioned_fn</span><span class="p">:</span> <span class="n">PartitionedCallable</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CompiledPartitionedCallable</span><span class="p">:</span>

        <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_mesh_size" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">data_mesh_size</span><span class="p">:</span> <span class="nb">int</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Data mesh size.</p>
<p>Data mesh size is defined as the number of global devices involved to
carry out data parallel. Let's say we have a global mesh: ('replica': 2,
'data': 4, 'model': 2), and axes 'replica' and 'data' are responsible for
the data parallel, that means we have 2*4 = 8 devices involved - i.e., data
mesh size is 8.</p>
<p>Returns:
  the id of the shard for the axes being replicated among the devices used
  to shard the sharded_mesh_axes.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_shard_id" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">data_shard_id</span><span class="p">:</span> <span class="nb">int</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Data shard id for the current host.</p>
<p>Returns:
  Index of data shard that will be sent to the current local host.</p>
  </div>

</div>

<div class="doc doc-object doc-attribute">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_shards" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">data_shards</span><span class="p">:</span> <span class="nb">int</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Number of data shards.</p>
<p>Let's say we are dealing with 2 slices of df4x2 TPUs. In data pipeline
we need prepare / send one data shard to each local host. This means, we
need 4 shards since we have 4 local hosts. How to infer the number of hosts
from mesh information? In this case, we have a global mesh: ('replica': 2,
'data': 8, 'model': 2). Each local host (i.e., df2x2) has this local mesh:
('replica': 1, 'data': 4, 'model': 2). By dividing global mesh with local
mesh, we can get the count of hosts.</p>
<p>Returns:
  Number of data shards. Each shard will be sent to one local host.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">num_partitions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">model_parallel_submesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">params_on_devices</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ici_mesh_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dcn_mesh_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Configures the partitioner.</p>
<p>Args:
  num_partitions: the number of partitions to use. Ignored if
    <code>model_parallel_submesh</code> is provided.
  model_parallel_submesh: 4-tuple that specifies the x,y,z,c submesh to use
    as the model-parallel device tile. This submesh is used for the larger
    of the two parameter dimensions, and, if 2-D activation sharding is
    enabled, for the model dimension of activations. The rest of the mesh is
    used for data parallelism and, if 2-D parameter sharding is enabled, the
    other parameter dimension.
  params_on_devices: whether to keep the params on devices, if False -
    params stay in the host memory. Note that some partitioners might ignore
    this setting, for example if they don't support storing all params on
    device memory.
  backend: get devices from the pinned backend, if specified. This is useful
    for explicitly specifying the devices other than relying on
    jax_platform_name.
  ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in
    each slice. The meaning of each mesh axis is defined by mesh_axis_names,
    so these two params must be the same length. If dcn_mesh_shape is
    present, the overall mesh is the product of ici_mesh_shape and
    dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with
    mesh_axis_names ['replica', 'data', 'mdl'] indicates 2-way replica
    parallelism, 3-way data parallelism, and 4-way model parallelism over 24
    devices. None, the default, is equivalent to a sequence of ones and
    means that the model is placed on a single device.
  dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over
    multiple slices. The overall mesh is the product of ici_mesh_shape and
    dcn_mesh_shape, and the meaning of each mesh axis is defined by
    mesh_axis_names, so these three params must be the same length.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_partitions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_parallel_submesh</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params_on_devices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ici_mesh_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dcn_mesh_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Configures the partitioner.</span>

<span class="sd">    Args:</span>
<span class="sd">      num_partitions: the number of partitions to use. Ignored if</span>
<span class="sd">        `model_parallel_submesh` is provided.</span>
<span class="sd">      model_parallel_submesh: 4-tuple that specifies the x,y,z,c submesh to use</span>
<span class="sd">        as the model-parallel device tile. This submesh is used for the larger</span>
<span class="sd">        of the two parameter dimensions, and, if 2-D activation sharding is</span>
<span class="sd">        enabled, for the model dimension of activations. The rest of the mesh is</span>
<span class="sd">        used for data parallelism and, if 2-D parameter sharding is enabled, the</span>
<span class="sd">        other parameter dimension.</span>
<span class="sd">      params_on_devices: whether to keep the params on devices, if False -</span>
<span class="sd">        params stay in the host memory. Note that some partitioners might ignore</span>
<span class="sd">        this setting, for example if they don&#39;t support storing all params on</span>
<span class="sd">        device memory.</span>
<span class="sd">      backend: get devices from the pinned backend, if specified. This is useful</span>
<span class="sd">        for explicitly specifying the devices other than relying on</span>
<span class="sd">        jax_platform_name.</span>
<span class="sd">      ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in</span>
<span class="sd">        each slice. The meaning of each mesh axis is defined by mesh_axis_names,</span>
<span class="sd">        so these two params must be the same length. If dcn_mesh_shape is</span>
<span class="sd">        present, the overall mesh is the product of ici_mesh_shape and</span>
<span class="sd">        dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with</span>
<span class="sd">        mesh_axis_names [&#39;replica&#39;, &#39;data&#39;, &#39;mdl&#39;] indicates 2-way replica</span>
<span class="sd">        parallelism, 3-way data parallelism, and 4-way model parallelism over 24</span>
<span class="sd">        devices. None, the default, is equivalent to a sequence of ones and</span>
<span class="sd">        means that the model is placed on a single device.</span>
<span class="sd">      dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over</span>
<span class="sd">        multiple slices. The overall mesh is the product of ici_mesh_shape and</span>
<span class="sd">        dcn_mesh_shape, and the meaning of each mesh axis is defined by</span>
<span class="sd">        mesh_axis_names, so these three params must be the same length.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">num_partitions</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">model_parallel_submesh</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;At least one of `num_partitions` or &#39;</span>
                         <span class="s1">&#39;`model_parallel_submesh` must be set.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model_parallel_submesh</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_parallel_submesh</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="s1">&#39;`model_parallel_submesh` must be either None or a 4-tuple. Got&#39;</span>
                <span class="s1">&#39; `model_parallel_submesh`=</span><span class="si">%r</span><span class="s1">. A ValueError will be raised&#39;</span>
                <span class="s1">&#39; beginning March 1, 2022.&#39;</span>
            <span class="p">),</span>
            <span class="n">model_parallel_submesh</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="nb">bool</span><span class="p">(</span><span class="n">num_partitions</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">bool</span><span class="p">(</span><span class="n">model_parallel_submesh</span><span class="p">):</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
            <span class="s1">&#39;At most one of `num_partitions` or `model_parallel_submesh` can be &#39;</span>
            <span class="s1">&#39;set. Got `num_partitions=</span><span class="si">%r</span><span class="s1">` and `model_parallel_submesh`=</span><span class="si">%r</span><span class="s1">. A &#39;</span>
            <span class="s1">&#39;ValueError will be raised beginning March 21, 2022.&#39;</span><span class="p">,</span>
            <span class="n">num_partitions</span><span class="p">,</span>
            <span class="n">model_parallel_submesh</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span> <span class="o">=</span> <span class="n">num_partitions</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_model_parallel_submesh</span> <span class="o">=</span> <span class="n">model_parallel_submesh</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_params_on_devices</span> <span class="o">=</span> <span class="n">params_on_devices</span>
    <span class="k">if</span> <span class="n">ici_mesh_shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">dcn_mesh_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span> <span class="o">=</span> <span class="s1">&#39;data&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;replica&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span> <span class="o">=</span> <span class="n">backend</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ici_mesh_shape</span> <span class="o">=</span> <span class="n">ici_mesh_shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dcn_mesh_shape</span> <span class="o">=</span> <span class="n">dcn_mesh_shape</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_data_layout" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_data_layout</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">host_index</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns filled <code>DataLayout</code> based on the partitioned model layout.</p>
<p>Args:
  batch_size: if set, indicates the requested batch size. The exception will
    be raised if this batch size is not compatible with the layout. If not
    set, the batch size is inferred from the layout.
  host_index: indicates the host index to use for the calculations, if not
    set - use JAX-provided one. Should be in [0, num_hosts) interval and the
    order should match the order of corresponding CPU devices in
    <code>jax.devices()</code>.</p>
<p>Returns:
  Filled <code>DataLayout</code> structure.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_data_layout</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">host_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLayout</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns filled `DataLayout` based on the partitioned model layout.</span>

<span class="sd">    Args:</span>
<span class="sd">      batch_size: if set, indicates the requested batch size. The exception will</span>
<span class="sd">        be raised if this batch size is not compatible with the layout. If not</span>
<span class="sd">        set, the batch size is inferred from the layout.</span>
<span class="sd">      host_index: indicates the host index to use for the calculations, if not</span>
<span class="sd">        set - use JAX-provided one. Should be in [0, num_hosts) interval and the</span>
<span class="sd">        order should match the order of corresponding CPU devices in</span>
<span class="sd">        `jax.devices()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Filled `DataLayout` structure.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">host_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;Explicit host_index is not yet implemented.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">DataLayout</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">shard_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">num_shards</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">is_first_host_in_replica_set</span><span class="o">=</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">process_index</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_mesh_size</span>
    <span class="k">if</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_mesh_size</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;Batch size (</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">) must be divisible by corresponding &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;data mesh size (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data_mesh_size</span><span class="si">}</span><span class="s1">).&#39;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_shards</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;Batch size (</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s1">) must be divisible by number of &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;data shards (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data_shards</span><span class="si">}</span><span class="s1">).&#39;</span>
        <span class="p">)</span>
    <span class="n">replica_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_chunker</span><span class="o">.</span><span class="n">get_replica_id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">DataLayout</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span>
        <span class="n">shard_id</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_shard_id</span><span class="p">),</span>
        <span class="n">num_shards</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_shards</span><span class="p">),</span>
        <span class="n">is_first_host_in_replica_set</span><span class="o">=</span><span class="p">(</span><span class="n">replica_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_local_chunk_info" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_local_chunk_info</span><span class="p">(</span><span class="n">global_shape</span><span class="p">,</span> <span class="n">mesh_axes</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the local chunk info for a given array shape and sharded axes.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_local_chunk_info</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">mesh_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">LocalChunkInfo</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the local chunk info for a given array shape and sharded axes.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_chunker</span><span class="o">.</span><span class="n">get_local_chunk_info</span><span class="p">(</span><span class="n">global_shape</span><span class="p">,</span> <span class="n">mesh_axes</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_logical_axes" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_logical_axes</span><span class="p">(</span><span class="n">train_state</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns a copy of TrainState with Optional[AxisNames] as leaves.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_logical_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a copy of TrainState with Optional[AxisNames] as leaves.&quot;&quot;&quot;</span>
    <span class="c1"># By default, return None for the logical axes.</span>
    <span class="k">return</span> <span class="n">train_state</span><span class="o">.</span><span class="n">restore_state</span><span class="p">(</span>
        <span class="n">jax</span><span class="o">.</span><span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_mesh_axes" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_mesh_axes</span><span class="p">(</span><span class="n">train_state</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_mesh_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.&quot;&quot;&quot;</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.move_params_to_devices" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">move_params_to_devices</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">train_state_axes</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Moves the optimizer parameters to devices.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">move_params_to_devices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span>
                           <span class="n">train_state_axes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Moves the optimizer parameters to devices.&quot;&quot;&quot;</span>
    <span class="n">p_id_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partition</span><span class="p">(</span>
        <span class="n">_id_fn</span><span class="p">,</span>
        <span class="n">in_axis_resources</span><span class="o">=</span><span class="p">(</span><span class="n">train_state_axes</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">out_axis_resources</span><span class="o">=</span><span class="p">(</span><span class="n">train_state_axes</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">donate_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">process_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">train_state</span> <span class="o">=</span> <span class="n">host_local_array_to_global_array</span><span class="p">(</span>
            <span class="n">train_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span> <span class="n">train_state_axes</span>
        <span class="p">)</span>
    <span class="n">train_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">p_id_fn</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">uint32</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">train_state</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.BasePjitPartitioner" class="doc doc-heading">
          <code>BasePjitPartitioner</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="src.fjformer.partition_utils.t5x_partitioning.BasePartitioner" href="#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner">BasePartitioner</a></code></p>

  
      <p>Partitioner that uses T5X version of jax.pjit.</p>

            <details class="quote">
              <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">BasePjitPartitioner</span><span class="p">(</span><span class="n">BasePartitioner</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Partitioner that uses T5X version of jax.pjit.&quot;&quot;&quot;</span>

    <span class="nd">@cached_property</span>
    <span class="k">def</span> <span class="nf">_local_chunker</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LocalChunker</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">LocalChunker</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mesh</span><span class="p">)</span>

    <span class="nd">@cached_property</span>
    <span class="k">def</span> <span class="nf">mesh</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mesh</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">default_mesh</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_num_partitions</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model_parallel_submesh</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ici_mesh_shape</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dcn_mesh_shape</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">partition</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>  <span class="c1"># pylint: disable=g-bare-generic</span>
            <span class="n">in_axis_resources</span><span class="p">,</span>
            <span class="n">out_axis_resources</span><span class="p">,</span>
            <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
            <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PjittedFnWithContext</span><span class="p">:</span>
        <span class="n">pjitted</span> <span class="o">=</span> <span class="n">pjit</span><span class="p">(</span>
            <span class="n">fn</span><span class="p">,</span>
            <span class="n">in_shardings</span><span class="o">=</span><span class="n">in_axis_resources</span><span class="p">,</span>
            <span class="n">out_shardings</span><span class="o">=</span><span class="n">out_axis_resources</span><span class="p">,</span>
            <span class="n">static_argnums</span><span class="o">=</span><span class="n">static_argnums</span><span class="p">,</span>
            <span class="n">donate_argnums</span><span class="o">=</span><span class="n">donate_argnums</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">PjittedFnWithContext</span><span class="p">(</span><span class="n">pjitted</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partitioned_fn</span><span class="p">:</span> <span class="n">PjittedFnWithContext</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CompiledPartitionedCallable</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">partitioned_fn</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.DataLayout" class="doc doc-heading">
          <code>DataLayout</code>

  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-dataclass"><code>dataclass</code></small>
  </span>

</h2>


  <div class="doc doc-contents ">

  
      <p>Represents data layout for the partitioned model.</p>

            <details class="quote">
              <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">DataLayout</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Represents data layout for the partitioned model.&quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">shard_id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">num_shards</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">is_first_host_in_replica_set</span><span class="p">:</span> <span class="nb">bool</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.LocalChunker" class="doc doc-heading">
          <code>LocalChunker</code>


</h2>


  <div class="doc doc-contents ">

  
      <p>Utility class to aid chunking of sharded arrays in multihost settings.</p>

            <details class="quote">
              <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">LocalChunker</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Utility class to aid chunking of sharded arrays in multihost settings.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_mesh</span><span class="p">:</span> <span class="n">Mesh</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_mesh</span> <span class="o">=</span> <span class="n">global_mesh</span>
        <span class="n">local_mesh</span> <span class="o">=</span> <span class="n">global_mesh</span><span class="o">.</span><span class="n">local_mesh</span>
        <span class="n">first_local_device</span> <span class="o">=</span> <span class="n">local_mesh</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">host_location</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span>
                <span class="n">global_mesh</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span>
                <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span>
                    <span class="n">global_mesh</span><span class="o">.</span><span class="n">devices</span> <span class="o">==</span> <span class="n">first_local_device</span><span class="p">)))[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chunk_ids</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mesh_axes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">global_mesh</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">mesh_axis</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh_axes</span><span class="p">:</span>
            <span class="n">num_devices_per_chunk</span> <span class="o">=</span> <span class="n">local_mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">global_mesh</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">num_devices_per_chunk</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">chunk_ids</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">host_location</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span> <span class="o">//</span> <span class="n">num_devices_per_chunk</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_local_chunk_info</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
            <span class="n">mesh_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">LocalChunkInfo</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the local chunk info for a given array shape and sharded axes.</span>

<span class="sd">        Args:</span>
<span class="sd">          global_shape: the global, unsharded shape of the array to chunk.</span>
<span class="sd">          mesh_axes: a sequence of names (or None) of equal rank to `global_shape`</span>
<span class="sd">            that specifies which mesh dimensions the array is sharded along.</span>

<span class="sd">        Returns:</span>
<span class="sd">          LocalChunkInfo containing the logical slices of the array found on this</span>
<span class="sd">          host&#39;s local devices, as well as the replica index for this chunk among</span>
<span class="sd">          chunks with the same slice. The latter is used to determine which</span>
<span class="sd">          host should write this chunk during checkpointing.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">local_slice</span> <span class="o">=</span> <span class="p">[</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">global_shape</span><span class="p">]</span>
        <span class="n">sharded_mesh_axes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">mesh_axis</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">mesh_axes</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">)):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">mesh_axis</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">sharded_mesh_axes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mesh_axis</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mesh_axis</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;TODO(jekbradbury)&#39;</span><span class="p">)</span>
            <span class="n">chunk_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_ids</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span>
            <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span>
            <span class="n">local_slice</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">chunk_id</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="p">(</span><span class="n">chunk_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">)</span>

        <span class="n">replica_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_replica_id</span><span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">LocalChunkInfo</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">local_slice</span><span class="p">),</span> <span class="n">replica_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_shard_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sharded_mesh_axes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Given mesh axes used for sharding, computes current host&#39;s shard id.</span>

<span class="sd">        To give an example, let&#39;s say there are two axes globally: replica, data,</span>
<span class="sd">        and model, the mesh axes for sharding is (&#39;replica&#39;, &#39;data&#39;), which means we</span>
<span class="sd">        are going to partition an array along &#39;replica&#39; and &#39;data&#39; axes.</span>
<span class="sd">        The shard_id is to show the index of the current local host along the</span>
<span class="sd">        sharding axes (in this example, it&#39;s &#39;replica&#39; and &#39;data&#39; axes).</span>

<span class="sd">        More concretely, let&#39;s say we have 4 local hosts, and we use &#39;replica&#39; and</span>
<span class="sd">        &#39;data&#39; axes for data parallel (2 hosts along the replica axis, and 2 host</span>
<span class="sd">        along the data axis). The host located in (&#39;replica&#39;: 0, &#39;data&#39;: 0), we</span>
<span class="sd">        should assign data shard-0 to it. For host (&#39;replica&#39;: 0, &#39;data&#39;: 1), we</span>
<span class="sd">        assign shard-1. For host (&#39;replica&#39;: 1, &#39;data&#39;: 0), we assign shard-2.</span>
<span class="sd">        For host (&#39;replica&#39;: 1, &#39;data&#39;: 1), we assign shard-3.</span>

<span class="sd">        Note: the host location along &#39;replica&#39; and &#39;data&#39; axes, e.g.,</span>
<span class="sd">        (&#39;replica&#39;: 0, &#39;data&#39;: 0) is named chunk_id and stored in</span>
<span class="sd">        self._local_chunker.chunk_ids[axis].</span>

<span class="sd">        Args:</span>
<span class="sd">          sharded_mesh_axes: the mesh axes for sharding.</span>

<span class="sd">        Returns:</span>
<span class="sd">          the index of the current local host along the sharding axes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">sharded_mesh_axes</span> <span class="o">=</span> <span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">,)</span>

        <span class="n">shard_id</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">mesh_axis</span> <span class="ow">in</span> <span class="n">sharded_mesh_axes</span><span class="p">:</span>
            <span class="n">chunk_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_ids</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span>
            <span class="n">shard_id</span> <span class="o">=</span> <span class="n">shard_id</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">chunk_id</span>

        <span class="k">return</span> <span class="n">shard_id</span>

    <span class="k">def</span> <span class="nf">get_replica_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sharded_mesh_axes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Given mesh axes used for sharding, computes current host&#39;s replica id.</span>

<span class="sd">        To give an example, let&#39;s say there are two axes globally: data, and model,</span>
<span class="sd">        the mesh axes for sharding is (&#39;data&#39;, ), which means we are going to</span>
<span class="sd">        partition an array along &#39;data&#39; axis and replicate it along &#39;model&#39; axis.</span>
<span class="sd">        The replica_id is to show the index of the current local host along the</span>
<span class="sd">        &#39;model&#39; axis.</span>

<span class="sd">        Args:</span>
<span class="sd">          sharded_mesh_axes: the mesh axes for sharding.</span>

<span class="sd">        Returns:</span>
<span class="sd">          the index of the current local host along the non-sharding axes (i.e.,</span>
<span class="sd">          replicating axes).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">sharded_mesh_axes</span> <span class="o">=</span> <span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">,)</span>

        <span class="n">replicated_mesh_axes</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">mesh_axis</span> <span class="k">for</span> <span class="n">mesh_axis</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh_axes</span>
            <span class="k">if</span> <span class="n">mesh_axis</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sharded_mesh_axes</span>
        <span class="p">]</span>
        <span class="n">replica_id</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">mesh_axis</span> <span class="ow">in</span> <span class="n">replicated_mesh_axes</span><span class="p">:</span>
            <span class="n">chunk_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_ids</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span>
            <span class="n">replica_id</span> <span class="o">=</span> <span class="n">replica_id</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">chunk_id</span>

        <span class="k">return</span> <span class="n">replica_id</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_local_chunk_info" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_local_chunk_info</span><span class="p">(</span><span class="n">global_shape</span><span class="p">,</span> <span class="n">mesh_axes</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Get the local chunk info for a given array shape and sharded axes.</p>
<p>Args:
  global_shape: the global, unsharded shape of the array to chunk.
  mesh_axes: a sequence of names (or None) of equal rank to <code>global_shape</code>
    that specifies which mesh dimensions the array is sharded along.</p>
<p>Returns:
  LocalChunkInfo containing the logical slices of the array found on this
  host's local devices, as well as the replica index for this chunk among
  chunks with the same slice. The latter is used to determine which
  host should write this chunk during checkpointing.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_local_chunk_info</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
        <span class="n">mesh_axes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">LocalChunkInfo</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the local chunk info for a given array shape and sharded axes.</span>

<span class="sd">    Args:</span>
<span class="sd">      global_shape: the global, unsharded shape of the array to chunk.</span>
<span class="sd">      mesh_axes: a sequence of names (or None) of equal rank to `global_shape`</span>
<span class="sd">        that specifies which mesh dimensions the array is sharded along.</span>

<span class="sd">    Returns:</span>
<span class="sd">      LocalChunkInfo containing the logical slices of the array found on this</span>
<span class="sd">      host&#39;s local devices, as well as the replica index for this chunk among</span>
<span class="sd">      chunks with the same slice. The latter is used to determine which</span>
<span class="sd">      host should write this chunk during checkpointing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">local_slice</span> <span class="o">=</span> <span class="p">[</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">global_shape</span><span class="p">]</span>
    <span class="n">sharded_mesh_axes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">mesh_axis</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">mesh_axes</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">)):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">mesh_axis</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">sharded_mesh_axes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mesh_axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mesh_axis</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;TODO(jekbradbury)&#39;</span><span class="p">)</span>
        <span class="n">chunk_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_ids</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span>
        <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span>
        <span class="n">local_slice</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">chunk_id</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="p">(</span><span class="n">chunk_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">)</span>

    <span class="n">replica_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_replica_id</span><span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">LocalChunkInfo</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">local_slice</span><span class="p">),</span> <span class="n">replica_id</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_replica_id" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_replica_id</span><span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Given mesh axes used for sharding, computes current host's replica id.</p>
<p>To give an example, let's say there are two axes globally: data, and model,
the mesh axes for sharding is ('data', ), which means we are going to
partition an array along 'data' axis and replicate it along 'model' axis.
The replica_id is to show the index of the current local host along the
'model' axis.</p>
<p>Args:
  sharded_mesh_axes: the mesh axes for sharding.</p>
<p>Returns:
  the index of the current local host along the non-sharding axes (i.e.,
  replicating axes).</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_replica_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sharded_mesh_axes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Given mesh axes used for sharding, computes current host&#39;s replica id.</span>

<span class="sd">    To give an example, let&#39;s say there are two axes globally: data, and model,</span>
<span class="sd">    the mesh axes for sharding is (&#39;data&#39;, ), which means we are going to</span>
<span class="sd">    partition an array along &#39;data&#39; axis and replicate it along &#39;model&#39; axis.</span>
<span class="sd">    The replica_id is to show the index of the current local host along the</span>
<span class="sd">    &#39;model&#39; axis.</span>

<span class="sd">    Args:</span>
<span class="sd">      sharded_mesh_axes: the mesh axes for sharding.</span>

<span class="sd">    Returns:</span>
<span class="sd">      the index of the current local host along the non-sharding axes (i.e.,</span>
<span class="sd">      replicating axes).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">sharded_mesh_axes</span> <span class="o">=</span> <span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">,)</span>

    <span class="n">replicated_mesh_axes</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">mesh_axis</span> <span class="k">for</span> <span class="n">mesh_axis</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh_axes</span>
        <span class="k">if</span> <span class="n">mesh_axis</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sharded_mesh_axes</span>
    <span class="p">]</span>
    <span class="n">replica_id</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">mesh_axis</span> <span class="ow">in</span> <span class="n">replicated_mesh_axes</span><span class="p">:</span>
        <span class="n">chunk_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_ids</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span>
        <span class="n">replica_id</span> <span class="o">=</span> <span class="n">replica_id</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">chunk_id</span>

    <span class="k">return</span> <span class="n">replica_id</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_shard_id" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_shard_id</span><span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Given mesh axes used for sharding, computes current host's shard id.</p>
<p>To give an example, let's say there are two axes globally: replica, data,
and model, the mesh axes for sharding is ('replica', 'data'), which means we
are going to partition an array along 'replica' and 'data' axes.
The shard_id is to show the index of the current local host along the
sharding axes (in this example, it's 'replica' and 'data' axes).</p>
<p>More concretely, let's say we have 4 local hosts, and we use 'replica' and
'data' axes for data parallel (2 hosts along the replica axis, and 2 host
along the data axis). The host located in ('replica': 0, 'data': 0), we
should assign data shard-0 to it. For host ('replica': 0, 'data': 1), we
assign shard-1. For host ('replica': 1, 'data': 0), we assign shard-2.
For host ('replica': 1, 'data': 1), we assign shard-3.</p>
<p>Note: the host location along 'replica' and 'data' axes, e.g.,
('replica': 0, 'data': 0) is named chunk_id and stored in
self._local_chunker.chunk_ids[axis].</p>
<p>Args:
  sharded_mesh_axes: the mesh axes for sharding.</p>
<p>Returns:
  the index of the current local host along the sharding axes.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_shard_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sharded_mesh_axes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Given mesh axes used for sharding, computes current host&#39;s shard id.</span>

<span class="sd">    To give an example, let&#39;s say there are two axes globally: replica, data,</span>
<span class="sd">    and model, the mesh axes for sharding is (&#39;replica&#39;, &#39;data&#39;), which means we</span>
<span class="sd">    are going to partition an array along &#39;replica&#39; and &#39;data&#39; axes.</span>
<span class="sd">    The shard_id is to show the index of the current local host along the</span>
<span class="sd">    sharding axes (in this example, it&#39;s &#39;replica&#39; and &#39;data&#39; axes).</span>

<span class="sd">    More concretely, let&#39;s say we have 4 local hosts, and we use &#39;replica&#39; and</span>
<span class="sd">    &#39;data&#39; axes for data parallel (2 hosts along the replica axis, and 2 host</span>
<span class="sd">    along the data axis). The host located in (&#39;replica&#39;: 0, &#39;data&#39;: 0), we</span>
<span class="sd">    should assign data shard-0 to it. For host (&#39;replica&#39;: 0, &#39;data&#39;: 1), we</span>
<span class="sd">    assign shard-1. For host (&#39;replica&#39;: 1, &#39;data&#39;: 0), we assign shard-2.</span>
<span class="sd">    For host (&#39;replica&#39;: 1, &#39;data&#39;: 1), we assign shard-3.</span>

<span class="sd">    Note: the host location along &#39;replica&#39; and &#39;data&#39; axes, e.g.,</span>
<span class="sd">    (&#39;replica&#39;: 0, &#39;data&#39;: 0) is named chunk_id and stored in</span>
<span class="sd">    self._local_chunker.chunk_ids[axis].</span>

<span class="sd">    Args:</span>
<span class="sd">      sharded_mesh_axes: the mesh axes for sharding.</span>

<span class="sd">    Returns:</span>
<span class="sd">      the index of the current local host along the sharding axes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">sharded_mesh_axes</span> <span class="o">=</span> <span class="p">(</span><span class="n">sharded_mesh_axes</span><span class="p">,)</span>

    <span class="n">shard_id</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">mesh_axis</span> <span class="ow">in</span> <span class="n">sharded_mesh_axes</span><span class="p">:</span>
        <span class="n">chunk_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunk_ids</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span>
        <span class="n">shard_id</span> <span class="o">=</span> <span class="n">shard_id</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_chunks</span><span class="p">[</span><span class="n">mesh_axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">chunk_id</span>

    <span class="k">return</span> <span class="n">shard_id</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner" class="doc doc-heading">
          <code>PjitPartitioner</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="src.fjformer.partition_utils.t5x_partitioning.BasePjitPartitioner" href="#src.fjformer.partition_utils.t5x_partitioning.BasePjitPartitioner">BasePjitPartitioner</a></code></p>

  
      <p>Partitioner that uses named axes and jax.pjit.</p>

            <details class="quote">
              <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 944</span>
<span class="normal"> 945</span>
<span class="normal"> 946</span>
<span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span>
<span class="normal">1024</span>
<span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">PjitPartitioner</span><span class="p">(</span><span class="n">BasePjitPartitioner</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Partitioner that uses named axes and jax.pjit.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_partitions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">model_parallel_submesh</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">params_on_devices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">ici_mesh_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">dcn_mesh_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">logical_axis_rules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogicalAxisRules</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;PjitPartitioner constructor.</span>

<span class="sd">        See https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.mdx/usage/partitioning for details.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_partitions: an integer that specifies the size of the model parallel</span>
<span class="sd">            submesh to be automatically selected for the current topology. See</span>
<span class="sd">            `model_parallel_submesh` for details on how this submesh is used.</span>
<span class="sd">            Mutually exclusive with `model_parallel_submesh`.</span>
<span class="sd">          model_parallel_submesh: is a 4-tuple that specifies the `(x, y, z, c)`</span>
<span class="sd">            submesh model-parallel device tile, an axis of accelerator parallelism</span>
<span class="sd">            orthogonal to data parallelism. Array axes in a model&#39;s parameters or</span>
<span class="sd">            activations can be sharded over this submesh using axis rules (see</span>
<span class="sd">            `logical_axis_rules`) that map them to &#39;model&#39;. The effective number of</span>
<span class="sd">            model sub-partitions is equal to `np.prod(model_parallel_submesh)` and</span>
<span class="sd">            must evenly divide the total number of devices (i.e.,</span>
<span class="sd">            `jax.device_count() % np.prod(model_parallel_submesh) == 0`). The rest</span>
<span class="sd">            of the TPU mesh is the data parallel submesh, providing</span>
<span class="sd">            `jax.device_count() // np.prod(model_parallel_submesh)` partitions. It</span>
<span class="sd">            is used for data (batch) parallelism and to shard other array axes that</span>
<span class="sd">            are mapped to &#39;data&#39;. This argument is mutually exclusive with</span>
<span class="sd">            `num_partitions`.</span>
<span class="sd">          params_on_devices: whether to keep the params on devices, if False -</span>
<span class="sd">            params stay in the host memory. Note that some partitioners might ignore</span>
<span class="sd">            this setting, for example if they don&#39;t support storing all params on</span>
<span class="sd">            device memory.</span>
<span class="sd">          backend: get devices from the pinned backend, if specified. This is useful</span>
<span class="sd">            for explicitly specifying the devices other than relying on</span>
<span class="sd">            jax_platform_name.</span>
<span class="sd">          ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in</span>
<span class="sd">            each slice. The meaning of each mesh axis is defined by mesh_axis_names,</span>
<span class="sd">            so these two params must be the same length. If dcn_mesh_shape is</span>
<span class="sd">            present, the overall mesh is the product of ici_mesh_shape and</span>
<span class="sd">            dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with</span>
<span class="sd">            mesh_axis_names [&#39;replica&#39;, &#39;data&#39;, &#39;model&#39;] indicates 2-way replica</span>
<span class="sd">            parallelism, 3-way data parallelism, and 4-way model parallelism over 24</span>
<span class="sd">            devices. None, the default, is equivalent to a sequence of ones and</span>
<span class="sd">            means that the model is placed on a single device.</span>
<span class="sd">          dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over</span>
<span class="sd">            multiple slices. The overall mesh is the product of ici_mesh_shape and</span>
<span class="sd">            dcn_mesh_shape, and the meaning of each mesh axis is defined by</span>
<span class="sd">            mesh_axis_names, so these three params must be the same length.</span>
<span class="sd">          logical_axis_rules: a priority-ordered sequence of KV tuples that maps</span>
<span class="sd">            logical axis names to either `None` (not sharded), &#39;model&#39; (to shard</span>
<span class="sd">            across the model-parallel submesh), or &#39;data&#39; (to shard across the</span>
<span class="sd">            data-parallel submesh).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">num_partitions</span><span class="o">=</span><span class="n">num_partitions</span><span class="p">,</span>
            <span class="n">model_parallel_submesh</span><span class="o">=</span><span class="n">model_parallel_submesh</span><span class="p">,</span>
            <span class="n">params_on_devices</span><span class="o">=</span><span class="n">params_on_devices</span><span class="p">,</span>
            <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
            <span class="n">ici_mesh_shape</span><span class="o">=</span><span class="n">ici_mesh_shape</span><span class="p">,</span>
            <span class="n">dcn_mesh_shape</span><span class="o">=</span><span class="n">dcn_mesh_shape</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">logical_axis_rules</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logical_axis_rules</span> <span class="o">=</span> <span class="n">standard_logical_axis_rules</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">ici_mesh_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dcn_mesh_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Split batch over new replica axis.</span>
            <span class="n">logical_axis_rules</span> <span class="o">=</span> <span class="p">(</span>
                <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;replica&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s1">&#39;batch&#39;</span> <span class="k">else</span> <span class="n">v</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">logical_axis_rules</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">logical_axis_rules</span><span class="p">)</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">,)</span> <span class="o">=</span> <span class="n">flax_partitioning</span><span class="o">.</span><span class="n">logical_to_mesh_axes</span><span class="p">(</span>
            <span class="p">[</span><span class="s1">&#39;batch&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">partition</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>  <span class="c1"># pylint: disable=g-bare-generic</span>
            <span class="n">in_axis_resources</span><span class="p">,</span>
            <span class="n">out_axis_resources</span><span class="p">,</span>
            <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
            <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">()</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PjittedFnWithContext</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Partitions the function using jax.pjit.&quot;&quot;&quot;</span>
        <span class="n">pjitted</span> <span class="o">=</span> <span class="n">pjit</span><span class="p">(</span>
            <span class="n">fn</span><span class="p">,</span>
            <span class="n">in_shardings</span><span class="o">=</span><span class="n">in_axis_resources</span><span class="p">,</span>
            <span class="n">out_shardings</span><span class="o">=</span><span class="n">out_axis_resources</span><span class="p">,</span>
            <span class="n">static_argnums</span><span class="o">=</span><span class="n">static_argnums</span><span class="p">,</span>
            <span class="n">donate_argnums</span><span class="o">=</span><span class="n">donate_argnums</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">PjittedFnWithContext</span><span class="p">(</span><span class="n">pjitted</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">logical_axis_rules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the logical axis rules.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span>

    <span class="k">def</span> <span class="nf">get_logical_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a copy of TrainState with Optional[AxisNames] as leaves.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">train_state</span><span class="o">.</span><span class="n">as_logical_axes</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_mesh_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.&quot;&quot;&quot;</span>
        <span class="n">logical_axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_logical_axes</span><span class="p">(</span><span class="n">train_state</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_logical_to_mesh_axes</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">logical_axes</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">logical_axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">None</span>
            <span class="k">elif</span> <span class="n">logical_axes</span> <span class="ow">is</span> <span class="n">traverse_util</span><span class="o">.</span><span class="n">empty_node</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">traverse_util</span><span class="o">.</span><span class="n">empty_node</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">flax_partitioning</span><span class="o">.</span><span class="n">logical_to_mesh_axes</span><span class="p">(</span><span class="n">logical_axes</span><span class="p">,</span>
                                                              <span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Failed to map logical axes for </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

        <span class="n">flat_logical_axes</span> <span class="o">=</span> <span class="n">traverse_util</span><span class="o">.</span><span class="n">flatten_dict</span><span class="p">(</span>
            <span class="n">logical_axes</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">keep_empty_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;/&#39;</span><span class="p">)</span>
        <span class="n">flat_mesh_axes</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">_logical_to_mesh_axes</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">flat_logical_axes</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="n">logical_axes</span><span class="o">.</span><span class="n">restore_state</span><span class="p">(</span>
            <span class="n">traverse_util</span><span class="o">.</span><span class="n">unflatten_dict</span><span class="p">(</span><span class="n">flat_mesh_axes</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;/&#39;</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.logical_axis_rules" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">logical_axis_rules</span></code>
  
  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns the logical axis rules.</p>
  </div>

</div>




<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">num_partitions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">model_parallel_submesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">params_on_devices</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ici_mesh_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dcn_mesh_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">logical_axis_rules</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>PjitPartitioner constructor.</p>
<p>See https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.mdx/usage/partitioning for details.</p>
<p>Args:
  num_partitions: an integer that specifies the size of the model parallel
    submesh to be automatically selected for the current topology. See
    <code>model_parallel_submesh</code> for details on how this submesh is used.
    Mutually exclusive with <code>model_parallel_submesh</code>.
  model_parallel_submesh: is a 4-tuple that specifies the <code>(x, y, z, c)</code>
    submesh model-parallel device tile, an axis of accelerator parallelism
    orthogonal to data parallelism. Array axes in a model's parameters or
    activations can be sharded over this submesh using axis rules (see
    <code>logical_axis_rules</code>) that map them to 'model'. The effective number of
    model sub-partitions is equal to <code>np.prod(model_parallel_submesh)</code> and
    must evenly divide the total number of devices (i.e.,
    <code>jax.device_count() % np.prod(model_parallel_submesh) == 0</code>). The rest
    of the TPU mesh is the data parallel submesh, providing
    <code>jax.device_count() // np.prod(model_parallel_submesh)</code> partitions. It
    is used for data (batch) parallelism and to shard other array axes that
    are mapped to 'data'. This argument is mutually exclusive with
    <code>num_partitions</code>.
  params_on_devices: whether to keep the params on devices, if False -
    params stay in the host memory. Note that some partitioners might ignore
    this setting, for example if they don't support storing all params on
    device memory.
  backend: get devices from the pinned backend, if specified. This is useful
    for explicitly specifying the devices other than relying on
    jax_platform_name.
  ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in
    each slice. The meaning of each mesh axis is defined by mesh_axis_names,
    so these two params must be the same length. If dcn_mesh_shape is
    present, the overall mesh is the product of ici_mesh_shape and
    dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with
    mesh_axis_names ['replica', 'data', 'model'] indicates 2-way replica
    parallelism, 3-way data parallelism, and 4-way model parallelism over 24
    devices. None, the default, is equivalent to a sequence of ones and
    means that the model is placed on a single device.
  dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over
    multiple slices. The overall mesh is the product of ici_mesh_shape and
    dcn_mesh_shape, and the meaning of each mesh axis is defined by
    mesh_axis_names, so these three params must be the same length.
  logical_axis_rules: a priority-ordered sequence of KV tuples that maps
    logical axis names to either <code>None</code> (not sharded), 'model' (to shard
    across the model-parallel submesh), or 'data' (to shard across the
    data-parallel submesh).</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 947</span>
<span class="normal"> 948</span>
<span class="normal"> 949</span>
<span class="normal"> 950</span>
<span class="normal"> 951</span>
<span class="normal"> 952</span>
<span class="normal"> 953</span>
<span class="normal"> 954</span>
<span class="normal"> 955</span>
<span class="normal"> 956</span>
<span class="normal"> 957</span>
<span class="normal"> 958</span>
<span class="normal"> 959</span>
<span class="normal"> 960</span>
<span class="normal"> 961</span>
<span class="normal"> 962</span>
<span class="normal"> 963</span>
<span class="normal"> 964</span>
<span class="normal"> 965</span>
<span class="normal"> 966</span>
<span class="normal"> 967</span>
<span class="normal"> 968</span>
<span class="normal"> 969</span>
<span class="normal"> 970</span>
<span class="normal"> 971</span>
<span class="normal"> 972</span>
<span class="normal"> 973</span>
<span class="normal"> 974</span>
<span class="normal"> 975</span>
<span class="normal"> 976</span>
<span class="normal"> 977</span>
<span class="normal"> 978</span>
<span class="normal"> 979</span>
<span class="normal"> 980</span>
<span class="normal"> 981</span>
<span class="normal"> 982</span>
<span class="normal"> 983</span>
<span class="normal"> 984</span>
<span class="normal"> 985</span>
<span class="normal"> 986</span>
<span class="normal"> 987</span>
<span class="normal"> 988</span>
<span class="normal"> 989</span>
<span class="normal"> 990</span>
<span class="normal"> 991</span>
<span class="normal"> 992</span>
<span class="normal"> 993</span>
<span class="normal"> 994</span>
<span class="normal"> 995</span>
<span class="normal"> 996</span>
<span class="normal"> 997</span>
<span class="normal"> 998</span>
<span class="normal"> 999</span>
<span class="normal">1000</span>
<span class="normal">1001</span>
<span class="normal">1002</span>
<span class="normal">1003</span>
<span class="normal">1004</span>
<span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span>
<span class="normal">1022</span>
<span class="normal">1023</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">num_partitions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">model_parallel_submesh</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params_on_devices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ici_mesh_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dcn_mesh_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logical_axis_rules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogicalAxisRules</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;PjitPartitioner constructor.</span>

<span class="sd">    See https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.mdx/usage/partitioning for details.</span>

<span class="sd">    Args:</span>
<span class="sd">      num_partitions: an integer that specifies the size of the model parallel</span>
<span class="sd">        submesh to be automatically selected for the current topology. See</span>
<span class="sd">        `model_parallel_submesh` for details on how this submesh is used.</span>
<span class="sd">        Mutually exclusive with `model_parallel_submesh`.</span>
<span class="sd">      model_parallel_submesh: is a 4-tuple that specifies the `(x, y, z, c)`</span>
<span class="sd">        submesh model-parallel device tile, an axis of accelerator parallelism</span>
<span class="sd">        orthogonal to data parallelism. Array axes in a model&#39;s parameters or</span>
<span class="sd">        activations can be sharded over this submesh using axis rules (see</span>
<span class="sd">        `logical_axis_rules`) that map them to &#39;model&#39;. The effective number of</span>
<span class="sd">        model sub-partitions is equal to `np.prod(model_parallel_submesh)` and</span>
<span class="sd">        must evenly divide the total number of devices (i.e.,</span>
<span class="sd">        `jax.device_count() % np.prod(model_parallel_submesh) == 0`). The rest</span>
<span class="sd">        of the TPU mesh is the data parallel submesh, providing</span>
<span class="sd">        `jax.device_count() // np.prod(model_parallel_submesh)` partitions. It</span>
<span class="sd">        is used for data (batch) parallelism and to shard other array axes that</span>
<span class="sd">        are mapped to &#39;data&#39;. This argument is mutually exclusive with</span>
<span class="sd">        `num_partitions`.</span>
<span class="sd">      params_on_devices: whether to keep the params on devices, if False -</span>
<span class="sd">        params stay in the host memory. Note that some partitioners might ignore</span>
<span class="sd">        this setting, for example if they don&#39;t support storing all params on</span>
<span class="sd">        device memory.</span>
<span class="sd">      backend: get devices from the pinned backend, if specified. This is useful</span>
<span class="sd">        for explicitly specifying the devices other than relying on</span>
<span class="sd">        jax_platform_name.</span>
<span class="sd">      ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in</span>
<span class="sd">        each slice. The meaning of each mesh axis is defined by mesh_axis_names,</span>
<span class="sd">        so these two params must be the same length. If dcn_mesh_shape is</span>
<span class="sd">        present, the overall mesh is the product of ici_mesh_shape and</span>
<span class="sd">        dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with</span>
<span class="sd">        mesh_axis_names [&#39;replica&#39;, &#39;data&#39;, &#39;model&#39;] indicates 2-way replica</span>
<span class="sd">        parallelism, 3-way data parallelism, and 4-way model parallelism over 24</span>
<span class="sd">        devices. None, the default, is equivalent to a sequence of ones and</span>
<span class="sd">        means that the model is placed on a single device.</span>
<span class="sd">      dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over</span>
<span class="sd">        multiple slices. The overall mesh is the product of ici_mesh_shape and</span>
<span class="sd">        dcn_mesh_shape, and the meaning of each mesh axis is defined by</span>
<span class="sd">        mesh_axis_names, so these three params must be the same length.</span>
<span class="sd">      logical_axis_rules: a priority-ordered sequence of KV tuples that maps</span>
<span class="sd">        logical axis names to either `None` (not sharded), &#39;model&#39; (to shard</span>
<span class="sd">        across the model-parallel submesh), or &#39;data&#39; (to shard across the</span>
<span class="sd">        data-parallel submesh).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">num_partitions</span><span class="o">=</span><span class="n">num_partitions</span><span class="p">,</span>
        <span class="n">model_parallel_submesh</span><span class="o">=</span><span class="n">model_parallel_submesh</span><span class="p">,</span>
        <span class="n">params_on_devices</span><span class="o">=</span><span class="n">params_on_devices</span><span class="p">,</span>
        <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
        <span class="n">ici_mesh_shape</span><span class="o">=</span><span class="n">ici_mesh_shape</span><span class="p">,</span>
        <span class="n">dcn_mesh_shape</span><span class="o">=</span><span class="n">dcn_mesh_shape</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">logical_axis_rules</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logical_axis_rules</span> <span class="o">=</span> <span class="n">standard_logical_axis_rules</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">ici_mesh_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dcn_mesh_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Split batch over new replica axis.</span>
        <span class="n">logical_axis_rules</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;replica&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s1">&#39;batch&#39;</span> <span class="k">else</span> <span class="n">v</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">logical_axis_rules</span>
        <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">logical_axis_rules</span><span class="p">)</span>
    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_axis</span><span class="p">,)</span> <span class="o">=</span> <span class="n">flax_partitioning</span><span class="o">.</span><span class="n">logical_to_mesh_axes</span><span class="p">(</span>
        <span class="p">[</span><span class="s1">&#39;batch&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.get_logical_axes" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_logical_axes</span><span class="p">(</span><span class="n">train_state</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns a copy of TrainState with Optional[AxisNames] as leaves.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_logical_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a copy of TrainState with Optional[AxisNames] as leaves.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">train_state</span><span class="o">.</span><span class="n">as_logical_axes</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.get_mesh_axes" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_mesh_axes</span><span class="p">(</span><span class="n">train_state</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_mesh_axes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_state</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.&quot;&quot;&quot;</span>
    <span class="n">logical_axes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_logical_axes</span><span class="p">(</span><span class="n">train_state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_logical_to_mesh_axes</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">logical_axes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">logical_axes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">elif</span> <span class="n">logical_axes</span> <span class="ow">is</span> <span class="n">traverse_util</span><span class="o">.</span><span class="n">empty_node</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">traverse_util</span><span class="o">.</span><span class="n">empty_node</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">flax_partitioning</span><span class="o">.</span><span class="n">logical_to_mesh_axes</span><span class="p">(</span><span class="n">logical_axes</span><span class="p">,</span>
                                                          <span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Failed to map logical axes for </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

    <span class="n">flat_logical_axes</span> <span class="o">=</span> <span class="n">traverse_util</span><span class="o">.</span><span class="n">flatten_dict</span><span class="p">(</span>
        <span class="n">logical_axes</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">keep_empty_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;/&#39;</span><span class="p">)</span>
    <span class="n">flat_mesh_axes</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">_logical_to_mesh_axes</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">flat_logical_axes</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">logical_axes</span><span class="o">.</span><span class="n">restore_state</span><span class="p">(</span>
        <span class="n">traverse_util</span><span class="o">.</span><span class="n">unflatten_dict</span><span class="p">(</span><span class="n">flat_mesh_axes</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;/&#39;</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.partition" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">partition</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">in_axis_resources</span><span class="p">,</span> <span class="n">out_axis_resources</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(),</span> <span class="n">donate_argnums</span><span class="o">=</span><span class="p">())</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Partitions the function using jax.pjit.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1025</span>
<span class="normal">1026</span>
<span class="normal">1027</span>
<span class="normal">1028</span>
<span class="normal">1029</span>
<span class="normal">1030</span>
<span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">partition</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>  <span class="c1"># pylint: disable=g-bare-generic</span>
        <span class="n">in_axis_resources</span><span class="p">,</span>
        <span class="n">out_axis_resources</span><span class="p">,</span>
        <span class="n">static_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(),</span>
        <span class="n">donate_argnums</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">()</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PjittedFnWithContext</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Partitions the function using jax.pjit.&quot;&quot;&quot;</span>
    <span class="n">pjitted</span> <span class="o">=</span> <span class="n">pjit</span><span class="p">(</span>
        <span class="n">fn</span><span class="p">,</span>
        <span class="n">in_shardings</span><span class="o">=</span><span class="n">in_axis_resources</span><span class="p">,</span>
        <span class="n">out_shardings</span><span class="o">=</span><span class="n">out_axis_resources</span><span class="p">,</span>
        <span class="n">static_argnums</span><span class="o">=</span><span class="n">static_argnums</span><span class="p">,</span>
        <span class="n">donate_argnums</span><span class="o">=</span><span class="n">donate_argnums</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">PjittedFnWithContext</span><span class="p">(</span><span class="n">pjitted</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.PjittedFnWithContext" class="doc doc-heading">
          <code>PjittedFnWithContext</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="src.fjformer.partition_utils.t5x_partitioning.PartitionedCallable">PartitionedCallable</span></code></p>

  
      <p>Wraps pjitted function to apply the appropriate contexts.</p>

            <details class="quote">
              <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">PjittedFnWithContext</span><span class="p">(</span><span class="n">PartitionedCallable</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wraps pjitted function to apply the appropriate contexts.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">pjitted_fn</span><span class="p">,</span>
                 <span class="n">partition_mesh</span><span class="p">:</span> <span class="n">Mesh</span><span class="p">,</span>
                 <span class="n">logical_axis_rules</span><span class="p">:</span> <span class="n">flax_partitioning</span><span class="o">.</span><span class="n">LogicalRules</span> <span class="o">=</span> <span class="p">()):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pjitted_fn</span> <span class="o">=</span> <span class="n">pjitted_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mesh</span> <span class="o">=</span> <span class="n">partition_mesh</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span> <span class="o">=</span> <span class="n">logical_axis_rules</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">Mesh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_mesh</span><span class="o">.</span><span class="n">devices</span><span class="p">,</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">_mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">),</span> <span class="n">flax_partitioning</span><span class="o">.</span><span class="n">axis_rules</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pjitted_fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">lower</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">Mesh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_mesh</span><span class="o">.</span><span class="n">devices</span><span class="p">,</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">_mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">),</span> <span class="n">flax_partitioning</span><span class="o">.</span><span class="n">axis_rules</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_logical_axis_rules</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pjitted_fn</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">











  </div>

  </div>


</div>



<div class="doc doc-object doc-function">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.bounds_from_last_device" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">bounds_from_last_device</span><span class="p">(</span><span class="n">last_device</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Get the bound from the given last device.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">bounds_from_last_device</span><span class="p">(</span><span class="n">last_device</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">HardwareMesh</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the bound from the given last device.&quot;&quot;&quot;</span>
    <span class="c1"># Must be passed the device at the highest-coordinate corner of the</span>
    <span class="c1"># relevant mesh, which is a requirement we know is satisfied by the last</span>
    <span class="c1"># device in jax.devices().</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">last_device</span><span class="p">,</span> <span class="s1">&#39;coords&#39;</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">last_device</span><span class="o">.</span><span class="n">coords</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">z</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">last_device</span><span class="o">.</span><span class="n">core_on_chip</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># On non-TPU platforms, the &quot;mesh&quot; is hosts x devices per host in order</span>
        <span class="c1"># to take advantage of faster within-host interconnect.</span>
        <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">process_count</span><span class="p">(),</span> <span class="n">jax</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.default_mesh" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">default_mesh</span><span class="p">(</span><span class="n">num_partitions</span><span class="p">,</span> <span class="n">model_parallel_submesh</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ici_mesh_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dcn_mesh_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Attempt to return a default mesh for simple cases.</p>
<p>Args:
  num_partitions: number of partitions to use, will be ignored if
    model_parallel_submesh is provided.
  model_parallel_submesh: 4-tuple that specifies the x,y,z,c submesh to use as
    the model-parallel device tile.
  backend: get devices from the pinned backend, if specified. This is useful
    for explicitly specifying the devices other than relying on
    jax_platform_name.
  ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in each
    slice. The meaning of each mesh axis is defined by mesh_axis_names, so
    these two params must be the same length. If dcn_mesh_shape is present,
    the overall mesh is the product of ici_mesh_shape and dcn_mesh_shape. For
    example, an ici_mesh_shape of [2, 3, 4] with mesh_axis_names ['replica',
    'data', 'model'] indicates 2-way replica parallelism, 3-way data
    parallelism, and 4-way model parallelism over 24 devices. None, the
    default, is equivalent to a sequence of ones and means that the model is
    placed on a single device.
  dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over
    multiple slices. The overall mesh is the product of ici_mesh_shape and
    dcn_mesh_shape, and the meaning of each mesh axis is defined by
    mesh_axis_names, so these three params must be the same length.</p>
<p>Returns:
  xmap/pjit 2D Mesh with 'data', 'model' mesh axes if single-slice, otherwise
  3D Mesh with 'replica', 'data', and 'model' mesh axes.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">default_mesh</span><span class="p">(</span>
        <span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">model_parallel_submesh</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ici_mesh_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dcn_mesh_shape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">HardwareMesh</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mesh</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Attempt to return a default mesh for simple cases.</span>

<span class="sd">    Args:</span>
<span class="sd">      num_partitions: number of partitions to use, will be ignored if</span>
<span class="sd">        model_parallel_submesh is provided.</span>
<span class="sd">      model_parallel_submesh: 4-tuple that specifies the x,y,z,c submesh to use as</span>
<span class="sd">        the model-parallel device tile.</span>
<span class="sd">      backend: get devices from the pinned backend, if specified. This is useful</span>
<span class="sd">        for explicitly specifying the devices other than relying on</span>
<span class="sd">        jax_platform_name.</span>
<span class="sd">      ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in each</span>
<span class="sd">        slice. The meaning of each mesh axis is defined by mesh_axis_names, so</span>
<span class="sd">        these two params must be the same length. If dcn_mesh_shape is present,</span>
<span class="sd">        the overall mesh is the product of ici_mesh_shape and dcn_mesh_shape. For</span>
<span class="sd">        example, an ici_mesh_shape of [2, 3, 4] with mesh_axis_names [&#39;replica&#39;,</span>
<span class="sd">        &#39;data&#39;, &#39;model&#39;] indicates 2-way replica parallelism, 3-way data</span>
<span class="sd">        parallelism, and 4-way model parallelism over 24 devices. None, the</span>
<span class="sd">        default, is equivalent to a sequence of ones and means that the model is</span>
<span class="sd">        placed on a single device.</span>
<span class="sd">      dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over</span>
<span class="sd">        multiple slices. The overall mesh is the product of ici_mesh_shape and</span>
<span class="sd">        dcn_mesh_shape, and the meaning of each mesh axis is defined by</span>
<span class="sd">        mesh_axis_names, so these three params must be the same length.</span>

<span class="sd">    Returns:</span>
<span class="sd">      xmap/pjit 2D Mesh with &#39;data&#39;, &#39;model&#39; mesh axes if single-slice, otherwise</span>
<span class="sd">      3D Mesh with &#39;replica&#39;, &#39;data&#39;, and &#39;model&#39; mesh axes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
    <span class="n">last_device</span> <span class="o">=</span> <span class="n">devices</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">platform</span> <span class="o">=</span> <span class="n">last_device</span><span class="o">.</span><span class="n">platform</span>
    <span class="n">device_kind</span> <span class="o">=</span> <span class="n">last_device</span><span class="o">.</span><span class="n">device_kind</span>
    <span class="n">bounds</span> <span class="o">=</span> <span class="n">bounds_from_last_device</span><span class="p">(</span><span class="n">last_device</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ici_mesh_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dcn_mesh_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">device_mesh</span> <span class="o">=</span> <span class="n">create_hybrid_device_mesh</span><span class="p">(</span>
            <span class="n">ici_mesh_shape</span><span class="p">,</span>
            <span class="n">dcn_mesh_shape</span><span class="p">,</span>
            <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">multi_slice_global_mesh</span> <span class="o">=</span> <span class="n">Mesh</span><span class="p">(</span><span class="n">device_mesh</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;replica&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">])</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s1">&#39;multi_slice_global_mesh axis_names: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span>
            <span class="n">multi_slice_global_mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s1">&#39;multi_slice_global_mesh devices: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">multi_slice_global_mesh</span><span class="o">.</span><span class="n">devices</span>
        <span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s1">&#39;multi_slice_global_mesh devices shape: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span>
            <span class="n">multi_slice_global_mesh</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">multi_slice_global_mesh</span>

    <span class="k">if</span> <span class="n">model_parallel_submesh</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">get_mesh</span><span class="p">(</span><span class="n">model_parallel_submesh</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;cpu&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">get_cpu_mesh</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;gpu&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">get_gpu_mesh</span><span class="p">(</span><span class="n">num_partitions</span><span class="p">)</span>

    <span class="n">mps</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">device_kind</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;TPU v2&#39;</span><span class="p">,</span> <span class="s1">&#39;TPU v3&#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">num_partitions</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">num_partitions</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">num_partitions</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">num_partitions</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
            <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">num_partitions</span> <span class="o">==</span> <span class="mi">16</span><span class="p">:</span>
            <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># assume the use of megacore on TPU v4</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">device_kind</span> <span class="o">==</span> <span class="s1">&#39;TPU v4&#39;</span> <span class="ow">or</span>
          <span class="n">device_kind</span> <span class="o">==</span> <span class="s1">&#39;TPU v4 lite&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">num_partitions</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">num_partitions</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">num_partitions</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">4</span><span class="p">:</span>
                <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">num_partitions</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">8</span><span class="p">:</span>
                <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">num_partitions</span> <span class="o">==</span> <span class="mi">16</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">16</span><span class="p">:</span>
                <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">8</span><span class="p">:</span>
                <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">4</span><span class="p">:</span>
                <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mps</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;No default mesh for this configuration: specify &#39;</span>
            <span class="s1">&#39;config.model_parallel_submesh explicitly. </span><span class="se">\n</span><span class="s1">&#39;</span>
            <span class="sa">f</span><span class="s1">&#39;Platform: </span><span class="si">{</span><span class="n">platform</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span>
            <span class="sa">f</span><span class="s1">&#39;Device kind: </span><span class="si">{</span><span class="n">device_kind</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span>
            <span class="sa">f</span><span class="s1">&#39;Num partitions: </span><span class="si">{</span><span class="n">num_partitions</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span>
            <span class="sa">f</span><span class="s1">&#39;Bounds: </span><span class="si">{</span><span class="n">bounds</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">get_mesh</span><span class="p">(</span><span class="n">mps</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.get_coords" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_coords</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Returns the coordinates of the given device.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_coords</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">HardwareMesh</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the coordinates of the given device.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="s1">&#39;coords&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">*</span><span class="n">device</span><span class="o">.</span><span class="n">coords</span><span class="p">,</span> <span class="n">device</span><span class="o">.</span><span class="n">core_on_chip</span>
    <span class="k">return</span> <span class="n">device</span><span class="o">.</span><span class="n">process_index</span><span class="p">,</span> <span class="n">device</span><span class="o">.</span><span class="n">id</span> <span class="o">%</span> <span class="n">jax</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.get_cpu_mesh" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_cpu_mesh</span><span class="p">()</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Trivial mesh for CPU Testing.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_cpu_mesh</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">Mesh</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Trivial mesh for CPU Testing.&quot;&quot;&quot;</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
        <span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">process_count</span><span class="p">(),</span> <span class="n">jax</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">()),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">():</span>
        <span class="n">devices</span><span class="p">[</span><span class="n">device</span><span class="o">.</span><span class="n">process_index</span><span class="p">,</span> <span class="n">device</span><span class="o">.</span><span class="n">id</span> <span class="o">%</span> <span class="n">jax</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">()]</span> <span class="o">=</span> <span class="n">device</span>
    <span class="k">return</span> <span class="n">Mesh</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.get_gpu_mesh" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_gpu_mesh</span><span class="p">(</span><span class="n">num_partitions</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Mesh for GPUs that preferentially places 'model' on NVLink.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_gpu_mesh</span><span class="p">(</span><span class="n">num_partitions</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mesh</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mesh for GPUs that preferentially places &#39;model&#39; on NVLink.&quot;&quot;&quot;</span>
    <span class="n">nvlink_size</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">()</span>
    <span class="n">dcn_size</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">process_count</span><span class="p">()</span>
    <span class="n">nvlink_mp</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_partitions</span><span class="p">,</span> <span class="n">nvlink_size</span><span class="p">)</span>
    <span class="n">nvlink_dp</span><span class="p">,</span> <span class="n">extra1</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">nvlink_size</span><span class="p">,</span> <span class="n">nvlink_mp</span><span class="p">)</span>
    <span class="n">dcn_mp</span><span class="p">,</span> <span class="n">extra2</span> <span class="o">=</span> <span class="nb">divmod</span><span class="p">(</span><span class="n">num_partitions</span><span class="p">,</span> <span class="n">nvlink_mp</span><span class="p">)</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">extra1</span> <span class="ow">or</span> <span class="n">extra2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;number of partitions on GPU must be a factor&#39;</span>
                                    <span class="s1">&#39; or multiple of the number of local devices&#39;</span><span class="p">)</span>
    <span class="n">dcn_dp</span> <span class="o">=</span> <span class="n">dcn_size</span> <span class="o">//</span> <span class="n">dcn_mp</span>

    <span class="n">devices</span> <span class="o">=</span> <span class="n">create_hybrid_device_mesh</span><span class="p">(</span>
        <span class="n">mesh_shape</span><span class="o">=</span><span class="p">[</span><span class="n">nvlink_dp</span><span class="p">,</span> <span class="n">nvlink_mp</span><span class="p">],</span>
        <span class="n">dcn_mesh_shape</span><span class="o">=</span><span class="p">[</span><span class="n">dcn_dp</span><span class="p">,</span> <span class="n">dcn_mp</span><span class="p">],</span>
        <span class="n">process_is_granule</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">global_mesh</span> <span class="o">=</span> <span class="n">Mesh</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">])</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;global_mesh axis_names: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">global_mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;global_mesh devices: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">global_mesh</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">global_mesh</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.get_mesh" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_mesh</span><span class="p">(</span><span class="n">model_parallel_submesh</span><span class="p">,</span> <span class="n">input_devices</span><span class="o">=</span><span class="p">(),</span> <span class="n">input_local_devices</span><span class="o">=</span><span class="p">(),</span> <span class="n">tile_by_host_if_needed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Construct an xmap/pjit Mesh for the given model-parallel submesh.</p>
<p>The resulting mesh has two resource axes: 'model', with the provided submesh
shape, and 'data', which covers the rest of the mesh.</p>
<p>Args:
  model_parallel_submesh: a HardwareMesh spec, namely (x,y,z,core) on TPU for
    a single model-parallel replica's "tile" in the physical device mesh. The
    first three elements (<code>x</code>, <code>y</code>, and <code>z</code>) should be factors of the pod
    slice; e.g., if you are using df_4x8, then <code>x</code> should be a factor of 4
    (one of 1, 2, 4), <code>y</code> should be a factor of 8 (one of 1, 2, 4, 8), and <code>z</code>
    must be 1, because TPU v3 slices are only 2D. <code>z</code> can be &gt;1 for TPU v4
    (and maybe later TPUs) that allow 3D slices. <code>core</code> is the number of cores
    to use from each TPU node. As communication is usually fastest inside the
    same node, if you need a tile of more than 1 core, then
    you should first increase <code>core</code>: e.g., for TPU v3, (1,1,1,2) is better
      than (2,1,1,1). To pick a good spec, try a few possible values until you
      get high TPU utilization.
  input_devices: the devices to use, will use jax.devices() if this is not
    set.
  input_local_devices: the local devices to use, will use jax.local_devices()
    if this is not set.
  tile_by_host_if_needed: JAX currently requires that the parts of any sharded
    array that are located on one host's local devices form a single
    contiguous slice. A best effort will be made to achieve this without
    "tiling" the device assignment over hosts (which can reduce XLA collective
    performance). If this flag is True, then the device assignment will be
    tiled over hosts if necessary to satisfy this constraint and create a
    buildable mesh; if false, mesh construction will fail instead.
  backend: get devices from the pinned backend, if specified. This is
    useful for explicitly specifying the devices other than relying on
    jax_platform_name.</p>
<p>Returns:
  A xmap / pjit Mesh containing the virtual device mesh with data, model axes.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_mesh</span><span class="p">(</span><span class="n">model_parallel_submesh</span><span class="p">:</span> <span class="n">HardwareMesh</span><span class="p">,</span>
             <span class="n">input_devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">JaxDevice</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
             <span class="n">input_local_devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">JaxDevice</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
             <span class="n">tile_by_host_if_needed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
             <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mesh</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct an xmap/pjit Mesh for the given model-parallel submesh.</span>

<span class="sd">    The resulting mesh has two resource axes: &#39;model&#39;, with the provided submesh</span>
<span class="sd">    shape, and &#39;data&#39;, which covers the rest of the mesh.</span>

<span class="sd">    Args:</span>
<span class="sd">      model_parallel_submesh: a HardwareMesh spec, namely (x,y,z,core) on TPU for</span>
<span class="sd">        a single model-parallel replica&#39;s &quot;tile&quot; in the physical device mesh. The</span>
<span class="sd">        first three elements (`x`, `y`, and `z`) should be factors of the pod</span>
<span class="sd">        slice; e.g., if you are using df_4x8, then `x` should be a factor of 4</span>
<span class="sd">        (one of 1, 2, 4), `y` should be a factor of 8 (one of 1, 2, 4, 8), and `z`</span>
<span class="sd">        must be 1, because TPU v3 slices are only 2D. `z` can be &gt;1 for TPU v4</span>
<span class="sd">        (and maybe later TPUs) that allow 3D slices. `core` is the number of cores</span>
<span class="sd">        to use from each TPU node. As communication is usually fastest inside the</span>
<span class="sd">        same node, if you need a tile of more than 1 core, then</span>
<span class="sd">        you should first increase `core`: e.g., for TPU v3, (1,1,1,2) is better</span>
<span class="sd">          than (2,1,1,1). To pick a good spec, try a few possible values until you</span>
<span class="sd">          get high TPU utilization.</span>
<span class="sd">      input_devices: the devices to use, will use jax.devices() if this is not</span>
<span class="sd">        set.</span>
<span class="sd">      input_local_devices: the local devices to use, will use jax.local_devices()</span>
<span class="sd">        if this is not set.</span>
<span class="sd">      tile_by_host_if_needed: JAX currently requires that the parts of any sharded</span>
<span class="sd">        array that are located on one host&#39;s local devices form a single</span>
<span class="sd">        contiguous slice. A best effort will be made to achieve this without</span>
<span class="sd">        &quot;tiling&quot; the device assignment over hosts (which can reduce XLA collective</span>
<span class="sd">        performance). If this flag is True, then the device assignment will be</span>
<span class="sd">        tiled over hosts if necessary to satisfy this constraint and create a</span>
<span class="sd">        buildable mesh; if false, mesh construction will fail instead.</span>
<span class="sd">      backend: get devices from the pinned backend, if specified. This is</span>
<span class="sd">        useful for explicitly specifying the devices other than relying on</span>
<span class="sd">        jax_platform_name.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A xmap / pjit Mesh containing the virtual device mesh with data, model axes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_devices</span> <span class="o">=</span> <span class="n">input_devices</span> <span class="ow">or</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
    <span class="n">input_local_devices</span> <span class="o">=</span> <span class="n">input_local_devices</span> <span class="ow">or</span> <span class="n">jax</span><span class="o">.</span><span class="n">local_devices</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">backend</span><span class="p">)</span>
    <span class="c1"># Sort input_devices based on coords, as backends might not return devices</span>
    <span class="c1"># in order.</span>
    <span class="n">last_device</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">input_devices</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">get_coords</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">last_input_local_devices</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">input_local_devices</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">get_coords</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;last device coords : </span><span class="si">%r</span><span class="se">\n</span><span class="s1">last local device coords: </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span>
                 <span class="n">get_coords</span><span class="p">(</span><span class="n">last_device</span><span class="p">),</span> <span class="n">get_coords</span><span class="p">(</span><span class="n">last_input_local_devices</span><span class="p">))</span>
    <span class="n">global_hardware_mesh</span> <span class="o">=</span> <span class="n">bounds_from_last_device</span><span class="p">(</span><span class="n">last_device</span><span class="p">)</span>
    <span class="n">mesh_ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_hardware_mesh</span><span class="p">)</span>
    <span class="n">local_hardware_mesh</span> <span class="o">=</span> <span class="n">bounds_from_last_device</span><span class="p">(</span><span class="n">last_input_local_devices</span><span class="p">)</span>
    <span class="n">mesh_err</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s1">&#39;each dimension of the model parallel submesh </span><span class="si">{</span><span class="n">model_parallel_submesh</span><span class="si">}</span><span class="s1"> &#39;</span>
        <span class="s1">&#39;must be a factor of the corresponding dimension of the global device &#39;</span>
        <span class="sa">f</span><span class="s1">&#39;mesh </span><span class="si">{</span><span class="n">global_hardware_mesh</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">g</span> <span class="o">%</span> <span class="n">m</span>
        <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">global_hardware_mesh</span><span class="p">,</span> <span class="n">model_parallel_submesh</span><span class="p">)),</span> <span class="n">mesh_err</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
        <span class="n">g</span> <span class="o">%</span> <span class="n">l</span> <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">global_hardware_mesh</span><span class="p">,</span> <span class="n">local_hardware_mesh</span><span class="p">))</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">global_hardware_mesh</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">object</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">input_devices</span><span class="p">:</span>
        <span class="n">device_coords</span> <span class="o">=</span> <span class="n">get_coords</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">devices</span><span class="p">[</span><span class="n">device_coords</span><span class="p">]</span> <span class="o">=</span> <span class="n">device</span>
    <span class="n">tile_by_host</span> <span class="o">=</span> <span class="n">tile_by_host_if_needed</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">global_hardware_mesh</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="c1"># enable contiguous local chunks without host tiling by making Z major</span>
        <span class="n">global_hardware_mesh</span> <span class="o">=</span> <span class="n">typing</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
                                           <span class="n">global_hardware_mesh</span><span class="p">)</span>
        <span class="n">model_parallel_submesh</span> <span class="o">=</span> <span class="n">typing</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
                                             <span class="n">model_parallel_submesh</span><span class="p">)</span>
        <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span><span class="p">,</span> <span class="n">gc</span> <span class="o">=</span> <span class="n">global_hardware_mesh</span>
        <span class="n">mx</span><span class="p">,</span> <span class="n">my</span><span class="p">,</span> <span class="n">mz</span><span class="p">,</span> <span class="n">mc</span> <span class="o">=</span> <span class="n">model_parallel_submesh</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">mx</span> <span class="o">==</span> <span class="n">gx</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">my</span> <span class="o">==</span> <span class="n">mz</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">mx</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">my</span> <span class="o">==</span> <span class="n">gy</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span>
                                                <span class="n">mz</span> <span class="o">==</span> <span class="n">gz</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;ensuring YZ plane has a Z-major device order&#39;</span><span class="p">)</span>
            <span class="c1"># YZ should be ZY</span>
            <span class="k">assert</span> <span class="n">mc</span> <span class="o">==</span> <span class="n">gc</span><span class="p">,</span> <span class="p">(</span><span class="n">mc</span><span class="p">,</span> <span class="n">gc</span><span class="p">)</span>
            <span class="n">global_hardware_mesh</span> <span class="o">=</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gz</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gc</span>
            <span class="n">model_parallel_submesh</span> <span class="o">=</span> <span class="n">mx</span><span class="p">,</span> <span class="n">mz</span><span class="p">,</span> <span class="n">my</span><span class="p">,</span> <span class="n">mc</span>
            <span class="n">devices</span> <span class="o">=</span> <span class="n">devices</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">tile_by_host</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">my</span> <span class="o">==</span> <span class="n">gy</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">mx</span> <span class="o">==</span> <span class="n">mz</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">my</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">mx</span> <span class="o">==</span> <span class="n">gx</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span>
                                                <span class="n">mz</span> <span class="o">==</span> <span class="n">gz</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;ensuring XZ plane has a Z-major device order&#39;</span><span class="p">)</span>
            <span class="c1"># XZ should be ZX</span>
            <span class="k">assert</span> <span class="n">mc</span> <span class="o">==</span> <span class="n">gc</span><span class="p">,</span> <span class="p">(</span><span class="n">mc</span><span class="p">,</span> <span class="n">gc</span><span class="p">)</span>
            <span class="n">global_hardware_mesh</span> <span class="o">=</span> <span class="n">gz</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gc</span>
            <span class="n">model_parallel_submesh</span> <span class="o">=</span> <span class="n">mz</span><span class="p">,</span> <span class="n">my</span><span class="p">,</span> <span class="n">mx</span><span class="p">,</span> <span class="n">mc</span>
            <span class="n">devices</span> <span class="o">=</span> <span class="n">devices</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">tile_by_host</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">tile_by_host</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s1">&#39;Tiling device assignment mesh by hosts, which may lead to &#39;</span>
            <span class="s1">&#39;reduced XLA collective performance. To avoid this, modify &#39;</span>
            <span class="s1">&#39;the model parallel submesh or run with more tasks per host.&#39;</span><span class="p">)</span>
        <span class="n">tile_err</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s1">&#39;to tile the mesh by hosts, each dimension of the model parallel &#39;</span>
            <span class="s1">&#39;submesh must be either a factor or a multiple of the corresponding &#39;</span>
            <span class="s1">&#39;dimension of the per-host submesh&#39;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">dh_dd_mh_md</span><span class="p">(</span><span class="n">g</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">m</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">l</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Split a global mesh dimension into four tiling components.</span>

<span class="sd">            Args:</span>
<span class="sd">              g: global mesh bounds dimension size</span>
<span class="sd">              m: model-parallel submesh bounds dimension size</span>
<span class="sd">              l: local submesh bounds dimension size</span>

<span class="sd">            Returns:</span>
<span class="sd">              The resulting tuple divides the dimension into the hosts component of</span>
<span class="sd">              the data-parallel submesh, the devices component of the data-parallel</span>
<span class="sd">              submesh, the hosts component of the model-parallel submesh, and the</span>
<span class="sd">              devices component of the model-parallel submesh.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">d</span> <span class="o">=</span> <span class="n">g</span> <span class="o">//</span> <span class="n">m</span>
            <span class="k">if</span> <span class="n">m</span> <span class="o">&gt;=</span> <span class="n">l</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">m</span> <span class="o">%</span> <span class="n">l</span><span class="p">,</span> <span class="n">tile_err</span>
                <span class="k">return</span> <span class="n">d</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span> <span class="o">//</span> <span class="n">l</span><span class="p">,</span> <span class="n">l</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="ow">not</span> <span class="n">l</span> <span class="o">%</span> <span class="n">m</span><span class="p">,</span> <span class="n">tile_err</span>
                <span class="k">return</span> <span class="n">d</span> <span class="o">//</span> <span class="p">(</span><span class="n">l</span> <span class="o">//</span> <span class="n">m</span><span class="p">),</span> <span class="n">l</span> <span class="o">//</span> <span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span>

        <span class="n">dh_dd_mh_md_tups</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">dh_dd_mh_md</span><span class="p">,</span> <span class="n">global_hardware_mesh</span><span class="p">,</span>
                               <span class="n">model_parallel_submesh</span><span class="p">,</span> <span class="n">local_hardware_mesh</span><span class="p">)</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="n">devices</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">s</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">dh_dd_mh_md_tups</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">t</span><span class="p">))</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="n">devices</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mesh_ndim</span><span class="p">)),</span>
                                    <span class="o">*</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mesh_ndim</span><span class="p">)),</span>
                                    <span class="o">*</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mesh_ndim</span><span class="p">)),</span>
                                    <span class="o">*</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">3</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mesh_ndim</span><span class="p">)))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model_data_tups</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">g</span> <span class="o">//</span> <span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">global_hardware_mesh</span><span class="p">,</span> <span class="n">model_parallel_submesh</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="n">devices</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">s</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">model_data_tups</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">t</span><span class="p">))</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="n">devices</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mesh_ndim</span><span class="p">)),</span>
                                    <span class="o">*</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mesh_ndim</span><span class="p">)))</span>
    <span class="c1"># reshape to (data, model)</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="n">devices</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">model_parallel_submesh</span><span class="p">))</span>
    <span class="n">global_mesh</span> <span class="o">=</span> <span class="n">Mesh</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">])</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;global_mesh axis_names: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">global_mesh</span><span class="o">.</span><span class="n">axis_names</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;global_mesh devices: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">global_mesh</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;global_mesh devices shape: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">global_mesh</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">global_mesh</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.global_mesh_defined" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">global_mesh_defined</span><span class="p">()</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Checks if global xmap/pjit mesh resource environment is defined.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">global_mesh_defined</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks if global xmap/pjit mesh resource environment is defined.&quot;&quot;&quot;</span>
    <span class="n">maps_env</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">maps</span><span class="o">.</span><span class="n">thread_resources</span><span class="o">.</span><span class="n">env</span>
    <span class="k">return</span> <span class="n">maps_env</span><span class="o">.</span><span class="n">physical_mesh</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">()</span>  <span class="c1"># pylint: disable=g-explicit-bool-comparison</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.standard_logical_axis_rules" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">standard_logical_axis_rules</span><span class="p">(</span><span class="n">activation_partitioning_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">parameter_partitioning_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">additional_rules</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Default sharding rules for T5X model in terms of logical axis names.</p>
<p>Args:
  activation_partitioning_dims: enables 2-D activation sharding when set to 2.
  parameter_partitioning_dims: enables 2-D parameter sharding when set to 2.
  additional_rules: additional rules (a sequence of tuples) that will be
    appended to the standard rules.</p>
<p>Returns:
  Sequence of logical axis rules</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">standard_logical_axis_rules</span><span class="p">(</span>
        <span class="n">activation_partitioning_dims</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">parameter_partitioning_dims</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">additional_rules</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LogicalAxisRules</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LogicalAxisRules</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Default sharding rules for T5X model in terms of logical axis names.</span>

<span class="sd">    Args:</span>
<span class="sd">      activation_partitioning_dims: enables 2-D activation sharding when set to 2.</span>
<span class="sd">      parameter_partitioning_dims: enables 2-D parameter sharding when set to 2.</span>
<span class="sd">      additional_rules: additional rules (a sequence of tuples) that will be</span>
<span class="sd">        appended to the standard rules.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Sequence of logical axis rules</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="s1">&#39;`activation_partitioning_dims` = </span><span class="si">%d</span><span class="s1">, `parameter_partitioning_dims` = </span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">,</span>
        <span class="n">activation_partitioning_dims</span><span class="p">,</span> <span class="n">parameter_partitioning_dims</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">activation_partitioning_dims</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">parameter_partitioning_dims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">rules</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;vocab&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;embed&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;mlp&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;heads&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;kv&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;joined_kv&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>  <span class="c1"># joined heads+kv dim in 2D attn param layouts</span>
        <span class="p">]</span>
    <span class="k">elif</span> <span class="n">activation_partitioning_dims</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">parameter_partitioning_dims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">rules</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;vocab&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;mlp&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;heads&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;kv&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;joined_kv&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;embed&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="k">elif</span> <span class="n">activation_partitioning_dims</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">parameter_partitioning_dims</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">rules</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;vocab&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;mlp&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;heads&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;kv&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;joined_kv&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;embed&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="k">elif</span> <span class="n">activation_partitioning_dims</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">parameter_partitioning_dims</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">rules</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;vocab&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;mlp&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;heads&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;kv&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;joined_kv&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;embed&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">),</span>
            <span class="p">(</span><span class="s1">&#39;embed&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;`activation_partitioning_dims` = </span><span class="si">{</span><span class="n">activation_partitioning_dims</span><span class="si">}</span><span class="s1"> &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;`parameter_partitioning_dims` = </span><span class="si">{</span><span class="n">parameter_partitioning_dims</span><span class="si">}</span><span class="s1"> &#39;</span>
            <span class="s1">&#39;is not supported.&#39;</span><span class="p">)</span>

    <span class="c1"># Add the common rules for the replicated logical axes names.</span>
    <span class="n">replicated_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;relpos_buckets&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;abspos_buckets&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;length&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;layers&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;stack&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;mlp_activations&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
    <span class="p">]</span>
    <span class="n">rules</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">replicated_rules</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">additional_rules</span><span class="p">:</span>
        <span class="n">rules</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">additional_rules</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">rules</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h2 id="src.fjformer.partition_utils.t5x_partitioning.with_sharding_constraint" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">with_sharding_constraint</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis_resources</span><span class="p">)</span></code>

</h2>


  <div class="doc doc-contents ">
  
      <p>Wrapper for lax.with_sharding_constraint, no-op on cpu or outside pjit.</p>

          <details class="quote">
            <summary>Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">with_sharding_constraint</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis_resources</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wrapper for lax.with_sharding_constraint, no-op on cpu or outside pjit.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;cpu&#39;</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">global_mesh_defined</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">with_sharding_constraint</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis_resources</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Erfan Zare Chavoshi-FJFormer
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.5cfa9459.min.js"></script>
      
    
  </body>
</html>