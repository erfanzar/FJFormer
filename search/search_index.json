{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FJFormer","text":"<p>Embark on a journey of paralleled/unparalleled computational prowess with FJFormer - an arsenal of custom Jax Flax Functions and Utils that elevate your AI endeavors to new heights!</p>"},{"location":"#overview","title":"Overview","text":"<p>FJFormer is a collection of functions and utilities that can help with various tasks when using Flax and JAX. It includes checkpoint savers, partitioning tools, and other helpful functions. The goal of FJFormer is to make your life easier when working with Flax and JAX. Whether you are training a new model, fine-tuning an existing one, or just exploring the capabilities of these powerful frameworks, FJFormer offers</p> <ul> <li>FlashAttention on <code>TPU/GPU</code> \ud83e\uddec</li> <li>BITComputations for 8,6,4 BIT Flax Models \ud83e\udd0f</li> <li>Smart Dataset Loading</li> <li>Built-in functions and Loss functions</li> <li>GPU-Pallas triton like implementation of <code>Softmax</code>, <code>FlashAttention</code>, <code>RMSNorm</code>, <code>LayerNorm</code></li> <li>Distributed and sharding Model Loaders and Checkpoint Savers</li> <li>Monitoring Utils for TPU/GPU/CPU memory <code>foot-print</code></li> <li>Special Optimizers with schedulers and Easy to Use</li> <li>Partitioning Utils</li> <li>LoRA with <code>XRapture</code> \ud83e\udd20</li> </ul> <p>and A lot of these features are fully documented so i gusse FJFormer has something to offer, and it's not just a Computation BackEnd for EasyDel.</p> <p>checkout for documentations here.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>FJFormer is an open-source project, and contributions are always welcome! If you have a feature request, bug report, or just want to help out with development, please check out our GitHub repository and feel free to submit a pull request or open an issue.</p> <p>Thank you for using FJFormer, and happy training!</p>"},{"location":"generated-bits-bits/","title":"bits.bits","text":""},{"location":"generated-bits-bits/#src.fjformer.bits.bits.matmul","title":"<code>matmul(a, b, *, precision=None, dot_general=lax.dot_general)</code>","text":"<p>Quantized jax.numpy.matmul.</p> <p>Args:   a: Left-hand side of the matmul.   b: Right-hand side of the matmul.   precision: Indicates precision of a and b.   dot_general: lax.dot_general by default. To use quantized matmul, the     wrapper of q_dot_general in which TQs and <code>train</code> flag are provided     should be passed into this function.</p> <p>Returns:   An array containing the result with the same dtype as 'a' and 'b'.</p> Source code in <code>src/fjformer/bits/bits.py</code> <pre><code>def matmul(\n        a: jnp.ndarray,\n        b: jnp.ndarray,\n        *,\n        precision=None,\n        dot_general=lax.dot_general\n) -&gt; jnp.ndarray:\n    \"\"\"Quantized jax.numpy.matmul.\n\n    Args:\n      a: Left-hand side of the matmul.\n      b: Right-hand side of the matmul.\n      precision: Indicates precision of a and b.\n      dot_general: lax.dot_general by default. To use quantized matmul, the\n        wrapper of q_dot_general in which TQs and `train` flag are provided\n        should be passed into this function.\n\n    Returns:\n      An array containing the result with the same dtype as 'a' and 'b'.\n    \"\"\"\n    arraylike = (jax.Array, np.ndarray)\n    if not isinstance(a, arraylike) or not isinstance(b, arraylike):\n        raise TypeError(f\"matmul requires array-like arguments, got {a} and {b}\")\n    for i, x in enumerate((a, b)):\n        if ndim(x) &lt; 1:\n            msg = (f\"matmul input operand {i} must have ndim at least 1, \"\n                   f\"but it has ndim {ndim(x)}\")\n            raise ValueError(msg)\n\n    dtype = jnp.result_type(a.dtype, b.dtype)\n    a = a.astype(dtype)\n    b = b.astype(dtype)\n\n    a_is_mat, b_is_mat = (ndim(a) &gt; 1), (ndim(b) &gt; 1)\n    a_batch_dims = shape(a)[:-2] if a_is_mat else ()\n    b_batch_dims = shape(b)[:-2] if b_is_mat else ()\n    num_batch_dims = _max(len(a_batch_dims), len(b_batch_dims))\n    a_batch_dims = (None,) * (num_batch_dims - len(a_batch_dims)) + a_batch_dims\n    b_batch_dims = (None,) * (num_batch_dims - len(b_batch_dims)) + b_batch_dims\n\n    # Dimensions to squeeze from the inputs.\n    a_squeeze = []\n    b_squeeze = []\n\n    # Positions of batch dimensions in squeezed inputs.\n    a_batch = []\n    b_batch = []\n\n    # Desired index in final output of each kind of dimension, in the order that\n    # q_dot_general will emit them.\n    idx_batch = []\n    idx_a_other = []  # other = non-batch, non-contracting.\n    idx_b_other = []\n    for i, (ba, bb) in enumerate(zip(a_batch_dims, b_batch_dims)):\n        if ba is None:\n            idx_b_other.append(i)\n        elif bb is None:\n            idx_a_other.append(i)\n        elif core.symbolic_equal_dim(ba, 1):\n            idx_b_other.append(i)\n            a_squeeze.append(len(idx_batch) + len(idx_a_other) + len(a_squeeze))\n        elif core.symbolic_equal_dim(bb, 1):\n            idx_a_other.append(i)\n            b_squeeze.append(len(idx_batch) + len(idx_b_other) + len(b_squeeze))\n        elif core.symbolic_equal_dim(ba, bb):\n            a_batch.append(len(idx_batch) + len(idx_a_other))\n            b_batch.append(len(idx_batch) + len(idx_b_other))\n            idx_batch.append(i)\n        else:\n            raise ValueError(\"Incompatible shapes for matmul arguments: {} and {}\"\n                             .format(shape(a), shape(b)))\n\n    if a_is_mat:\n        idx_a_other.append(num_batch_dims)\n    if b_is_mat:\n        idx_b_other.append(num_batch_dims + a_is_mat)\n    perm = np.argsort(np.concatenate([idx_batch, idx_a_other, idx_b_other]))\n\n    a = lax.squeeze(a, tuple(a_squeeze))\n    b = lax.squeeze(b, tuple(b_squeeze))\n    out = dot_general(\n        a,\n        b, (((ndim(a) - 1,), (ndim(b) - 1 - b_is_mat,)), (a_batch, b_batch)),\n        precision=precision)\n    return lax.transpose(out, perm)\n</code></pre>"},{"location":"generated-bits-bits/#src.fjformer.bits.bits.matmul_true_int8","title":"<code>matmul_true_int8(lhs, rhs)</code>","text":"<p>The matmul_true_int8 function is a helper function that takes in two int8 arrays and returns the result of their matrix multiplication.</p> <p>Parameters:</p> Name Type Description Default <code>lhs</code> <p>Specify the left hand side of the matrix multiplication</p> required <code>rhs</code> <p>Specify the right-hand side of the matrix multiplication</p> required <p>Returns:</p> Type Description <p>A matrix with elements of type int32</p> Source code in <code>src/fjformer/bits/bits.py</code> <pre><code>def matmul_true_int8(lhs, rhs):\n    \"\"\"\n    The matmul_true_int8 function is a helper function that takes in two int8 arrays and returns the result of their matrix multiplication.\n\n    :param lhs: Specify the left hand side of the matrix multiplication\n    :param rhs: Specify the right-hand side of the matrix multiplication\n    :return: A matrix with elements of type int32\n\n    \"\"\"\n    assert lhs.dtype == jnp.int8\n    assert rhs.dtype == jnp.int8\n    result = jnp.matmul(lhs, rhs, preferred_element_type=jnp.int32)\n    assert result.dtype == jnp.int32\n    return result\n</code></pre>"},{"location":"generated-bits-bits/#src.fjformer.bits.bits.q_matmul_int8","title":"<code>q_matmul_int8(a, w)</code>","text":"<p>The q_matmul_int8 function performs a quantized matrix multiplication.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <p>Calibrate the input data</p> required <code>w</code> <p>Store the weights of the neural network</p> required <p>Returns:</p> Type Description <p>A float32 array, which is the result of an int8 matrix multiplication</p> Source code in <code>src/fjformer/bits/bits.py</code> <pre><code>def q_matmul_int8(a, w):\n    \"\"\"\n    The q_matmul_int8 function performs a quantized matrix multiplication.\n\n    :param a: Calibrate the input data\n    :param w: Store the weights of the neural network\n    :return: A float32 array, which is the result of an int8 matrix multiplication\n\n    \"\"\"\n\n    # Calibration. Calibration function is also customizable and injectable.\n    a_s = 127 / jnp.max(jnp.abs(a), axis=1, keepdims=True)\n    w_s = 127 / jnp.max(jnp.abs(w), axis=0, keepdims=True)\n\n    # int8 matmul with int32 accumulator\n    result = matmul_true_int8(quant_int8(a * a_s), quant_int8(w * w_s)) / (a_s * w_s)\n\n    return result\n</code></pre>"},{"location":"generated-bits-bits/#src.fjformer.bits.bits.quant_int8","title":"<code>quant_int8(x)</code>","text":"<p>The quant_int8 function takes a floating point number and rounds it to the nearest integer. If the rounded value is outside the range [-127, 127], then it clips it to that range. Finally, we cast this clipped integer into an int8 type.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Pass the input to the function</p> required <p>Returns:</p> Type Description <p>The rounded and clipped values</p> Source code in <code>src/fjformer/bits/bits.py</code> <pre><code>def quant_int8(x):\n    \"\"\"\n    The quant_int8 function takes a floating point number and rounds it to the nearest integer.\n    If the rounded value is outside the range [-127, 127], then it clips it to that range.\n    Finally, we cast this clipped integer into an int8 type.\n\n    :param x: Pass the input to the function\n    :return: The rounded and clipped values\n\n    \"\"\"\n    return jnp.clip(jnp.round(x), -127, 127).astype(jnp.int8)\n</code></pre>"},{"location":"generated-bits-calibration/","title":"bits.calibration","text":"<p>Quantization calibration methods.</p>"},{"location":"generated-bits-calibration/#src.fjformer.bits.calibration.AbsMaxCalibration","title":"<code>AbsMaxCalibration</code>","text":"<p>             Bases: <code>Calibration</code></p> <p>Simple max(abs(x)) calibration.</p> Source code in <code>src/fjformer/bits/calibration.py</code> <pre><code>@flax.struct.dataclass\nclass AbsMaxCalibration(Calibration):\n    \"\"\"Simple max(abs(x)) calibration.\"\"\"\n\n    def get_bound(self, x, shared_axes) -&gt; jnp.ndarray:\n        \"\"\"Calibration.\"\"\"\n        msg = 'Perhaps you are using fake_quant and forgot to set them.'\n        assert shared_axes is not None, msg\n\n        # NOTE: If you want to clip, consider using clip and clip_gradient in\n        # int_numerics.IntNumerics.\n        abs_max = jnp.max(jnp.abs(x), axis=shared_axes, keepdims=True)\n        abs_max = jnp.where(abs_max == 0.0, jnp.ones_like(abs_max), abs_max)\n        return abs_max\n</code></pre>"},{"location":"generated-bits-calibration/#src.fjformer.bits.calibration.AbsMaxCalibration.get_bound","title":"<code>get_bound(x, shared_axes)</code>","text":"<p>Calibration.</p> Source code in <code>src/fjformer/bits/calibration.py</code> <pre><code>def get_bound(self, x, shared_axes) -&gt; jnp.ndarray:\n    \"\"\"Calibration.\"\"\"\n    msg = 'Perhaps you are using fake_quant and forgot to set them.'\n    assert shared_axes is not None, msg\n\n    # NOTE: If you want to clip, consider using clip and clip_gradient in\n    # int_numerics.IntNumerics.\n    abs_max = jnp.max(jnp.abs(x), axis=shared_axes, keepdims=True)\n    abs_max = jnp.where(abs_max == 0.0, jnp.ones_like(abs_max), abs_max)\n    return abs_max\n</code></pre>"},{"location":"generated-bits-calibration/#src.fjformer.bits.calibration.ConstantCalibration","title":"<code>ConstantCalibration</code>","text":"<p>             Bases: <code>Calibration</code></p> Source code in <code>src/fjformer/bits/calibration.py</code> <pre><code>@flax.struct.dataclass\nclass ConstantCalibration(Calibration):\n    bound: Union[jnp.ndarray, float]\n\n    def get_bound(self, x, shared_axes) -&gt; jnp.ndarray:\n        \"\"\"Calibration.\"\"\"\n        del shared_axes\n        assert self.bound &gt; 0, 'Bound should be positive.'\n        return jnp.asarray(self.bound).reshape((1,) * len(x.shape))\n</code></pre>"},{"location":"generated-bits-calibration/#src.fjformer.bits.calibration.ConstantCalibration.get_bound","title":"<code>get_bound(x, shared_axes)</code>","text":"<p>Calibration.</p> Source code in <code>src/fjformer/bits/calibration.py</code> <pre><code>def get_bound(self, x, shared_axes) -&gt; jnp.ndarray:\n    \"\"\"Calibration.\"\"\"\n    del shared_axes\n    assert self.bound &gt; 0, 'Bound should be positive.'\n    return jnp.asarray(self.bound).reshape((1,) * len(x.shape))\n</code></pre>"},{"location":"generated-bits-config/","title":"bits.config","text":"<p>Configuration dataclasses.</p>"},{"location":"generated-bits-config/#src.fjformer.bits.config.DotGeneral","title":"<code>DotGeneral</code>  <code>dataclass</code>","text":"<p>Configuration of quantization of dot_general and its gradients.</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>@dataclasses.dataclass(slots=True)\nclass DotGeneral:\n    \"\"\"Configuration of quantization of dot_general and its gradients.\"\"\"\n\n    fwd: DotGeneralRaw\n    dlhs: DotGeneralRaw\n    drhs: DotGeneralRaw\n\n    @classmethod\n    def make(cls, *args, **kwargs) -&gt; 'DotGeneral':\n        return dot_general_make(*args, **kwargs)\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.DotGeneralRaw","title":"<code>DotGeneralRaw</code>  <code>dataclass</code>","text":"<p>Configuration of quantization of one dot_general without gradient.</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>@dataclasses.dataclass(slots=True)\nclass DotGeneralRaw:\n    \"\"\"Configuration of quantization of one dot_general without gradient.\"\"\"\n\n    lhs: Tensor\n    rhs: Tensor\n    dg_accumulator_dtype: Optional[DType]\n    local_aqt: Optional[LocalQ]\n\n    @classmethod\n    def make(cls, *args, **kwargs) -&gt; 'DotGeneralRaw':\n        \"\"\"\n        The make function is a factory function that creates an instance of the DotGeneralRaw class.\n\n        :param cls: Create a new instance of the class\n        :param args: Send a non-keyworded variable length argument list to the function\n        :param kwargs: Pass a variable number of keyword arguments to the function\n        :return: A dotgeneralraw object\n        \"\"\"\n        return dot_general_raw_make(*args, **kwargs)\n\n    @classmethod\n    def make_conv_general_dilated(cls, *args, **kwargs) -&gt; 'DotGeneralRaw':\n        \"\"\"\n        The make_conv_general_dilated function is a wrapper for the conv_general_dilated_make function.\n        It allows us to use the make function in our DotGeneralRaw class, which we can then use as a\n        component of our network. The make function takes in arguments that are used by the\n        conv_general_dilated_make function to create an instance of DotGeneralRaw.\n\n        :param cls: Create a new instance of the class\n        :param args: Send a non-keyworded variable length argument list to the function\n        :param kwargs: Pass a variable number of keyword arguments to a function\n        :return: A dotgeneralraw object\n        \"\"\"\n        return conv_general_dilated_make(*args, **kwargs)\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.DotGeneralRaw.make","title":"<code>make(*args, **kwargs)</code>  <code>classmethod</code>","text":"<p>The make function is a factory function that creates an instance of the DotGeneralRaw class.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Create a new instance of the class</p> required <code>args</code> <p>Send a non-keyworded variable length argument list to the function</p> <code>()</code> <code>kwargs</code> <p>Pass a variable number of keyword arguments to the function</p> <code>{}</code> <p>Returns:</p> Type Description <code>DotGeneralRaw</code> <p>A dotgeneralraw object</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>@classmethod\ndef make(cls, *args, **kwargs) -&gt; 'DotGeneralRaw':\n    \"\"\"\n    The make function is a factory function that creates an instance of the DotGeneralRaw class.\n\n    :param cls: Create a new instance of the class\n    :param args: Send a non-keyworded variable length argument list to the function\n    :param kwargs: Pass a variable number of keyword arguments to the function\n    :return: A dotgeneralraw object\n    \"\"\"\n    return dot_general_raw_make(*args, **kwargs)\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.DotGeneralRaw.make_conv_general_dilated","title":"<code>make_conv_general_dilated(*args, **kwargs)</code>  <code>classmethod</code>","text":"<p>The make_conv_general_dilated function is a wrapper for the conv_general_dilated_make function. It allows us to use the make function in our DotGeneralRaw class, which we can then use as a component of our network. The make function takes in arguments that are used by the conv_general_dilated_make function to create an instance of DotGeneralRaw.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Create a new instance of the class</p> required <code>args</code> <p>Send a non-keyworded variable length argument list to the function</p> <code>()</code> <code>kwargs</code> <p>Pass a variable number of keyword arguments to a function</p> <code>{}</code> <p>Returns:</p> Type Description <code>DotGeneralRaw</code> <p>A dotgeneralraw object</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>@classmethod\ndef make_conv_general_dilated(cls, *args, **kwargs) -&gt; 'DotGeneralRaw':\n    \"\"\"\n    The make_conv_general_dilated function is a wrapper for the conv_general_dilated_make function.\n    It allows us to use the make function in our DotGeneralRaw class, which we can then use as a\n    component of our network. The make function takes in arguments that are used by the\n    conv_general_dilated_make function to create an instance of DotGeneralRaw.\n\n    :param cls: Create a new instance of the class\n    :param args: Send a non-keyworded variable length argument list to the function\n    :param kwargs: Pass a variable number of keyword arguments to a function\n    :return: A dotgeneralraw object\n    \"\"\"\n    return conv_general_dilated_make(*args, **kwargs)\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.Tensor","title":"<code>Tensor</code>  <code>dataclass</code>","text":"<p>Configuration of quantization of one tensor or one side of tensor op.</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>@dataclasses.dataclass(slots=True)\nclass Tensor:\n    \"\"\"Configuration of quantization of one tensor or one side of tensor op.\"\"\"\n\n    numerics: numerics.QNumerics\n    calib_shared_axes: Optional[list[int]]\n    scale_stop_grad: bool\n    # noise+clip+round\n    # We apply gradient of clip_and_round in bwd pass.\n    calibration: calibration.Calibration\n    # Round up the calibration to power of 2 (po2).\n    po2_scale: bool\n    use_fake_quant: bool\n    # Controls at what value of input tensor should be used.\n    # Setting it to True, but not quantizing fwd pass will assert-fail.\n    use_fwd_quant: Optional[bool]\n    # Operations for retrieving or storing quantized tensors and their scales\n    # TODO(yichizh): Factor out auxilliary dataclasses into a separate file.\n    # The following dtype Any should be q_dot_general.QTensor but that triggers\n    # recursive importing\n    preprocess: Optional[Callable[[Optional[Any]], Optional[Any]]]\n\n    @classmethod\n    def make(cls, *args, **kwargs) -&gt; 'Tensor':\n        return tensor_make(*args, **kwargs)\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.config_v3","title":"<code>config_v3(*, fwd_bits=8, dlhs_bits=8, drhs_bits=None, use_dummy_static_bound=False, rng_type='jax.uniform', dlhs_local_aqt=None, drhs_local_aqt=None, fwd_accumulator_dtype=jnp.int32, dlhs_accumulator_dtype=jnp.int32, drhs_accumulator_dtype=None)</code>","text":"<p>The config_v3 function is a helper function that configures the DotGeneral object. It takes in keyword arguments and returns a configured DotGeneral object. The following are the keyword arguments:</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <p>Indicate that all the following parameters are keyword-only</p> required <code>fwd_bits</code> <code>Optional[int]</code> <p>Optional[int]: Set the number of bits used for forward pass</p> <code>8</code> <code>dlhs_bits</code> <code>Optional[int]</code> <p>Optional[int]: Set the number of bits for the</p> <code>8</code> <code>drhs_bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits</p> <code>None</code> <code>use_dummy_static_bound</code> <code>bool</code> <p>bool: Set the static bound to 1</p> <code>False</code> <code>rng_type</code> <code>str</code> <p>str: Specify the random number generator</p> <code>'jax.uniform'</code> <code>dlhs_local_aqt</code> <code>Optional[LocalQ]</code> <p>Optional[LocalQ]: Set the local quantization of the dlhs</p> <code>None</code> <code>drhs_local_aqt</code> <code>Optional[LocalQ]</code> <p>Optional[LocalQ]: Set the local quantization</p> <code>None</code> <code>fwd_accumulator_dtype</code> <code>...</code> <p>...: Specify the accumulator dtype for the forward pass</p> <code>int32</code> <code>dlhs_accumulator_dtype</code> <code>...</code> <p>...: Specify the accumulator dtype for the gradient</p> <code>int32</code> <code>drhs_accumulator_dtype</code> <code>...</code> <p>...: Specify the data type of the accumulator in drhs</p> <code>None</code> <code></code> <p>Specify the number of bits used for quantization</p> required <p>Returns:</p> Type Description <code>DotGeneral</code> <p>A dotgeneral object</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>def config_v3(\n        *,\n        fwd_bits: Optional[int] = 8,\n        dlhs_bits: Optional[int] = 8,\n        drhs_bits: Optional[int] = None,\n        use_dummy_static_bound: bool = False,\n        rng_type: str = 'jax.uniform',  # 'custom-1'\n        dlhs_local_aqt: Optional[LocalQ] = None,\n        drhs_local_aqt: Optional[LocalQ] = None,\n        fwd_accumulator_dtype: ... = jnp.int32,\n        dlhs_accumulator_dtype: ... = jnp.int32,\n        drhs_accumulator_dtype: ... = None,\n) -&gt; DotGeneral:\n    \"\"\"\n    The config_v3 function is a helper function that configures the DotGeneral\n    object. It takes in keyword arguments and returns a configured DotGeneral object.\n    The following are the keyword arguments:\n\n    :param *: Indicate that all the following parameters are keyword-only\n    :param fwd_bits: Optional[int]: Set the number of bits used for forward pass\n    :param dlhs_bits: Optional[int]: Set the number of bits for the\n    :param drhs_bits: Optional[int]: Specify the number of bits\n    :param use_dummy_static_bound: bool: Set the static bound to 1\n    :param rng_type: str: Specify the random number generator\n    :param dlhs_local_aqt: Optional[LocalQ]: Set the local quantization of the dlhs\n    :param drhs_local_aqt: Optional[LocalQ]: Set the local quantization\n    :param fwd_accumulator_dtype: ...: Specify the accumulator dtype for the forward pass\n    :param dlhs_accumulator_dtype: ...: Specify the accumulator dtype for the gradient\n    :param drhs_accumulator_dtype: ...: Specify the data type of the accumulator in drhs\n    :param : Specify the number of bits used for quantization\n    :return: A dotgeneral object\n    \"\"\"\n    fwd = dot_general_raw_make(fwd_bits, fwd_bits)\n    dlhs = dot_general_raw_make(dlhs_bits, dlhs_bits, local_aqt=dlhs_local_aqt)\n    drhs = dot_general_raw_make(drhs_bits, drhs_bits, local_aqt=drhs_local_aqt)\n    cfg = DotGeneral(fwd=fwd, dlhs=dlhs, drhs=drhs)\n\n    cfg.dlhs.rhs.use_fwd_quant = False\n    cfg.drhs.rhs.use_fwd_quant = False\n\n    # Typically we have (but I don't know if it is guraranteed):\n    # - vjp_lhs_stochastic_rounding is referring to the gradient and\n    # - vjp_rhs_stochastic_rounding is referring to the activations/weights.\n    set_stochastic_rounding(\n        cfg,\n        vjp_lhs_stochastic_rounding=True,\n        vjp_rhs_stochastic_rounding=False,\n        implementation=rng_type,\n    )\n\n    if use_dummy_static_bound:\n        set_static_bound(cfg, 1.0)\n\n    set_accumulator_dtype(\n        cfg,\n        fwd_dtype=fwd_accumulator_dtype,\n        dlhs_dtype=dlhs_accumulator_dtype,\n        drhs_dtype=drhs_accumulator_dtype,\n    )\n    return cfg\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.conv_general_dilated_make","title":"<code>conv_general_dilated_make(spatial_dimensions=2, lhs_bits=None, rhs_bits=None)</code>","text":"<p>Create quantization config conv_general_dilated.</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>def conv_general_dilated_make(\n        spatial_dimensions=2,\n        lhs_bits: Optional[int] = None,\n        rhs_bits: Optional[int] = None,\n) -&gt; 'DotGeneralRaw':\n    \"\"\"Create quantization config conv_general_dilated.\"\"\"\n    config = dot_general_raw_make(lhs_bits, rhs_bits)\n    # Hardcoding flax assumptions.\n    if config.lhs:\n        config.lhs.calib_shared_axes = list(range(1, spatial_dimensions + 2))\n    if config.rhs:\n        config.rhs.calib_shared_axes = list(range(0, spatial_dimensions + 2 - 1))\n    return config\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.dot_general_make","title":"<code>dot_general_make(lhs_bits=None, rhs_bits=None, bwd_bits=None, use_fwd_quant=True, dlhs_local_aqt=None, drhs_local_aqt=None)</code>","text":"<p>Create quantization configs for input matrices to a matmul.</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>def dot_general_make(\n        lhs_bits: Optional[int] = None,\n        rhs_bits: Optional[int] = None,\n        bwd_bits: Optional[int] = None,\n        use_fwd_quant: bool = True,\n        dlhs_local_aqt=None,\n        drhs_local_aqt=None,\n) -&gt; 'DotGeneral':\n    \"\"\"Create quantization configs for input matrices to a matmul.\"\"\"\n    fwd = dot_general_raw_make(lhs_bits, rhs_bits)\n    dlhs = dot_general_raw_make(bwd_bits, bwd_bits, local_aqt=dlhs_local_aqt)\n    drhs = dot_general_raw_make(bwd_bits, bwd_bits, local_aqt=drhs_local_aqt)\n    cfg = DotGeneral(fwd=fwd, dlhs=dlhs, drhs=drhs)\n\n    # Surprising: lhs quantization determines what drhs can do.\n    if lhs_bits is not None:\n        # Only rhs is accepting MultiTensor.\n        cfg.drhs.rhs.use_fwd_quant = use_fwd_quant\n    if rhs_bits is not None:\n        cfg.dlhs.rhs.use_fwd_quant = use_fwd_quant\n    return cfg\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.dot_general_raw_make","title":"<code>dot_general_raw_make(lhs_bits=None, rhs_bits=None, local_aqt=None)</code>","text":"<p>The dot_general_raw_make function is a helper function that creates a DotGeneralRaw object.</p> <p>Parameters:</p> Name Type Description Default <code>lhs_bits</code> <p>Determine the dtype of the lhs tensor</p> <code>None</code> <code>rhs_bits</code> <p>Determine the dtype of the accumulator</p> <code>None</code> <code>local_aqt</code> <p>Determine the type of accumulator used</p> <code>None</code> <code></code> <p>Determine the dtype of the accumulator</p> required <p>Returns:</p> Type Description <code>DotGeneralRaw</code> <p>A dotgeneralraw object</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>def dot_general_raw_make(\n        lhs_bits=None,\n        rhs_bits=None,\n        local_aqt=None,\n) -&gt; 'DotGeneralRaw':\n    \"\"\"\n    The dot_general_raw_make function is a helper function that creates a DotGeneralRaw object.\n\n    :param lhs_bits: Determine the dtype of the lhs tensor\n    :param rhs_bits: Determine the dtype of the accumulator\n    :param local_aqt: Determine the type of accumulator used\n    :param : Determine the dtype of the accumulator\n    :return: A dotgeneralraw object\n    \"\"\"\n    lhs_cfg = tensor_make(lhs_bits)\n    rhs_cfg = tensor_make(rhs_bits)\n\n    # Binary uses 0.5 right now.\n    if (\n            lhs_bits is not None\n            and rhs_bits is not None\n            and 2 &lt;= lhs_bits &lt;= 8\n            and 2 &lt;= rhs_bits &lt;= 8\n    ):\n        dg_accumulator_dtype = jnp.int32\n    else:\n        dg_accumulator_dtype = None\n\n    return DotGeneralRaw(\n        lhs=lhs_cfg,\n        rhs=rhs_cfg,\n        dg_accumulator_dtype=dg_accumulator_dtype,\n        local_aqt=local_aqt,\n    )\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.fully_quantized","title":"<code>fully_quantized(*, fwd_bits=8, bwd_bits=8, use_fwd_quant=True, use_stochastic_rounding=True, vjp_lhs_stochastic_rounding=None, vjp_rhs_stochastic_rounding=None, use_dummy_static_bound=False, dlhs_local_aqt=None, drhs_local_aqt=None)</code>","text":"<p>The fully_quantized function is a helper function that allows you to quickly configure the dot_general primitive with all of its quantization parameters. It takes in keyword arguments for each of the quantization parameters, and returns a DotGeneral configuration object. The following table shows what each parameter does:</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <p>Indicate that all the parameters are keyword-only</p> required <code>fwd_bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits used for forward quantization</p> <code>8</code> <code>bwd_bits</code> <code>Optional[int]</code> <p>Optional[int]: Set the number of bits used for backpropagation</p> <code>8</code> <code>use_fwd_quant</code> <code>bool</code> <p>bool: Control whether to quantize the</p> <code>True</code> <code>use_stochastic_rounding</code> <code>Optional[bool]</code> <p>Optional[bool]: Enable stochastic rounding</p> <code>True</code> <code>vjp_lhs_stochastic_rounding</code> <code>Optional[bool]</code> <p>Optional[bool]: Ensure that we don't mix</p> <code>None</code> <code>vjp_rhs_stochastic_rounding</code> <code>Optional[bool]</code> <p>Optional[bool]:</p> <code>None</code> <code>use_dummy_static_bound</code> <code>bool</code> <p>bool: Set the static bound to 1</p> <code>False</code> <code>dlhs_local_aqt</code> <code>Optional[LocalQ]</code> <p>Optional[LocalQ]: Specify the quantization scheme for the left-hand side of a matrix multiplication</p> <code>None</code> <code>drhs_local_aqt</code> <code>Optional[LocalQ]</code> <p>Optional[LocalQ]: Specify the quantization scheme for the right hand side of a matrix multiplication</p> <code>None</code> <code></code> <p>Set the number of bits used for forward and backward pass</p> required <p>Returns:</p> Type Description <code>DotGeneral</code> <p>A dotgeneral object, which is a</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>def fully_quantized(\n        *,\n        fwd_bits: Optional[int] = 8,\n        bwd_bits: Optional[int] = 8,\n        use_fwd_quant: bool = True,\n        use_stochastic_rounding: Optional[bool] = True,\n        # Typically we have (but it's a caller's responsibility to check):\n        # - vjp_lhs_stochastic_rounding is referring to the gradient and\n        # - vjp_rhs_stochastic_rounding is referring to the activations/weights.\n        vjp_lhs_stochastic_rounding: Optional[bool] = None,\n        vjp_rhs_stochastic_rounding: Optional[bool] = None,\n        # The dummy static bound flag is temporary, for performance benchmarking.\n        use_dummy_static_bound: bool = False,\n        dlhs_local_aqt: Optional[LocalQ] = None,\n        drhs_local_aqt: Optional[LocalQ] = None,\n) -&gt; DotGeneral:\n    \"\"\"\n    The fully_quantized function is a helper function that allows you to quickly\n    configure the dot_general primitive with all of its quantization parameters.\n    It takes in keyword arguments for each of the quantization parameters, and returns\n    a DotGeneral configuration object. The following table shows what each parameter does:\n\n    :param *: Indicate that all the parameters are keyword-only\n    :param fwd_bits: Optional[int]: Specify the number of bits used for forward quantization\n    :param bwd_bits: Optional[int]: Set the number of bits used for backpropagation\n    :param use_fwd_quant: bool: Control whether to quantize the\n    :param use_stochastic_rounding: Optional[bool]: Enable stochastic rounding\n    :param vjp_lhs_stochastic_rounding: Optional[bool]: Ensure that we don't mix\n    :param vjp_rhs_stochastic_rounding: Optional[bool]:\n    :param use_dummy_static_bound: bool: Set the static bound to 1\n    :param dlhs_local_aqt: Optional[LocalQ]: Specify the quantization scheme for the left-hand side of a matrix multiplication\n    :param drhs_local_aqt: Optional[LocalQ]: Specify the quantization scheme for the right hand side of a matrix multiplication\n    :param : Set the number of bits used for forward and backward pass\n    :return: A dotgeneral object, which is a\n    \"\"\"\n    cfg = dot_general_make(\n        lhs_bits=fwd_bits,\n        rhs_bits=fwd_bits,\n        bwd_bits=bwd_bits,\n        use_fwd_quant=use_fwd_quant,\n        dlhs_local_aqt=dlhs_local_aqt,\n        drhs_local_aqt=drhs_local_aqt,\n    )\n\n    # Stochastic Rounding\n    # These 3 variables are used to ensure we don't mix\n    # old and new style of SR configuration.\n    old_style_sr_config = use_stochastic_rounding is not None\n    new_style_sr_config_lhs = vjp_lhs_stochastic_rounding is not None\n    new_style_sr_config_rhs = vjp_rhs_stochastic_rounding is not None\n    assert new_style_sr_config_lhs == new_style_sr_config_rhs, (\n        'if you use new style SR config (vjp_xhs_stochastic_rounding), do pass'\n        ' both lhs and rhs explicitely.'\n    )\n    assert new_style_sr_config_lhs != old_style_sr_config\n\n    true = True  # A crude way to get around g-explicit-bool-comparison warning\n\n    assert not (vjp_lhs_stochastic_rounding and vjp_rhs_stochastic_rounding), (\n        'This config is buggy when you set both to True. Contact lew@ or use'\n        ' config_v3'\n    )\n\n    # By default use jax.uniform for stochastic rounding\n    if use_stochastic_rounding == true:\n        set_stochastic_rounding(cfg, True, True, 'jax.uniform')\n\n    if vjp_lhs_stochastic_rounding == true:\n        set_stochastic_rounding(cfg, True, False, 'jax.uniform')\n\n    if vjp_rhs_stochastic_rounding == true:\n        set_stochastic_rounding(cfg, False, True, 'jax.uniform')\n\n    if use_dummy_static_bound:\n        set_static_bound(cfg, 1.0)\n\n    return cfg\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.set_accumulator_dtype","title":"<code>set_accumulator_dtype(cfg, fwd_dtype, dlhs_dtype, drhs_dtype)</code>","text":"<p>The set_accumulator_dtype function sets the accumulator dtype for each of the three differentiable functions.  The accumulator dtype is used to store intermediate results during forward and backward passes.  It is also used to store gradients during backward pass. The default value for this parameter is None, which means that it will be set automatically by the library based on other parameters such as input data types and output data type.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DotGeneral</code> <p>DotGeneral: Set the accumulator dtype for all three</p> required <code>fwd_dtype</code> <code>Optional[DType]</code> <p>Optional[DType]: Set the dtype of the forward pass</p> required <code>dlhs_dtype</code> <code>Optional[DType]</code> <p>Optional[DType]: Set the data type of the left hand side</p> required <code>drhs_dtype</code> <code>Optional[DType]</code> <p>Optional[DType]: Set the data type for the drhs accumulator</p> required <code></code> <p>Set the dtype of the accumulator</p> required Source code in <code>src/fjformer/bits/config.py</code> <pre><code>def set_accumulator_dtype(\n        cfg: DotGeneral,\n        fwd_dtype: Optional[DType],\n        dlhs_dtype: Optional[DType],\n        drhs_dtype: Optional[DType],\n):\n    \"\"\"\n    The set_accumulator_dtype function sets the accumulator dtype for each of the three\n    differentiable functions.  The accumulator dtype is used to store intermediate results\n    during forward and backward passes.  It is also used to store gradients during backward pass.\n    The default value for this parameter is None, which means that it will be set automatically by\n    the library based on other parameters such as input data types and output data type.\n\n    :param cfg: DotGeneral: Set the accumulator dtype for all three\n    :param fwd_dtype: Optional[DType]: Set the dtype of the forward pass\n    :param dlhs_dtype: Optional[DType]: Set the data type of the left hand side\n    :param drhs_dtype: Optional[DType]: Set the data type for the drhs accumulator\n    :param : Set the dtype of the accumulator\n    \"\"\"\n    cfg.fwd.dg_accumulator_dtype = fwd_dtype\n    cfg.dlhs.dg_accumulator_dtype = dlhs_dtype\n    cfg.drhs.dg_accumulator_dtype = drhs_dtype\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.set_fwd_numerics","title":"<code>set_fwd_numerics(cfg, fwd_numerics)</code>","text":"<p>The set_fwd_numerics function sets the numerics of the forward problem.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Store the configuration of the simulation</p> required <code>fwd_numerics</code> <code>QNumerics</code> <p>numerics.QNumerics: Set the numerical</p> required <p>Returns:</p> Type Description <p>The configuration object with the numerics for the forward problem set</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>def set_fwd_numerics(cfg, fwd_numerics: numerics.QNumerics):\n    \"\"\"\n    The set_fwd_numerics function sets the numerics of the forward problem.\n\n    :param cfg: Store the configuration of the simulation\n    :param fwd_numerics: numerics.QNumerics: Set the numerical\n    :return: The configuration object with the numerics for the forward problem set\n    \"\"\"\n    cfg.fwd.lhs.numerics = fwd_numerics\n    cfg.fwd.rhs.numerics = fwd_numerics\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.set_static_bound","title":"<code>set_static_bound(cfg, bound=1.0)</code>","text":"<p>The set_static_bound function sets the calibration of all the forward and backward differentiation operators to a constant value. This is useful for testing purposes, as it allows us to check that our implementation is correct by comparing against known values.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DotGeneral</code> <p>DotGeneral: Set the bounds for each of the six functions in a dotgeneral object</p> required <code>bound</code> <code>float</code> <p>float: Set the bound of the calibration</p> <code>1.0</code> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>def set_static_bound(cfg: DotGeneral, bound: float = 1.0):\n\n    \"\"\"\n    The set_static_bound function sets the calibration of all the forward and backward\n    differentiation operators to a constant value. This is useful for testing purposes, as it\n    allows us to check that our implementation is correct by comparing against known values.\n\n    :param cfg: DotGeneral: Set the bounds for each of the six functions in a dotgeneral object\n    :param bound: float: Set the bound of the calibration\n    \"\"\"\n    cfg.fwd.lhs.calibration = calibration.ConstantCalibration(bound)\n    cfg.fwd.rhs.calibration = calibration.ConstantCalibration(bound)\n    cfg.drhs.lhs.calibration = calibration.ConstantCalibration(bound)\n    cfg.drhs.rhs.calibration = calibration.ConstantCalibration(bound)\n    cfg.dlhs.lhs.calibration = calibration.ConstantCalibration(bound)\n    cfg.dlhs.rhs.calibration = calibration.ConstantCalibration(bound)\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.set_stochastic_rounding","title":"<code>set_stochastic_rounding(cfg, vjp_lhs_stochastic_rounding, vjp_rhs_stochastic_rounding, implementation)</code>","text":"<p>Configure stochastic rounding implementation.</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>def set_stochastic_rounding(\n        cfg: DotGeneral,\n        vjp_lhs_stochastic_rounding: bool,\n        vjp_rhs_stochastic_rounding: bool,\n        implementation: str,\n):\n    \"\"\"Configure stochastic rounding implementation.\"\"\"\n    noise_implementations = {\n        'jax.uniform': lambda shape, key: jax.random.uniform(key, shape) - 0.5,\n        'custom-1': stochastic_rounding.random_centered_uniform,\n    }\n    msg = f'{implementation} not supported.'\n    assert implementation in noise_implementations.keys(), msg\n    noise_fn = noise_implementations[implementation]\n\n    if vjp_lhs_stochastic_rounding:\n        cfg.dlhs.lhs.numerics = cfg.dlhs.lhs.numerics.replace(noise_fn=noise_fn)\n        cfg.drhs.lhs.numerics = cfg.drhs.lhs.numerics.replace(noise_fn=noise_fn)\n    else:\n        cfg.dlhs.lhs.numerics = cfg.dlhs.lhs.numerics.replace(noise_fn=None)\n        cfg.drhs.lhs.numerics = cfg.drhs.lhs.numerics.replace(noise_fn=None)\n\n    if vjp_rhs_stochastic_rounding:\n        cfg.dlhs.rhs.numerics = cfg.dlhs.rhs.numerics.replace(noise_fn=noise_fn)\n        cfg.drhs.rhs.numerics = cfg.drhs.rhs.numerics.replace(noise_fn=noise_fn)\n    else:\n        cfg.dlhs.rhs.numerics = cfg.dlhs.rhs.numerics.replace(noise_fn=None)\n        cfg.drhs.rhs.numerics = cfg.drhs.rhs.numerics.replace(noise_fn=None)\n</code></pre>"},{"location":"generated-bits-config/#src.fjformer.bits.config.tensor_make","title":"<code>tensor_make(bits)</code>","text":"<p>The tensor_make function is a helper function that creates a Tensor object.</p> <p>Parameters:</p> Name Type Description Default <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Set the number of bits for quantization</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor object</p> Source code in <code>src/fjformer/bits/config.py</code> <pre><code>def tensor_make(bits: Optional[int]) -&gt; 'Tensor':\n\n    \"\"\"\n    The tensor_make function is a helper function that creates a Tensor object.\n\n    :param bits: Optional[int]: Set the number of bits for quantization\n    :return: A tensor object\n    \"\"\"\n    if bits is None:\n        effective_numerics = no_numerics.NoNumerics()\n    else:\n        pz = False if bits == 1 else True\n        dtype = jnp.int8 if 2 &lt;= bits &lt;= 8 and pz else None\n        effective_numerics = int_numerics.IntNumerics(\n            bits=bits,\n            preserve_zero=pz,\n            preserve_max_val=False,\n            clip=True,\n            round=True,\n            noise_fn=None,\n            clip_gradient=False,  # This can be disabled when using abs-max scaling.\n            dtype=dtype,\n        )\n\n    return Tensor(\n        numerics=effective_numerics,\n        calib_shared_axes=None,\n        scale_stop_grad=True,\n        calibration=calibration.AbsMaxCalibration(),\n        po2_scale=False,\n        use_fake_quant=False,\n        use_fwd_quant=None,\n        preprocess=None,\n    )\n</code></pre>"},{"location":"generated-bits-int_numerics/","title":"bits.int_numerics","text":"<p>Numerics for int8, int4, binary and other integer types.</p>"},{"location":"generated-bits-int_numerics/#src.fjformer.bits.int_numerics.IntNumerics","title":"<code>IntNumerics</code>","text":"<p>             Bases: <code>QNumerics</code>, <code>PyTreeNode</code></p> <p>Numerics for int8, int4, binary, etc.</p> Source code in <code>src/fjformer/bits/int_numerics.py</code> <pre><code>class IntNumerics(numerics.QNumerics, flax.struct.PyTreeNode):\n    \"\"\"Numerics for int8, int4, binary, etc.\"\"\"\n\n    bits: int\n    preserve_zero: bool\n    preserve_max_val: bool\n    clip: bool\n    clip_gradient: bool\n    round: bool\n    noise_fn: Optional[stochastic_rounding.NoiseFn]\n    dtype: Optional[Any] = None\n\n    def get_edge_of_last_int_bucket(self):\n        ret = 2.0 ** (self.bits - 1)\n        if self.preserve_zero:\n            # Lose one bucket.\n            ret -= 0.5\n        return ret\n\n    def get_center_of_last_int_bucket(self):\n        return self.get_edge_of_last_int_bucket() - 0.5\n\n    def abs_val_mapped_to(self):\n        if self.preserve_max_val:\n            return self.get_center_of_last_int_bucket()\n        else:\n            return self.get_edge_of_last_int_bucket()\n\n    def _get_fwd_clip_bound(self):\n        # If we are not rounding, we just clip to bucket edges.\n        fwd_clip_bound = self.get_edge_of_last_int_bucket()\n        # If, after clip, we are rounding, we need to make sure that\n        # we won't round values at the clip_bound away to the\n        # non-existing bucket.\n        if self.round:\n            # Reducing fwd_clip_bound by any value in (0.0, 1.0) is correct.\n            fwd_clip_bound -= 0.5\n        return fwd_clip_bound\n\n    def get_dtype(self):\n        return self.dtype\n\n    def fwd(self, x, context):\n        \"\"\"Forward pass.\"\"\"\n        assert self.bits &lt;= 22, 'Too many bits, float32 has less precision.'\n        # Maybe noise\n        if self.noise_fn:\n            assert context.key is not None, (\n                'noise_fn is set, requestic stochastic rounding, but RNG was not '\n                'passed in Context.key'\n            )\n            x = (x + self.noise_fn(x.shape, context.key)).astype(x.dtype)\n\n        if self.clip:\n            fwd_clip_bound = self._get_fwd_clip_bound()\n            x = jnp.clip(x, -fwd_clip_bound, fwd_clip_bound)\n\n        # Maybe round\n        if self.round:\n            # TODO(lew): Have bucket centers at 2*k + 1, not at halves.\n            round_to_halves = not self.preserve_zero\n            if round_to_halves:\n                x = jnp.floor(x) + 0.5\n            else:\n                x = lax.round(x, lax.RoundingMethod.TO_NEAREST_EVEN)\n\n        return x\n\n    def vjp_fwd(self, x, context):\n        res = (x,)\n        return self.fwd(x, context), res\n\n    def vjp_bwd(self, res, grad):\n        # Gradient of the clip function.\n        # For boundary values we will have full gradient.\n        # When using abs(max(x)) scaling, x is always in the interior, and the\n        # gradient clip is always 1. So, we can always set clip_gradient to false.\n        # However, other types of scaling may result in x being outside (i.e., there\n        # is clipping). In that case it may be desirable to make the gradient zero.\n        ret = grad\n        if self.clip_gradient:\n            (x,) = res\n            clip_bound = self._get_fwd_clip_bound()\n            ret *= (-clip_bound &lt;= x) * (x &lt;= clip_bound)\n        return (ret, None)\n</code></pre>"},{"location":"generated-bits-int_numerics/#src.fjformer.bits.int_numerics.IntNumerics.fwd","title":"<code>fwd(x, context)</code>","text":"<p>Forward pass.</p> Source code in <code>src/fjformer/bits/int_numerics.py</code> <pre><code>def fwd(self, x, context):\n    \"\"\"Forward pass.\"\"\"\n    assert self.bits &lt;= 22, 'Too many bits, float32 has less precision.'\n    # Maybe noise\n    if self.noise_fn:\n        assert context.key is not None, (\n            'noise_fn is set, requestic stochastic rounding, but RNG was not '\n            'passed in Context.key'\n        )\n        x = (x + self.noise_fn(x.shape, context.key)).astype(x.dtype)\n\n    if self.clip:\n        fwd_clip_bound = self._get_fwd_clip_bound()\n        x = jnp.clip(x, -fwd_clip_bound, fwd_clip_bound)\n\n    # Maybe round\n    if self.round:\n        # TODO(lew): Have bucket centers at 2*k + 1, not at halves.\n        round_to_halves = not self.preserve_zero\n        if round_to_halves:\n            x = jnp.floor(x) + 0.5\n        else:\n            x = lax.round(x, lax.RoundingMethod.TO_NEAREST_EVEN)\n\n    return x\n</code></pre>"},{"location":"generated-bits-no_numerics/","title":"bits.no_numerics","text":"<p>Identity numerics for passing through input as-is.</p>"},{"location":"generated-bits-no_numerics/#src.fjformer.bits.no_numerics.NoNumerics","title":"<code>NoNumerics</code>","text":"<p>             Bases: <code>QNumerics</code>, <code>PyTreeNode</code></p> <p>No quantization, use a native type such as bf16.</p> Source code in <code>src/fjformer/bits/no_numerics.py</code> <pre><code>class NoNumerics(numerics.QNumerics, flax.struct.PyTreeNode):\n    \"\"\"No quantization, use a native type such as bf16.\"\"\"\n\n    # TODO(lew): This is a workaround. We should separate Stochastic Rounding.\n    # noise_fn has no effect in NoNumerics.\n    noise_fn: Optional[stochastic_rounding.NoiseFn] = None\n    dtype: Optional[Any] = None\n\n    # TODO(lew): This is a hack. We treat check isinstance(NoNumerics) and treat\n    # it in a special way right now. These functions are never called\n    def get_dtype(self):\n        return None\n\n    def fwd(self, x, context):\n        pass\n\n    def abs_val_mapped_to(self):\n        pass\n\n    def vjp_fwd(self, x, context):\n        pass\n\n    def vjp_bwd(self, res, grad):\n        pass\n</code></pre>"},{"location":"generated-bits-numerics/","title":"bits.numerics","text":"<p>Base abstract class for all numerics.</p>"},{"location":"generated-bits-numerics/#src.fjformer.bits.numerics.QNumerics","title":"<code>QNumerics</code>","text":"<p>             Bases: <code>PyTreeNode</code>, <code>ABC</code></p> <p>Numerics for int8, int4, binary, etc.</p> Source code in <code>src/fjformer/bits/numerics.py</code> <pre><code>class QNumerics(flax.struct.PyTreeNode, abc.ABC):\n    \"\"\"Numerics for int8, int4, binary, etc.\"\"\"\n\n    # TODO(lew): Currently this is a part of API, only because it is used to set\n    # it in test. Remove and leave only get_dtype(\n\n    @abc.abstractmethod\n    def get_dtype(self):\n        pass\n\n    @abc.abstractmethod\n    def fwd(self, x, context):\n        \"\"\"Forward pass.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def abs_val_mapped_to(self):\n        \"\"\"The value returned is the end of quantization range.\n\n        It could be the biggest value that can be represented by numerical format\n        exactly. E.g. in case of int8, 127 . Or it could be edge of the last bucket.\n        Edge in case of int8, 127.5\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def vjp_fwd(self, x, context):\n        pass\n\n    @abc.abstractmethod\n    def vjp_bwd(self, res, grad):\n        pass\n</code></pre>"},{"location":"generated-bits-numerics/#src.fjformer.bits.numerics.QNumerics.abs_val_mapped_to","title":"<code>abs_val_mapped_to()</code>  <code>abstractmethod</code>","text":"<p>The value returned is the end of quantization range.</p> <p>It could be the biggest value that can be represented by numerical format exactly. E.g. in case of int8, 127 . Or it could be edge of the last bucket. Edge in case of int8, 127.5</p> Source code in <code>src/fjformer/bits/numerics.py</code> <pre><code>@abc.abstractmethod\ndef abs_val_mapped_to(self):\n    \"\"\"The value returned is the end of quantization range.\n\n    It could be the biggest value that can be represented by numerical format\n    exactly. E.g. in case of int8, 127 . Or it could be edge of the last bucket.\n    Edge in case of int8, 127.5\n    \"\"\"\n    pass\n</code></pre>"},{"location":"generated-bits-numerics/#src.fjformer.bits.numerics.QNumerics.fwd","title":"<code>fwd(x, context)</code>  <code>abstractmethod</code>","text":"<p>Forward pass.</p> Source code in <code>src/fjformer/bits/numerics.py</code> <pre><code>@abc.abstractmethod\ndef fwd(self, x, context):\n    \"\"\"Forward pass.\"\"\"\n    pass\n</code></pre>"},{"location":"generated-bits-q_dot_general/","title":"bits.q_dot_general","text":"<p>Quantized dot_general.</p>"},{"location":"generated-bits-q_dot_general/#src.fjformer.bits.q_dot_general.TensorRes","title":"<code>TensorRes</code>","text":"<p>All the things we pass from the forward pass to the backward pass.</p> Source code in <code>src/fjformer/bits/q_dot_general.py</code> <pre><code>@flax.struct.dataclass\nclass TensorRes:\n    \"\"\"All the things we pass from the forward pass to the backward pass.\"\"\"\n    mt: MultiTensor\n    quant_grad: Union[Callable[[jnp.ndarray], tuple[jnp.ndarray]], None]\n</code></pre>"},{"location":"generated-bits-q_dot_general/#src.fjformer.bits.q_dot_general.make_dot_general","title":"<code>make_dot_general(cfg)</code>","text":"<p>The make_dot_general function is a wrapper around the dot_general function. It takes in two QTensors, lhs and rhs, and returns a QTensor out. The make_dot_general function also handles preprocessing of the inputs to dot_general (lhs and rhs) and postprocessing of the output from dot_general (out).  The pre-/post-processing steps are:</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Optional[DotGeneral]</code> <p>Optional[config.DotGeneral]: Specify the configuration of the dot_general operation</p> required <p>Returns:</p> Type Description <p>A function that returns a function</p> Source code in <code>src/fjformer/bits/q_dot_general.py</code> <pre><code>def make_dot_general(cfg: Optional[config.DotGeneral]):\n\n    \"\"\"\n    The make_dot_general function is a wrapper around the dot_general function.\n    It takes in two QTensors, lhs and rhs, and returns a QTensor out.\n    The make_dot_general function also handles preprocessing of the inputs to dot_general (lhs and rhs)\n    and postprocessing of the output from dot_general (out).  The pre-/post-processing steps are:\n\n    :param cfg: Optional[config.DotGeneral]: Specify the configuration of the dot_general operation\n    :return: A function that returns a function\n    \"\"\"\n    if cfg is None:\n        def ret_lax_dg(\n                lhs,\n                rhs,\n                dimension_numbers,\n                precision=None,\n                preferred_element_type=None,\n                *,\n                context=Context(key=None, train_step=None),\n        ):\n\n            \"\"\"\n            The ret_lax_dg function is a wrapper for the jax.lax.dot_general function,\n            which performs a general matrix multiplication of two arrays with batch dimensions\n            and/or transpositions applied to either or both inputs. The ret_lax_dg function\n            is used in the implementation of the dot operation in this module.\n\n            :param lhs: Specify the left-hand side of the dot product\n            :param rhs: Specify the right-hand side of the matrix multiplication\n            :param dimension_numbers: Specify the dimensions of the operands\n            :param precision: Specify the precision of the computation\n            :param preferred_element_type: Specify the type of element that should be used to store the result\n            :param *: Indicate that all the following parameters are keyword only\n            :param context: Pass in the context of the computation\n            :param : Specify the dimension numbers of the dot product\n            :return: The result of the dot_general operation\n            \"\"\"\n            del context\n            return jax.lax.dot_general(\n                lhs, rhs, dimension_numbers, precision, preferred_element_type\n            )\n\n        return ret_lax_dg\n\n    dg = _dot_general_raw_attach_gradient(\n        fwd_dot_general_raw=_make_dot_general_raw(cfg.fwd),\n        dlhs_dot_general_raw=_make_dot_general_raw(cfg.dlhs),\n        drhs_dot_general_raw=_make_dot_general_raw(cfg.drhs),\n    )\n\n    def ret_dg(\n            lhs,\n            rhs,\n            dimension_numbers,\n            precision=None,\n            preferred_element_type=None,\n            *,\n            context=Context(key=None, train_step=None),\n    ):\n\n        \"\"\"\n        The ret_dg function is a wrapper around the dg function.\n        It takes in two QTensors, lhs and rhs, and returns a QTensor out.\n        The ret_dg function also handles preprocessing of the inputs to dg (lhs and rhs)\n        and postprocessing of the output from dg (out).  The pre-/post-processing steps are:\n\n        :param lhs: Pass the left hand side of the matrix multiplication\n        :param rhs: Pass on the right hand side of the matrix multiplication\n        :param dimension_numbers: Specify the contraction pattern\n        :param precision: Specify the precision of the output\n        :param preferred_element_type: Specify the dtype of the output\n        :param *: Indicate that the argument is a keyword-only\n        :param context: Pass the context to the dg function\n        :return: A function that returns a function\n        \"\"\"\n        del preferred_element_type\n        assert (\n                precision is None\n        ), f'Precision {precision} requested together with quantization.'\n\n        msg = 'AQT is not yet optimized to accept quantized types directly. '\n        msg += f'lhs.dtype: {lhs.dtype}, rhs.dtype: {rhs.dtype}'\n        assert lhs.dtype in [jnp.bfloat16, jnp.float32, jnp.float16], msg\n        assert rhs.dtype in [jnp.bfloat16, jnp.float32, jnp.float16], msg\n        # TODO(lew): Refactor Have a flax class with get and set.\n        # TODO(lew): Have a function to handle lhs and rhs uniformly.\n        lhs_qt = None\n        if cfg.fwd.lhs.preprocess is not None:\n            # lhs_q is quantized dtype.\n            # we are breaking the invariant that QTensor has a float qvalue\n            # But it will just be cast again to the same type.\n            lhs_qt = cfg.fwd.lhs.preprocess(None)\n        rhs_qt = None\n        if cfg.fwd.rhs.preprocess is not None:\n            rhs_qt = cfg.fwd.rhs.preprocess(None)\n\n        out, (out_lhs_qt, out_rhs_qt) = dg(\n            lhs=lhs,\n            rhs=rhs,\n            lhs_qt=lhs_qt,\n            rhs_qt=rhs_qt,\n            dimension_numbers=dimension_numbers,\n            context=context,\n        )\n\n        if cfg.fwd.lhs.preprocess is not None:\n            lhs_dtype = cfg.fwd.lhs.numerics.get_dtype()\n            out_lhs_qt = QTensor(\n                out_lhs_qt.qvalue.astype(lhs_dtype), out_lhs_qt.qvalue_scale_t\n            )\n            none = cfg.fwd.lhs.preprocess(out_lhs_qt)\n            assert none is None\n        if cfg.fwd.rhs.preprocess is not None:\n            rhs_dtype = cfg.fwd.rhs.numerics.get_dtype()\n            out_rhs_qt = QTensor(\n                out_rhs_qt.qvalue.astype(rhs_dtype), out_rhs_qt.qvalue_scale_t\n            )\n            none = cfg.fwd.rhs.preprocess(out_rhs_qt)\n            assert none is None\n\n        return out\n\n    return ret_dg\n</code></pre>"},{"location":"generated-bits-q_flax/","title":"bits.q_flax","text":"<p>Flax layer for AQT injection.</p>"},{"location":"generated-bits-q_flax/#src.fjformer.bits.q_flax.Freezer","title":"<code>Freezer</code>","text":"<p>             Bases: <code>Module</code></p> <p>Identity function that can freeze its input.</p> <p>On default it is an identity function that saves the input in a variable. In 'use_frozen=True' mode, ignores the input and returns the frozen value. It is usefult to implement 'constant folding' and put quantized weights and scales in the checkpoint for serving.</p> Source code in <code>src/fjformer/bits/q_flax.py</code> <pre><code>class Freezer(nn.Module):\n    \"\"\"Identity function that can freeze its input.\n\n    On default it is an identity function that saves the input in a variable.\n    In 'use_frozen=True' mode, ignores the input and returns the frozen value. It\n    is usefult to implement 'constant folding' and put quantized weights and\n    scales in the checkpoint for serving.\n    \"\"\"\n\n    quant_collection: str\n    quant_mode: QuantMode\n    q_shape: Iterable[int]\n    q_init: nn.initializers.Initializer\n    s_shape: Iterable[int]\n    s_init: nn.initializers.Initializer\n\n    @nn.compact\n    def __call__(\n            self, inputs: Optional[q_dot_general.QTensor]\n    ) -&gt; Optional[q_dot_general.QTensor]:\n        collection = self.quant_collection\n        if inputs is None:  # getter mode\n            if self.quant_mode == QuantMode.TRAIN:\n                return inputs\n            elif self.quant_mode == QuantMode.CONVERT:\n                return inputs\n            elif self.quant_mode == QuantMode.SERVE:\n                # We could have created one self.variable whose value is a QTensor,\n                # but this would complicate the init function, which could potentially\n                # be used by adding metadata such as sharding axises, etc.\n                qvalue = self.variable(collection, 'value', self.q_init, self.q_shape)\n                scale = self.variable(collection, 'scale', self.s_init, self.s_shape)\n                return q_dot_general.QTensor(qvalue.value, scale.value)\n            else:\n                assert False, 'Unknown quant mode.'\n        else:  # setter mode\n            if self.quant_mode == QuantMode.TRAIN:\n                pass\n            elif self.quant_mode == QuantMode.CONVERT:\n                qvalue = self.variable(collection, 'value', self.q_init, self.q_shape)\n                scale = self.variable(collection, 'scale', self.s_init, self.s_shape)\n                qvalue.value = inputs.qvalue\n                scale.value = inputs.qvalue_scale_t\n            elif self.quant_mode == QuantMode.SERVE:\n                # TODO(lew): Optionally compare stored and served value.\n                pass\n            else:\n                assert False, 'Unknown quant mode.'\n            return None\n</code></pre>"},{"location":"generated-bits-q_flax/#src.fjformer.bits.q_flax.QDotGeneral","title":"<code>QDotGeneral</code>","text":"<p>             Bases: <code>Module</code></p> <p>A layer that can be injected into flax.nn.Dense, etc.</p> Source code in <code>src/fjformer/bits/q_flax.py</code> <pre><code>class QDotGeneral(nn.Module):\n    \"\"\"A layer that can be injected into flax.nn.Dense, etc.\"\"\"\n\n    cfg: Optional[config.DotGeneral] = None\n    prng_name: Optional[str] = 'params'\n\n    # TODO(lew): split out separate class for each side.\n    lhs_quant_mode: QuantMode = QuantMode.TRAIN\n    lhs_init: nn.initializers.Initializer = jnp.zeros\n    lhs_scale_init: nn.initializers.Initializer = jnp.zeros\n    lhs_var_name: str = 'qlhs'\n\n    rhs_quant_mode: QuantMode = QuantMode.TRAIN\n    rhs_init: nn.initializers.Initializer = jnp.zeros\n    rhs_scale_init: nn.initializers.Initializer = jnp.zeros\n    rhs_var_name: str = 'qrhs'\n\n    # If you want use 'params' make sure that there is another mechanism to hide\n    # these variables from the optimizer.\n    quant_collection: str = 'aqt'\n\n    def make_aqt_dg(\n            self,\n            lhs_shape,\n            rhs_shape,\n            dimension_numbers: tuple[Iterable[int], Iterable[int]],\n    ):\n        lhs_scale_shape = list(lhs_shape)\n        rhs_scale_shape = list(rhs_shape)\n        (contr, _) = dimension_numbers\n        for li, ri in zip(*contr):\n            lhs_scale_shape[li] = 1\n            rhs_scale_shape[ri] = 1\n        lhs_scale = q_dot_general._lhs_scale_transpose(  # pylint: disable=protected-access\n            jnp.zeros(lhs_scale_shape), dimension_numbers, lhs_shape, rhs_shape\n        )\n        assert lhs_scale is not None\n        lhs_scale_shape = lhs_scale.shape\n        rhs_scale = q_dot_general._rhs_scale_transpose(  # pylint: disable=protected-access\n            jnp.zeros(rhs_scale_shape), dimension_numbers, lhs_shape, rhs_shape\n        )\n        assert rhs_scale is not None\n        rhs_scale_shape = rhs_scale.shape\n\n        cfg = copy.deepcopy(self.cfg)\n        if cfg is not None:\n            rhs_qm = self.rhs_quant_mode\n            lhs_qm = self.lhs_quant_mode\n\n            msg = 'The only function that is setting preprocess can be QQuantized.'\n            assert cfg.fwd.rhs.preprocess is None, msg\n            assert cfg.fwd.lhs.preprocess is None, msg\n            cfg.fwd.lhs.preprocess = Freezer(\n                name=self.lhs_var_name,\n                quant_mode=lhs_qm,\n                q_shape=lhs_shape,\n                q_init=self.lhs_init,\n                s_shape=lhs_scale_shape,\n                s_init=self.lhs_scale_init,\n                quant_collection=self.quant_collection,\n            )\n            cfg.fwd.rhs.preprocess = Freezer(\n                name=self.rhs_var_name,\n                quant_mode=rhs_qm,\n                q_shape=rhs_shape,\n                q_init=self.rhs_init,\n                s_shape=rhs_scale_shape,\n                s_init=self.rhs_scale_init,\n                quant_collection=self.quant_collection,\n            )\n        key = self.make_rng(self.prng_name) if self.prng_name is not None else None\n        context = q_dot_general.Context(key=key, train_step=None)\n        aqt_dg = q_dot_general.make_dot_general(cfg)\n        aqt_dg = functools.partial(aqt_dg, context=context)\n        return aqt_dg\n\n    @nn.compact\n    def __call__(\n            self,\n            lhs,\n            rhs,\n            dimension_numbers,\n            precision,\n            preferred_element_type=None,\n    ):\n        aqt_dg = self.make_aqt_dg(lhs.shape, rhs.shape, dimension_numbers)\n        return aqt_dg(\n            lhs,\n            rhs,\n            dimension_numbers,\n            precision,\n            preferred_element_type=preferred_element_type,\n        )\n</code></pre>"},{"location":"generated-bits-q_flax/#src.fjformer.bits.q_flax.QEinsum","title":"<code>QEinsum</code>","text":"<p>             Bases: <code>PyTreeNode</code></p> <p>Quantized Einsum class for model injection.</p> Source code in <code>src/fjformer/bits/q_flax.py</code> <pre><code>class QEinsum(flax.struct.PyTreeNode):\n    \"\"\"Quantized Einsum class for model injection.\"\"\"\n\n    cfg: Optional[config.DotGeneral] = None\n    prng_name: Optional[str] = 'params'\n\n    # TODO(lew): split out separate class for each side.\n    lhs_quant_mode: QuantMode = QuantMode.TRAIN\n    lhs_init: nn.initializers.Initializer = jnp.zeros\n    lhs_scale_init: nn.initializers.Initializer = jnp.zeros\n    lhs_var_name: str = 'qlhs'\n\n    rhs_quant_mode: QuantMode = QuantMode.TRAIN\n    rhs_init: nn.initializers.Initializer = jnp.zeros\n    rhs_scale_init: nn.initializers.Initializer = jnp.zeros\n    rhs_var_name: str = 'qrhs'\n\n    # If you want use 'params' make sure that there is another mechanism to hide\n    # these variables from the optimizer.\n    quant_collection: str = 'aqt'\n\n    def __call__(self, eqn, lhs_g, rhs_g):\n        def einsum(lhs_l, rhs_l, dg=jax.lax.dot_general):\n            operands, contractions = lax_numpy._default_poly_einsum_handler(  # pylint: disable=protected-access\n                eqn, lhs_l, rhs_l, einsum_call=True, use_blas=True, optimize='optimal'\n            )\n            contractions = tuple((a, frozenset(b), c) for a, b, c, *_ in contractions)\n            return jax.named_call(lax_numpy._einsum, name=eqn)(  # pylint: disable=protected-access\n                operands,\n                contractions,\n                precision=None,\n                preferred_element_type=None,\n                _dot_general=dg,\n            )\n\n        # yes_swap = whether einsum swaps [lhs,rhs] when passing them to dot_general\n        a = jax.make_jaxpr(einsum)(lhs_g, rhs_g)\n        [lhs_g_id, rhs_g_id] = a.eqns[0].invars\n        [lhs_l_id, rhs_l_id] = a.jaxpr.invars\n        not_swap = lhs_g_id == lhs_l_id and rhs_g_id == rhs_l_id\n        yes_swap = lhs_g_id == rhs_l_id and rhs_g_id == lhs_l_id\n        assert not_swap != yes_swap\n\n        cfg = copy.deepcopy(self.cfg)\n        prng_name = self.prng_name\n\n        lhs_quant_mode = self.lhs_quant_mode\n        lhs_init = self.lhs_init\n        lhs_scale_init = self.lhs_scale_init\n        lhs_var_name = self.lhs_var_name\n\n        rhs_quant_mode = self.rhs_quant_mode\n        rhs_init = self.rhs_init\n        rhs_scale_init = self.rhs_scale_init\n        rhs_var_name = self.rhs_var_name\n\n        quant_collection = self.quant_collection\n\n        if yes_swap:\n            if cfg is not None:\n                cfg.fwd.lhs, cfg.fwd.rhs = cfg.fwd.rhs, cfg.fwd.lhs\n                cfg.dlhs, cfg.drhs = cfg.drhs, cfg.dlhs\n            lhs_quant_mode, rhs_quant_mode = rhs_quant_mode, lhs_quant_mode\n            lhs_init, rhs_init = rhs_init, lhs_init\n            lhs_scale_init, rhs_scale_init = rhs_scale_init, lhs_scale_init\n            lhs_var_name, rhs_var_name = rhs_var_name, lhs_var_name\n\n        aqt_dg = QDotGeneral(\n            cfg=cfg,\n            prng_name=prng_name,\n            lhs_quant_mode=lhs_quant_mode,\n            lhs_init=lhs_init,\n            lhs_scale_init=lhs_scale_init,\n            lhs_var_name=lhs_var_name,\n            rhs_quant_mode=rhs_quant_mode,\n            rhs_init=rhs_init,\n            rhs_scale_init=rhs_scale_init,\n            rhs_var_name=rhs_var_name,\n            quant_collection=quant_collection,\n        )\n        return einsum(lhs_g, rhs_g, aqt_dg)\n</code></pre>"},{"location":"generated-bits-q_flax/#src.fjformer.bits.q_flax.config_v4","title":"<code>config_v4(*, fwd_bits=8, dlhs_bits=8, drhs_bits=None, use_dummy_static_bound=False, rng_type='jax.uniform', dlhs_local_aqt=None, drhs_local_aqt=None, fwd_accumulator_dtype=jnp.int32, dlhs_accumulator_dtype=jnp.int32, drhs_accumulator_dtype=None)</code>","text":"<p>The config_v4 function is a helper function that creates a DotGeneral config object. It takes in the following arguments: - fwd_bits: The number of bits to use for forward pass quantization. If None, no quantization will be used. Defaults to 8 bits. - dlhs_bits: The number of bits to use for left hand side gradient quantization (i.e., the weights). If None,  no quantization will be used. Defaults to 8 bits.. - drhs_bits: The number of bits to use for right hand side gradient quanitzation</p> <p>Parameters:</p> Name Type Description Default <code>*</code> <p>Indicate that the function accepts a variable number of arguments</p> required <code>fwd_bits</code> <code>Optional[int]</code> <p>Optional[int]: Set the number of bits for the forward pass</p> <code>8</code> <code>dlhs_bits</code> <code>Optional[int]</code> <p>Optional[int]: Set the number of bits used for quantization</p> <code>8</code> <code>drhs_bits</code> <code>Optional[int]</code> <p>Optional[int]: Set the number of bits for the right hand side</p> <code>None</code> <code>use_dummy_static_bound</code> <code>bool</code> <p>bool: Set the static bound to 1</p> <code>False</code> <code>rng_type</code> <code>str</code> <p>str: Set the type of random number generator</p> <code>'jax.uniform'</code> <code>dlhs_local_aqt</code> <code>Optional[LocalQ]</code> <p>Optional[config.LocalQ]: Set the local quantization parameters for the lhs gradient</p> <code>None</code> <code>drhs_local_aqt</code> <code>Optional[LocalQ]</code> <p>Optional[config.LocalQ]: Set the local quantization parameters for the drhs tensor</p> <code>None</code> <code>fwd_accumulator_dtype</code> <code>...</code> <p>...: Set the accumulator dtype for forward pass</p> <code>int32</code> <code>dlhs_accumulator_dtype</code> <code>...</code> <p>...: Determine the dtype of the accumulator in</p> <code>int32</code> <code>drhs_accumulator_dtype</code> <code>...</code> <p>...: Determine the dtype of the accumulator used in q_dot_general</p> <code>None</code> <code></code> <p>Determine the number of bits used for quantization</p> required <p>Returns:</p> Type Description <code>DotGeneral</code> <p>A config</p> Source code in <code>src/fjformer/bits/q_flax.py</code> <pre><code>def config_v4(\n        *,\n        fwd_bits: Optional[int] = 8,\n        dlhs_bits: Optional[int] = 8,\n        drhs_bits: Optional[int] = None,\n        # The dummy static bound flag is for performance benchmarking.\n        use_dummy_static_bound: bool = False,\n        rng_type: str = 'jax.uniform',  # 'custom-1'\n        dlhs_local_aqt: Optional[config.LocalQ] = None,\n        drhs_local_aqt: Optional[config.LocalQ] = None,\n        fwd_accumulator_dtype: ... = jnp.int32,\n        dlhs_accumulator_dtype: ... = jnp.int32,\n        drhs_accumulator_dtype: ... = None,\n) -&gt; config.DotGeneral:\n    \"\"\"\n    The config_v4 function is a helper function that creates a DotGeneral config\n    object. It takes in the following arguments:\n    - fwd_bits: The number of bits to use for forward pass quantization. If None, no quantization will be used.\n    Defaults to 8 bits.\n    - dlhs_bits: The number of bits to use for left hand side gradient quantization (i.e., the weights). If None,\n     no quantization will be used. Defaults to 8 bits..\n    - drhs_bits: The number of bits to use for right hand side gradient quanitzation\n\n    :param *: Indicate that the function accepts a variable number of arguments\n    :param fwd_bits: Optional[int]: Set the number of bits for the forward pass\n    :param dlhs_bits: Optional[int]: Set the number of bits used for quantization\n    :param drhs_bits: Optional[int]: Set the number of bits for the right hand side\n    :param use_dummy_static_bound: bool: Set the static bound to 1\n    :param rng_type: str: Set the type of random number generator\n    :param dlhs_local_aqt: Optional[config.LocalQ]: Set the local quantization parameters for the lhs gradient\n    :param drhs_local_aqt: Optional[config.LocalQ]: Set the local quantization parameters for the drhs tensor\n    :param fwd_accumulator_dtype: ...: Set the accumulator dtype for forward pass\n    :param dlhs_accumulator_dtype: ...: Determine the dtype of the accumulator in\n    :param drhs_accumulator_dtype: ...: Determine the dtype of the accumulator used in q_dot_general\n    :param : Determine the number of bits used for quantization\n    :return: A config\n    \"\"\"\n\n    def tensor_config(bits: Optional[int]) -&gt; config.Tensor:\n        assert bits is None or bits &gt;= 2, 'Need at least 2 bits.'\n        if bits is None:\n            numerics = no_numerics.NoNumerics()\n        else:\n            numerics = int_numerics.IntNumerics(\n                bits=bits,\n                preserve_zero=True,\n                preserve_max_val=False,\n                clip=True,\n                round=True,\n                noise_fn=None,\n                clip_gradient=False,  # Can be False when using abs-max scaling.\n                dtype=jnp.int8 if 2 &lt;= bits &lt;= 8 else None,\n            )\n\n        return config.Tensor(\n            numerics=numerics,\n            calib_shared_axes=None,\n            scale_stop_grad=True,\n            calibration=calibration.AbsMaxCalibration(),\n            po2_scale=False,\n            use_fake_quant=False,\n            # dtype_x=dtype,\n            use_fwd_quant=None,\n            preprocess=None,\n        )\n\n    def dg_raw_config(lhs_bits, rhs_bits, local_aqt=None) -&gt; config.DotGeneralRaw:\n        lhs_cfg = tensor_config(lhs_bits)\n        rhs_cfg = tensor_config(rhs_bits)\n        if (\n                True  # Just to format lines below\n                and lhs_bits is not None\n                and rhs_bits is not None\n                and lhs_bits &lt;= 8\n                and rhs_bits &lt;= 8\n        ):\n            dg_accumulator_dtype = jnp.int32\n        else:\n            # None determines the dtype on the fly in q_dot_general\n            dg_accumulator_dtype = None\n\n        return config.DotGeneralRaw(\n            lhs=lhs_cfg,\n            rhs=rhs_cfg,\n            dg_accumulator_dtype=dg_accumulator_dtype,\n            local_aqt=local_aqt,\n        )\n\n    cfg = config.DotGeneral(\n        fwd=dg_raw_config(fwd_bits, fwd_bits),\n        dlhs=dg_raw_config(dlhs_bits, dlhs_bits, local_aqt=dlhs_local_aqt),\n        drhs=dg_raw_config(drhs_bits, drhs_bits, local_aqt=drhs_local_aqt),\n    )\n\n    cfg.dlhs.rhs.use_fwd_quant = False\n    cfg.drhs.rhs.use_fwd_quant = False\n\n    # Typically we have (but I don't know if it is guraranteed):\n    # - vjp_lhs_stochastic_rounding is referring to the gradient and\n    # - vjp_rhs_stochastic_rounding is referring to the activations/weights.\n    config.set_stochastic_rounding(\n        cfg,\n        vjp_lhs_stochastic_rounding=True,\n        vjp_rhs_stochastic_rounding=False,\n        implementation=rng_type,\n    )\n\n    if use_dummy_static_bound:\n        config.set_static_bound(cfg, 1.0)\n\n    config.set_accumulator_dtype(\n        cfg,\n        fwd_dtype=fwd_accumulator_dtype,\n        dlhs_dtype=dlhs_accumulator_dtype,\n        drhs_dtype=drhs_accumulator_dtype,\n    )\n\n    return cfg\n</code></pre>"},{"location":"generated-bits-qk/","title":"bits.qk","text":""},{"location":"generated-bits-qk/#src.fjformer.bits.qk.quantize_kv","title":"<code>quantize_kv(kv)</code>","text":"<p>Quantize key/values stored in kvcache.</p> Source code in <code>src/fjformer/bits/qk.py</code> <pre><code>def quantize_kv(kv: chex.Array):\n    \"\"\"Quantize key/values stored in kvcache.\"\"\"\n    scale = jnp.max(jnp.abs(kv), axis=-1, keepdims=True)\n    value = jnp.int8(jnp.rint(kv * (MAX_INT8 / scale)))\n    return value, scale\n</code></pre>"},{"location":"generated-bits-qk/#src.fjformer.bits.qk.unquantize_kv","title":"<code>unquantize_kv(value, scale, dtype)</code>","text":"<p>Unquantize key/values stored in kvcache.</p> Source code in <code>src/fjformer/bits/qk.py</code> <pre><code>def unquantize_kv(value: chex.Array, scale: chex.Array, dtype: jnp.dtype):\n    \"\"\"Unquantize key/values stored in kvcache.\"\"\"\n    return value.astype(dtype) * scale / MAX_INT8\n</code></pre>"},{"location":"generated-bits-stochastic_rounding/","title":"bits.stochastic_rounding","text":"<p>Efficient stochastic rounding implementation.</p>"},{"location":"generated-bits-stochastic_rounding/#src.fjformer.bits.stochastic_rounding.random_centered_uniform","title":"<code>random_centered_uniform(shape, key)</code>","text":"<p>Generates uniform number in [-0.5, 0.5].</p> Source code in <code>src/fjformer/bits/stochastic_rounding.py</code> <pre><code>def random_centered_uniform(\n        shape: tuple[int, ...], key: jax.Array\n) -&gt; jnp.ndarray:\n    \"\"\"Generates uniform number in [-0.5, 0.5].\"\"\"\n    dtype = jnp.dtype('uint16')\n    nbits = jnp.iinfo(dtype).bits\n\n    # Generate random bits.\n    bits = jax.random.bits(key, shape, dtype)\n\n    # Align bits with the mantissa of f32.\n    nmant = jnp.finfo(jnp.float32).nmant\n    r_bitpattern = jnp.uint32(bits) &lt;&lt; (nmant - nbits)\n    r_bitpattern = r_bitpattern | jnp.float32(1).view(jnp.uint32)\n    assert r_bitpattern.dtype == jnp.uint32\n\n    # Gen random floats and shift\n    rand_floats = jax.lax.bitcast_convert_type(r_bitpattern, jnp.float32)\n    shift = 2 ** (-1 - nbits)\n    centered = rand_floats - (1.5 - shift)\n\n    return centered\n</code></pre>"},{"location":"generated-checkpoint-_load/","title":"checkpoint._load","text":""},{"location":"generated-checkpoint-streamer/","title":"checkpoint.streamer","text":""},{"location":"generated-checkpoint-streamer/#src.fjformer.checkpoint.streamer.CheckpointManager","title":"<code>CheckpointManager</code>","text":"<p>             Bases: <code>object</code></p> <p>Custom msgpack checkpointer that saves large train states by serializing and saving tensors one by one in a streaming fashion. Avoids running out of memory or local TPU disk with default flax checkpointer.</p> Source code in <code>src/fjformer/checkpoint/streamer.py</code> <pre><code>class CheckpointManager(object):\n    \"\"\"\n    Custom msgpack checkpointer that saves large train states by serializing\n    and saving tensors one by one in a streaming fashion. Avoids running\n    out of memory or local TPU disk with default flax checkpointer.\n    \"\"\"\n\n    def __init__(\n            self,\n            checkpoint_dir,\n            enable=True,\n            float_dtype: Union[str, jnp.dtype] = \"bf16\",\n            save_optimizer_state: bool = True,\n            verbose: bool = False\n    ):\n        self.float_dtype = float_dtype\n        self.save_optimizer_state = save_optimizer_state\n        self.checkpoint_dir = checkpoint_dir\n        self.enable = enable\n        self.verbose = verbose\n\n    def save_checkpoint(\n            self,\n            state: struct.PyTreeNode,\n            filename: Union[str, os.PathLike],\n            gather_fns: dict[Callable] = None,\n            mismatch_allowed: bool = True\n\n    ):\n        if self.enable:\n            path = os.path.join(self.checkpoint_dir, filename)\n        else:\n            path = \"/dev/null\"\n        self.save_state_to_file(\n            state, path, gather_fns, self.float_dtype, mismatch_allowed=mismatch_allowed\n        )\n\n    @staticmethod\n    def save_state_to_file(\n            state: struct.PyTreeNode,\n            path: Union[str, os.PathLike],\n            gather_fns: dict[Callable] = None,\n            float_dtype=None,\n            verbose: bool = False,\n            mismatch_allowed: bool = True\n    ):\n        state = to_state_dict(state)\n        packer = msgpack.Packer()\n        flatten_state = flatten_dict(state)\n        if gather_fns is not None:\n            gather_fns = flatten_dict(to_state_dict(gather_fns))\n        pbar = tqdm.tqdm(\n            flatten_state.items(),\n            disable=not verbose,\n            desc=\"Saving State to File\",\n        )\n\n        gather_functions_mismatch = 0\n\n        with open(path, \"wb\") as stream:\n            for key, value in pbar:\n                if gather_fns is not None:\n                    try:\n                        callable_func = gather_fns[key]\n                        if callable_func is None and not mismatch_allowed:\n                            raise KeyError(f\"Gather Function {key} is None and NoneType OBJ is not callable.\")\n                        value = callable_func(value) if callable_func is not None else value\n                        if callable_func is None:\n                            gather_functions_mismatch += 1\n                    except KeyError as k_err:\n                        if mismatch_allowed:\n                            gather_functions_mismatch += 1\n                        else:\n                            raise KeyError(k_err)\n                pbar.set_postfix(gather_functions_mismatch=gather_functions_mismatch)\n                value = get_dtype(value, float_dtype)\n                stream.write(packer.pack((key, to_bytes(value))))\n\n    def save_pickle(\n            self,\n            obj,\n            filename: Union[str, os.PathLike]\n    ):\n        \"\"\"\n        The save_pickle function saves a Python object to disk using the pickle module.\n\n        :param self: Represent the instance of the class\n        :param obj: Pass the object that is to be pickled\n        :param filename: Specify the name of the file to be saved\n        :return: A pickle object\n\n        \"\"\"\n        import pickle\n\n        def save_pickle(obj_, path_):\n            with open(path_, \"wb\") as stream:\n                pickle.dump(obj_, stream)\n\n        if self.enable:\n            path = os.path.join(self.checkpoint_dir, filename)\n        else:\n            path = \"/dev/null\"\n        save_pickle(obj, path)\n\n    def save_all(\n            self,\n            state: struct.PyTreeNode,\n            gather_fns,\n            metadata=None,\n            dataset=None,\n            milestone=False\n    ):\n        \"\"\"\n        The save_all function saves the following:\n            - metadata.pkl (a pickle file containing a dictionary of metadata)\n            - dataset.pkl (a pickle file containing the training data)\n            - streaming_params_{step}.pkl or streaming_state_{step}.pkl\n                (depending on whether we want to save optimizer state or not,\n                this is a checkpoint that will not be overwritten by future checkpoints)\n\n        :param self: Access the attributes and methods of the class\n        :param state: struct.PyTreeNode: Save the current state of the model\n        :param gather_fns: Gather the state of the optimizer\n        :param metadata: Save the metadata of the training\n        :param dataset: Save the dataset to disk\n        :param milestone: Determine whether the checkpoint is a milestone or not\n        :return: Nothing\n\n        \"\"\"\n        step = int(jax.device_get(state.step))\n        if self.save_optimizer_state:\n            checkpoint_state = state\n            checkpoint_name = \"streaming_state\"\n            checkpoint_gather_fns = gather_fns\n        else:\n            checkpoint_state = state.params[\"params\"]\n            checkpoint_name = \"streaming_params\"\n            checkpoint_gather_fns = gather_fns.params[\"params\"]\n\n        if milestone:\n            # Save a milestone checkpoint that will not be overwritten\n            self.save_pickle(metadata, f\"metadata_{step}.pkl\")\n            self.save_pickle(dataset, f\"dataset_{step}.pkl\")\n            self.save_checkpoint(\n                checkpoint_state, f\"{checkpoint_name}_{step}\", checkpoint_gather_fns\n            )\n        else:\n            # Save a normal checkpoint that can be overwritten\n            self.save_pickle(metadata, \"metadata.pkl\")\n            self.save_pickle(dataset, \"dataset.pkl\")\n            self.save_checkpoint(\n                checkpoint_state, f\"{checkpoint_name}\", checkpoint_gather_fns\n            )\n\n    @staticmethod\n    def load_checkpoint(\n            path: Union[str, os.PathLike],\n            target=None,\n            shard_fns: dict[Callable] = None,\n            remove_dict_prefix=None,\n            verbose: bool = False,\n            mismatch_allowed: bool = True,\n    ):\n        \"\"\"\n        The load_checkpoint function is used to checkpoint a checkpoint from disk.\n\n        :param path: Specify the path to the checkpoint file\n        :param target: Specify the model to checkpoint the checkpoint into\n        :param shard_fns: Specify a function that will be applied to each tensor in the checkpoint\n        :param remove_dict_prefix: Remove the prefix of a dictionary     \n        :param verbose: print state and other stuff\n        :param mismatch_allowed: when ever to allow shard_fns to be passed even if their None\n        :return:  of the form {key: value}, where key is a tuple and value is a tensor\n\n        \"\"\"\n        if shard_fns is not None:\n            shard_fns = flatten_dict(\n                to_state_dict(shard_fns)\n            )\n        if remove_dict_prefix is not None:\n            remove_dict_prefix = tuple(remove_dict_prefix)\n        flatten_state = {}\n\n        shard_functions_mismatch = 0\n        with open(path, \"rb\") as fin:\n            unpacker = msgpack.Unpacker(fin, read_size=83886080, max_buffer_size=0)\n            pbar = tqdm.tqdm(\n                unpacker,\n                disable=not verbose,\n                desc=\"Loading Checkpoints From File\"\n            )\n            for key, value in pbar:\n                key = tuple(key)\n                if remove_dict_prefix is not None:\n                    if key[:len(remove_dict_prefix)] == remove_dict_prefix:\n                        key = key[len(remove_dict_prefix):]\n                    else:\n                        continue\n\n                tensor = from_bytes(None, value)\n                if shard_fns is not None:\n                    try:\n                        callable_func = shard_fns[key]\n                        if callable_func is None and not mismatch_allowed:\n                            raise KeyError(f\"Shard Function {key} is None and NoneType OBJ is not callable.\")\n                        tensor = callable_func(tensor) if callable_func is not None else tensor\n                        if callable_func is None:\n                            shard_functions_mismatch += 1\n                    except KeyError as k_err:\n                        if mismatch_allowed:\n                            shard_functions_mismatch += 1\n                        else:\n                            raise KeyError(k_err)\n                flatten_state[key] = tensor\n                pbar.set_postfix(shard_functions_mismatch=shard_functions_mismatch)\n        if target is not None:\n            flattened_target = flatten_dict(\n                to_state_dict(target), keep_empty_nodes=True\n            )\n            for key, value in flattened_target.items():\n                if key not in flatten_state and value == empty_node:\n                    flatten_state[key] = value\n\n        state = unflatten_dict(flatten_state)\n        if target is None:\n            return state\n\n        return from_state_dict(target, state)\n\n    @staticmethod\n    def load_flax_checkpoint(\n            path,\n            target=None,\n            shard_fns=None\n    ):\n        \"\"\" Load a standard flax checkpoint that\"s not saved with the\n            msgpack streaming format.\n        \"\"\"\n        with open(path, \"rb\") as fin:\n            encoded_bytes = fin.read()\n\n        state_dict = flax.serialization.msgpack_restore(encoded_bytes)\n        if shard_fns is not None:\n            shard_fns = to_state_dict(shard_fns)\n            state_dict = jax.tree_util.tree_map(lambda fn, x: fn(x), shard_fns, state_dict)\n\n        if target is None:\n            return state_dict\n        return from_state_dict(target, state_dict)\n\n    @classmethod\n    def load_state_checkpoint(\n            cls,\n            load_type: Literal[\n                \"state\",\n                \"state_params\",\n                \"params\",\n                \"flax_params\"\n            ],\n            load_path: Union[str, os.PathLike],\n            state_target=None,\n            state_shard_fns=None,\n            disallow_state=False,\n            mismatch_allowed: bool = True\n    ):\n        \"\"\"\n        The load_state_checkpoint function is used to checkpoint a checkpoint from disk.\n\n        :param cls: Call the load_checkpoint function\n        :param load_type: Specify which part of state to checkpoint\n        :param load_path: Specify where to checkpoint the model from\n        :param state_target: Specify the target for the train state\n        :param state_shard_fns: Specify the sharding function\n        :param disallow_state: Prevent loading the entire state\n        :param mismatch_allowed: when ever to allow shard func to be None\n        :return: A tuple of two objects, the state and restored_params\n\n        \"\"\"\n        if state_target is not None:\n            params_target = state_target.params[\"params\"]\n        else:\n            params_target = None\n\n        if state_shard_fns is not None:\n            params_shard_fns = state_shard_fns.params[\"params\"]\n        else:\n            params_shard_fns = None\n\n        if disallow_state:\n            assert load_type != \"state\", \"Loading full state is not allowed!\"\n        state = None\n        restored_params = None\n        if load_type == \"state\":\n            state = cls.load_checkpoint(\n                path=load_path,\n                target=state_target,\n                shard_fns=state_shard_fns,\n                mismatch_allowed=mismatch_allowed\n            )\n        elif load_type == \"state_params\":\n            restored_params = cls.load_checkpoint(\n                path=load_path,\n                target=params_target,\n                shard_fns=params_shard_fns,\n                remove_dict_prefix=(\"params\", \"params\"),\n                mismatch_allowed=mismatch_allowed\n            )\n            restored_params = flax.core.frozen_dict.freeze(\n                {\"params\": restored_params}\n            )\n        elif load_type == \"params\":\n            restored_params = cls.load_checkpoint(\n                path=load_path,\n                target=params_target,\n                shard_fns=params_shard_fns,\n                mismatch_allowed=mismatch_allowed\n            )\n            restored_params = flax.core.frozen_dict.freeze(\n                {\"params\": restored_params}\n            )\n        elif load_type == \"flax_params\":\n            restored_params = cls.load_flax_checkpoint(\n                path=load_path,\n                target=params_target,\n                shard_fns=params_shard_fns,\n            )\n            restored_params = flax.core.frozen_dict.freeze(\n                {\"params\": restored_params}\n            )\n        else:\n            raise ValueError(f\"Invalid load_from type: {load_type}\")\n\n        return state, restored_params\n</code></pre>"},{"location":"generated-checkpoint-streamer/#src.fjformer.checkpoint.streamer.CheckpointManager.load_checkpoint","title":"<code>load_checkpoint(path, target=None, shard_fns=None, remove_dict_prefix=None, verbose=False, mismatch_allowed=True)</code>  <code>staticmethod</code>","text":"<p>The load_checkpoint function is used to checkpoint a checkpoint from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, PathLike]</code> <p>Specify the path to the checkpoint file</p> required <code>target</code> <p>Specify the model to checkpoint the checkpoint into</p> <code>None</code> <code>shard_fns</code> <code>dict[Callable]</code> <p>Specify a function that will be applied to each tensor in the checkpoint</p> <code>None</code> <code>remove_dict_prefix</code> <p>Remove the prefix of a dictionary</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>print state and other stuff</p> <code>False</code> <code>mismatch_allowed</code> <code>bool</code> <p>when ever to allow shard_fns to be passed even if their None</p> <code>True</code> <p>Returns:</p> Type Description <p>of the form {key: value}, where key is a tuple and value is a tensor</p> Source code in <code>src/fjformer/checkpoint/streamer.py</code> <pre><code>@staticmethod\ndef load_checkpoint(\n        path: Union[str, os.PathLike],\n        target=None,\n        shard_fns: dict[Callable] = None,\n        remove_dict_prefix=None,\n        verbose: bool = False,\n        mismatch_allowed: bool = True,\n):\n    \"\"\"\n    The load_checkpoint function is used to checkpoint a checkpoint from disk.\n\n    :param path: Specify the path to the checkpoint file\n    :param target: Specify the model to checkpoint the checkpoint into\n    :param shard_fns: Specify a function that will be applied to each tensor in the checkpoint\n    :param remove_dict_prefix: Remove the prefix of a dictionary     \n    :param verbose: print state and other stuff\n    :param mismatch_allowed: when ever to allow shard_fns to be passed even if their None\n    :return:  of the form {key: value}, where key is a tuple and value is a tensor\n\n    \"\"\"\n    if shard_fns is not None:\n        shard_fns = flatten_dict(\n            to_state_dict(shard_fns)\n        )\n    if remove_dict_prefix is not None:\n        remove_dict_prefix = tuple(remove_dict_prefix)\n    flatten_state = {}\n\n    shard_functions_mismatch = 0\n    with open(path, \"rb\") as fin:\n        unpacker = msgpack.Unpacker(fin, read_size=83886080, max_buffer_size=0)\n        pbar = tqdm.tqdm(\n            unpacker,\n            disable=not verbose,\n            desc=\"Loading Checkpoints From File\"\n        )\n        for key, value in pbar:\n            key = tuple(key)\n            if remove_dict_prefix is not None:\n                if key[:len(remove_dict_prefix)] == remove_dict_prefix:\n                    key = key[len(remove_dict_prefix):]\n                else:\n                    continue\n\n            tensor = from_bytes(None, value)\n            if shard_fns is not None:\n                try:\n                    callable_func = shard_fns[key]\n                    if callable_func is None and not mismatch_allowed:\n                        raise KeyError(f\"Shard Function {key} is None and NoneType OBJ is not callable.\")\n                    tensor = callable_func(tensor) if callable_func is not None else tensor\n                    if callable_func is None:\n                        shard_functions_mismatch += 1\n                except KeyError as k_err:\n                    if mismatch_allowed:\n                        shard_functions_mismatch += 1\n                    else:\n                        raise KeyError(k_err)\n            flatten_state[key] = tensor\n            pbar.set_postfix(shard_functions_mismatch=shard_functions_mismatch)\n    if target is not None:\n        flattened_target = flatten_dict(\n            to_state_dict(target), keep_empty_nodes=True\n        )\n        for key, value in flattened_target.items():\n            if key not in flatten_state and value == empty_node:\n                flatten_state[key] = value\n\n    state = unflatten_dict(flatten_state)\n    if target is None:\n        return state\n\n    return from_state_dict(target, state)\n</code></pre>"},{"location":"generated-checkpoint-streamer/#src.fjformer.checkpoint.streamer.CheckpointManager.load_flax_checkpoint","title":"<code>load_flax_checkpoint(path, target=None, shard_fns=None)</code>  <code>staticmethod</code>","text":"<p>Load a standard flax checkpoint that\"s not saved with the msgpack streaming format.</p> Source code in <code>src/fjformer/checkpoint/streamer.py</code> <pre><code>@staticmethod\ndef load_flax_checkpoint(\n        path,\n        target=None,\n        shard_fns=None\n):\n    \"\"\" Load a standard flax checkpoint that\"s not saved with the\n        msgpack streaming format.\n    \"\"\"\n    with open(path, \"rb\") as fin:\n        encoded_bytes = fin.read()\n\n    state_dict = flax.serialization.msgpack_restore(encoded_bytes)\n    if shard_fns is not None:\n        shard_fns = to_state_dict(shard_fns)\n        state_dict = jax.tree_util.tree_map(lambda fn, x: fn(x), shard_fns, state_dict)\n\n    if target is None:\n        return state_dict\n    return from_state_dict(target, state_dict)\n</code></pre>"},{"location":"generated-checkpoint-streamer/#src.fjformer.checkpoint.streamer.CheckpointManager.load_state_checkpoint","title":"<code>load_state_checkpoint(load_type, load_path, state_target=None, state_shard_fns=None, disallow_state=False, mismatch_allowed=True)</code>  <code>classmethod</code>","text":"<p>The load_state_checkpoint function is used to checkpoint a checkpoint from disk.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Call the load_checkpoint function</p> required <code>load_type</code> <code>Literal['state', 'state_params', 'params', 'flax_params']</code> <p>Specify which part of state to checkpoint</p> required <code>load_path</code> <code>Union[str, PathLike]</code> <p>Specify where to checkpoint the model from</p> required <code>state_target</code> <p>Specify the target for the train state</p> <code>None</code> <code>state_shard_fns</code> <p>Specify the sharding function</p> <code>None</code> <code>disallow_state</code> <p>Prevent loading the entire state</p> <code>False</code> <code>mismatch_allowed</code> <code>bool</code> <p>when ever to allow shard func to be None</p> <code>True</code> <p>Returns:</p> Type Description <p>A tuple of two objects, the state and restored_params</p> Source code in <code>src/fjformer/checkpoint/streamer.py</code> <pre><code>@classmethod\ndef load_state_checkpoint(\n        cls,\n        load_type: Literal[\n            \"state\",\n            \"state_params\",\n            \"params\",\n            \"flax_params\"\n        ],\n        load_path: Union[str, os.PathLike],\n        state_target=None,\n        state_shard_fns=None,\n        disallow_state=False,\n        mismatch_allowed: bool = True\n):\n    \"\"\"\n    The load_state_checkpoint function is used to checkpoint a checkpoint from disk.\n\n    :param cls: Call the load_checkpoint function\n    :param load_type: Specify which part of state to checkpoint\n    :param load_path: Specify where to checkpoint the model from\n    :param state_target: Specify the target for the train state\n    :param state_shard_fns: Specify the sharding function\n    :param disallow_state: Prevent loading the entire state\n    :param mismatch_allowed: when ever to allow shard func to be None\n    :return: A tuple of two objects, the state and restored_params\n\n    \"\"\"\n    if state_target is not None:\n        params_target = state_target.params[\"params\"]\n    else:\n        params_target = None\n\n    if state_shard_fns is not None:\n        params_shard_fns = state_shard_fns.params[\"params\"]\n    else:\n        params_shard_fns = None\n\n    if disallow_state:\n        assert load_type != \"state\", \"Loading full state is not allowed!\"\n    state = None\n    restored_params = None\n    if load_type == \"state\":\n        state = cls.load_checkpoint(\n            path=load_path,\n            target=state_target,\n            shard_fns=state_shard_fns,\n            mismatch_allowed=mismatch_allowed\n        )\n    elif load_type == \"state_params\":\n        restored_params = cls.load_checkpoint(\n            path=load_path,\n            target=params_target,\n            shard_fns=params_shard_fns,\n            remove_dict_prefix=(\"params\", \"params\"),\n            mismatch_allowed=mismatch_allowed\n        )\n        restored_params = flax.core.frozen_dict.freeze(\n            {\"params\": restored_params}\n        )\n    elif load_type == \"params\":\n        restored_params = cls.load_checkpoint(\n            path=load_path,\n            target=params_target,\n            shard_fns=params_shard_fns,\n            mismatch_allowed=mismatch_allowed\n        )\n        restored_params = flax.core.frozen_dict.freeze(\n            {\"params\": restored_params}\n        )\n    elif load_type == \"flax_params\":\n        restored_params = cls.load_flax_checkpoint(\n            path=load_path,\n            target=params_target,\n            shard_fns=params_shard_fns,\n        )\n        restored_params = flax.core.frozen_dict.freeze(\n            {\"params\": restored_params}\n        )\n    else:\n        raise ValueError(f\"Invalid load_from type: {load_type}\")\n\n    return state, restored_params\n</code></pre>"},{"location":"generated-checkpoint-streamer/#src.fjformer.checkpoint.streamer.CheckpointManager.save_all","title":"<code>save_all(state, gather_fns, metadata=None, dataset=None, milestone=False)</code>","text":"<p>The save_all function saves the following:     - metadata.pkl (a pickle file containing a dictionary of metadata)     - dataset.pkl (a pickle file containing the training data)     - streaming_params_{step}.pkl or streaming_state_{step}.pkl         (depending on whether we want to save optimizer state or not,         this is a checkpoint that will not be overwritten by future checkpoints)</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the attributes and methods of the class</p> required <code>state</code> <code>PyTreeNode</code> <p>struct.PyTreeNode: Save the current state of the model</p> required <code>gather_fns</code> <p>Gather the state of the optimizer</p> required <code>metadata</code> <p>Save the metadata of the training</p> <code>None</code> <code>dataset</code> <p>Save the dataset to disk</p> <code>None</code> <code>milestone</code> <p>Determine whether the checkpoint is a milestone or not</p> <code>False</code> <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>src/fjformer/checkpoint/streamer.py</code> <pre><code>def save_all(\n        self,\n        state: struct.PyTreeNode,\n        gather_fns,\n        metadata=None,\n        dataset=None,\n        milestone=False\n):\n    \"\"\"\n    The save_all function saves the following:\n        - metadata.pkl (a pickle file containing a dictionary of metadata)\n        - dataset.pkl (a pickle file containing the training data)\n        - streaming_params_{step}.pkl or streaming_state_{step}.pkl\n            (depending on whether we want to save optimizer state or not,\n            this is a checkpoint that will not be overwritten by future checkpoints)\n\n    :param self: Access the attributes and methods of the class\n    :param state: struct.PyTreeNode: Save the current state of the model\n    :param gather_fns: Gather the state of the optimizer\n    :param metadata: Save the metadata of the training\n    :param dataset: Save the dataset to disk\n    :param milestone: Determine whether the checkpoint is a milestone or not\n    :return: Nothing\n\n    \"\"\"\n    step = int(jax.device_get(state.step))\n    if self.save_optimizer_state:\n        checkpoint_state = state\n        checkpoint_name = \"streaming_state\"\n        checkpoint_gather_fns = gather_fns\n    else:\n        checkpoint_state = state.params[\"params\"]\n        checkpoint_name = \"streaming_params\"\n        checkpoint_gather_fns = gather_fns.params[\"params\"]\n\n    if milestone:\n        # Save a milestone checkpoint that will not be overwritten\n        self.save_pickle(metadata, f\"metadata_{step}.pkl\")\n        self.save_pickle(dataset, f\"dataset_{step}.pkl\")\n        self.save_checkpoint(\n            checkpoint_state, f\"{checkpoint_name}_{step}\", checkpoint_gather_fns\n        )\n    else:\n        # Save a normal checkpoint that can be overwritten\n        self.save_pickle(metadata, \"metadata.pkl\")\n        self.save_pickle(dataset, \"dataset.pkl\")\n        self.save_checkpoint(\n            checkpoint_state, f\"{checkpoint_name}\", checkpoint_gather_fns\n        )\n</code></pre>"},{"location":"generated-checkpoint-streamer/#src.fjformer.checkpoint.streamer.CheckpointManager.save_pickle","title":"<code>save_pickle(obj, filename)</code>","text":"<p>The save_pickle function saves a Python object to disk using the pickle module.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>obj</code> <p>Pass the object that is to be pickled</p> required <code>filename</code> <code>Union[str, PathLike]</code> <p>Specify the name of the file to be saved</p> required <p>Returns:</p> Type Description <p>A pickle object</p> Source code in <code>src/fjformer/checkpoint/streamer.py</code> <pre><code>def save_pickle(\n        self,\n        obj,\n        filename: Union[str, os.PathLike]\n):\n    \"\"\"\n    The save_pickle function saves a Python object to disk using the pickle module.\n\n    :param self: Represent the instance of the class\n    :param obj: Pass the object that is to be pickled\n    :param filename: Specify the name of the file to be saved\n    :return: A pickle object\n\n    \"\"\"\n    import pickle\n\n    def save_pickle(obj_, path_):\n        with open(path_, \"wb\") as stream:\n            pickle.dump(obj_, stream)\n\n    if self.enable:\n        path = os.path.join(self.checkpoint_dir, filename)\n    else:\n        path = \"/dev/null\"\n    save_pickle(obj, path)\n</code></pre>"},{"location":"generated-func-_func/","title":"func._func","text":""},{"location":"generated-func-_func/#src.fjformer.func._func.average_metrics","title":"<code>average_metrics(metrics)</code>","text":"<p>The average_metrics function takes a list of metrics and averages them.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <p>Store the metrics for each batch</p> required <p>Returns:</p> Type Description <p>The mean of the metrics across all runs</p> Source code in <code>src/fjformer/func/_func.py</code> <pre><code>def average_metrics(metrics):\n    \"\"\"\n    The average_metrics function takes a list of metrics and averages them.\n\n    :param metrics: Store the metrics for each batch\n    :return: The mean of the metrics across all runs\n\n    \"\"\"\n    return jax.tree_map(\n        lambda *args: jnp.mean(jnp.stack(args)),\n        *metrics\n    )\n</code></pre>"},{"location":"generated-func-_func/#src.fjformer.func._func.fused_softmax","title":"<code>fused_softmax(x, axis=-1)</code>","text":"<p>The fused_softmax function is a fused version of the softmax function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>chex.Array: Specify the input to the function</p> required <code>axis</code> <code>int</code> <p>int: Specify the axis along which to apply the softmax function</p> <code>-1</code> <p>Returns:</p> Type Description <p>The same result as the softmax function</p> Source code in <code>src/fjformer/func/_func.py</code> <pre><code>def fused_softmax(x: chex.Array, axis: int = -1):\n    \"\"\"\n    The fused_softmax function is a fused version of the softmax function.\n\n    :param x: chex.Array: Specify the input to the function\n    :param axis: int: Specify the axis along which to apply the softmax function\n    :return: The same result as the softmax function\n\n    \"\"\"\n    return jnp.exp(jax.nn.log_softmax(x, axis=axis))\n</code></pre>"},{"location":"generated-func-_func/#src.fjformer.func._func.global_norm","title":"<code>global_norm(tree)</code>","text":"<p>Return the global L2 norm of a pytree.</p> Source code in <code>src/fjformer/func/_func.py</code> <pre><code>def global_norm(tree):\n    \"\"\" Return the global L2 norm of a pytree. \"\"\"\n    squared = jax.tree_util.tree_map(lambda x: jnp.sum(jnp.square(x)), tree)\n    flattened, _ = jax.flatten_util.ravel_pytree(squared)\n    return jnp.sqrt(jnp.sum(flattened))\n</code></pre>"},{"location":"generated-func-_func/#src.fjformer.func._func.transpose","title":"<code>transpose(array, dim0, dim1)</code>","text":"<p>The transpose function takes an array and two dimensions, and returns a new array with the specified dimensions transposed. The first dimension is given as a positive integer, where 0 represents the outermost dimension of the array. If the first dimension is negative, it counts from the end of the shape tuple; -2 is equivalent to len(shape) - 2. The second dimension may be specified in a similar way.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>Array</code> <p>chex.Array: Specify the array to be transposed</p> required <code>dim0</code> <code>int</code> <p>int: Specify the first dimension to be transposed</p> required <code>dim1</code> <code>int</code> <p>int: Specify the dimension of the array</p> required <p>Returns:</p> Type Description <p>A new array with the same data, but with axes permuted</p> Source code in <code>src/fjformer/func/_func.py</code> <pre><code>def transpose(array: chex.Array, dim0: int, dim1: int):\n    \"\"\"\n    The transpose function takes an array and two dimensions, and returns a new\n    array with the specified dimensions transposed. The first dimension is given as\n    a positive integer, where 0 represents the outermost dimension of the array. If\n    the first dimension is negative, it counts from the end of the shape tuple; -2\n    is equivalent to len(shape) - 2. The second dimension may be specified in a similar way.\n\n    :param array: chex.Array: Specify the array to be transposed\n    :param dim0: int: Specify the first dimension to be transposed\n    :param dim1: int: Specify the dimension of the array\n    :return: A new array with the same data, but with axes permuted\n\n    \"\"\"\n    dim0 = dim0 if dim0 &gt; 0 else array.ndim - dim0\n    dim1 = dim1 if dim1 &gt; 0 else array.ndim - dim1\n    perm = list(range(array.ndim))\n    perm[dim0], perm[dim1] = perm[dim1], perm[dim0]\n    return jnp.transpose(array, perm)\n</code></pre>"},{"location":"generated-func-loss_func/","title":"func.loss_func","text":""},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.SpecialLossNormalizingFactor","title":"<code>SpecialLossNormalizingFactor</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Specially calculated loss_normalizing_factors, that are not a constant.</p> <p>Attributes:   NUM_REAL_TARGET_TOKENS: Whether to divide the loss by the number of real     (non-padding) tokens in the current target batch. If     'decoder_loss_weights' are specified, it will be the sum of the weights.     Otherwise it will be the number of non-zero 'decoder_target_tokens'.   NUM_TOTAL_TARGET_TOKENS: Whether to divide the loss by the total number of     target tokens, i.e., batch_size * target_seq_length (including padding).   AVERAGE_PER_SEQUENCE: This will first compute the per-sequence loss     (averaged over the number of real target tokens in the sequence), and then     compute the average of that over the sequences. This can be preferable to     NUM_REAL_TARGET_TOKENS for finetuning, because it will weigh all examples     equally, regardless of sequence length (which can be especially important     for multi-task finetuning).</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>@enum.unique\nclass SpecialLossNormalizingFactor(enum.Enum):\n    \"\"\"Specially calculated loss_normalizing_factors, that are not a constant.\n\n    Attributes:\n      NUM_REAL_TARGET_TOKENS: Whether to divide the loss by the number of real\n        (non-padding) tokens in the current target batch. If\n        'decoder_loss_weights' are specified, it will be the sum of the weights.\n        Otherwise it will be the number of non-zero 'decoder_target_tokens'.\n      NUM_TOTAL_TARGET_TOKENS: Whether to divide the loss by the total number of\n        target tokens, i.e., batch_size * target_seq_length (including padding).\n      AVERAGE_PER_SEQUENCE: This will first compute the per-sequence loss\n        (averaged over the number of real target tokens in the sequence), and then\n        compute the average of that over the sequences. This can be preferable to\n        NUM_REAL_TARGET_TOKENS for finetuning, because it will weigh all examples\n        equally, regardless of sequence length (which can be especially important\n        for multi-task finetuning).\n    \"\"\"\n    NUM_REAL_TARGET_TOKENS = 1\n    NUM_TOTAL_TARGET_TOKENS = 2\n    AVERAGE_PER_SEQUENCE = 3\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.auxiliary_load_balancing_loss_func","title":"<code>auxiliary_load_balancing_loss_func(gate_logits, num_experts, top_k, attention_mask)</code>","text":"<p>Computes auxiliary load balancing loss as in Switch Transformer - implemented in JAX. See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between experts is too unbalanced.</p> <p>Parameters:</p> Name Type Description Default <code>gate_logits</code> <p>Union[Array, Tuple[Array]: Logits from the <code>gate</code>, should be a tuple of model.config.num_hidden_layers tensors of shape [batch_size X sequence_length, num_experts].</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Optional[chex.Array] : The attention_mask used in forward function shape [batch_size X sequence_length] if not None.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>chex.Array: The auxiliary loss.</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def auxiliary_load_balancing_loss_func(\n        gate_logits: chex.Array,\n        num_experts: int,\n        top_k: int,\n        attention_mask: Optional[chex.Array],\n) -&gt; chex.Array:\n    r\"\"\"\n    Computes auxiliary load balancing loss as in Switch Transformer - implemented in JAX.\n    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss\n    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n    experts is too unbalanced.\n    :param gate_logits :Union[Array, Tuple[Array]: Logits from the `gate`, should be a tuple of\n     model.config.num_hidden_layers tensors of shape [batch_size X sequence_length, num_experts].\n    :param num_experts :int:Number of experts.\n    :param top_k :int: The number of experts each token is routed to.\n    :param attention_mask: Optional[chex.Array] : The attention_mask used in forward function shape\n        [batch_size X sequence_length] if not None.\n\n    :return loss: chex.Array: The auxiliary loss.\n    \"\"\"\n    if gate_logits is None or not isinstance(gate_logits, tuple):\n        return jnp.array(0.0, dtype=jnp.float32)\n    elif isinstance(gate_logits, tuple):\n        concatenated_gate_logits = jnp.concatenate(gate_logits, axis=0)\n    else:\n        return jnp.array(0.0, dtype=jnp.float32)\n    routing_weights = jax.nn.softmax(\n        concatenated_gate_logits,\n        axis=-1\n    )\n\n    _, selected_experts = jax.lax.top_k(routing_weights, top_k)\n\n    expert_mask = jax.nn.one_hot(selected_experts, num_experts)\n\n    if attention_mask is None:\n        tokens_per_expert = jnp.mean(expert_mask.astype(jnp.float32), axis=0)\n        router_prob_per_expert = jnp.mean(routing_weights, axis=0)\n    else:\n        batch_size, sequence_length = attention_mask.shape\n        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n\n        expert_attention_mask = jnp.broadcast_to(\n            attention_mask[jnp.newaxis, :, :, jnp.newaxis, jnp.newaxis],\n            (num_hidden_layers, batch_size, sequence_length, top_k, num_experts)\n        ).reshape(-1, top_k, num_experts)\n\n        tokens_per_expert = jnp.sum(\n            expert_mask.astype(jnp.float32) * expert_attention_mask, axis=0\n        ) / jnp.sum(expert_attention_mask, axis=0)\n\n        router_per_expert_attention_mask = jnp.broadcast_to(\n            attention_mask[jnp.newaxis, :, :, jnp.newaxis],\n            (num_hidden_layers, batch_size, sequence_length, num_experts)\n        ).reshape(-1, num_experts)\n\n        # Compute the average probability of routing to these experts\n        router_prob_per_expert = jnp.sum(\n            routing_weights * router_per_expert_attention_mask,\n            axis=0\n        ) / jnp.sum(router_per_expert_attention_mask, axis=0)\n\n    overall_loss = jnp.sum(tokens_per_expert * jnp.expand_dims(router_prob_per_expert, 0))\n    return overall_loss * num_experts\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.binary_cross_entropy","title":"<code>binary_cross_entropy(labels, predictions)</code>","text":"<p>The binary_cross_entropy function computes the binary cross entropy loss between the labels and predictions. The function takes two arguments:     1) labels: a tensor of shape (batch_size, num_classes) containing the ground truth class indices for each     example in the batch.     2) predictions: a tensor of shape (batch_size, num_classes) containing model output probabilities for each     example in the batch.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <p>Specify the true class of each example</p> required <code>predictions</code> <p>Calculate the loss</p> required <p>Returns:</p> Type Description <p>The cross-entropy loss for binary classification</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def binary_cross_entropy(labels, predictions):\n    \"\"\"\n    The binary_cross_entropy function computes the binary cross entropy loss between\n    the labels and predictions. The function takes two arguments:\n        1) labels: a tensor of shape (batch_size, num_classes) containing the ground truth class indices for each\n        example in the batch.\n        2) predictions: a tensor of shape (batch_size, num_classes) containing model output probabilities for each\n        example in the batch.\n\n    :param labels: Specify the true class of each example\n    :param predictions: Calculate the loss\n    :return: The cross-entropy loss for binary classification\n\n    \"\"\"\n    labels = jax.nn.one_hot(labels, predictions.shape[-1])\n    return -np.mean(labels * np.log(predictions + 1e-8) + (1 - labels) * np.log(1 - predictions + 1e-8))\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.binary_cross_entropy_onehot","title":"<code>binary_cross_entropy_onehot(labels, predictions)</code>","text":"<p>The binary_cross_entropy_onehot function takes in two arguments:     labels: a 1D array of integers, where each integer is the index of the correct class for that example.     predictions: a 2D array of floats, where each row represents an example and each column represents a class.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <p>Determine the correct class of each image</p> required <code>predictions</code> <p>Calculate the loss</p> required <p>Returns:</p> Type Description <p>The binary cross entropy loss between the labels and predictions</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def binary_cross_entropy_onehot(labels, predictions):\n    \"\"\"\n    The binary_cross_entropy_onehot function takes in two arguments:\n        labels: a 1D array of integers, where each integer is the index of the correct class for that example.\n        predictions: a 2D array of floats, where each row represents an example and each column represents a class.\n\n    :param labels: Determine the correct class of each image\n    :param predictions: Calculate the loss\n    :return: The binary cross entropy loss between the labels and predictions\n\n    \"\"\"\n    labels = jax.nn.one_hot(labels, predictions.shape[-1])\n    return -np.mean(labels * np.log(predictions + 1e-8) + (1 - labels) * np.log(1 - predictions + 1e-8))\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.compute_weighted_cross_entropy","title":"<code>compute_weighted_cross_entropy(logits, targets, weights=None, label_smoothing=0.0, z_loss=0.0, loss_normalizing_factor=None)</code>","text":"<p>Compute weighted cross entropy and entropy for log probs and targets.</p> <p>Args:  logits: [batch, length, num_classes] float array.  targets: categorical targets [batch, length] int array.  weights: None or array of shape [batch, length].  label_smoothing: label smoothing constant, used to determine the on and off    values.  z_loss: coefficient for auxiliary z-loss loss term.  loss_normalizing_factor: Constant to divide loss by. If not specified, loss    will not be normalized. Intended for backward compatibility with T5-MTF    training. Should not normally be used.</p> <p>Returns:   Tuple of scalar loss, z_loss, and weight sum.</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def compute_weighted_cross_entropy(\n        logits: jnp.ndarray,\n        targets: jnp.ndarray,\n        weights: Optional[jnp.ndarray] = None,\n        label_smoothing: float = 0.0,\n        z_loss: float = 0.0,\n        loss_normalizing_factor: Optional[float] = None\n) -&gt; Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n    \"\"\"Compute weighted cross entropy and entropy for log probs and targets.\n\n    Args:\n     logits: [batch, length, num_classes] float array.\n     targets: categorical targets [batch, length] int array.\n     weights: None or array of shape [batch, length].\n     label_smoothing: label smoothing constant, used to determine the on and off\n       values.\n     z_loss: coefficient for auxiliary z-loss loss term.\n     loss_normalizing_factor: Constant to divide loss by. If not specified, loss\n       will not be normalized. Intended for backward compatibility with T5-MTF\n       training. Should not normally be used.\n\n    Returns:\n      Tuple of scalar loss, z_loss, and weight sum.\n    \"\"\"\n    if logits.ndim != targets.ndim + 1:\n        raise ValueError('Incorrect shapes. Got shape %s logits and %s targets' %\n                         (str(logits.shape), str(targets.shape)))\n    vocab_size = logits.shape[-1]\n    confidence = 1.0 - label_smoothing\n    low_confidence = (1.0 - confidence) / (vocab_size - 1)\n    normalizing_constant = -(\n            confidence * jnp.log(confidence) +\n            (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20))\n    soft_targets = common_utils.onehot(\n        targets, vocab_size, on_value=confidence, off_value=low_confidence)\n    total_loss, total_z_loss = cross_entropy_with_logits(\n        logits, soft_targets, z_loss=z_loss)\n    total_loss = total_loss - normalizing_constant\n\n    shape_dtype_struct = jax.eval_shape(lambda x: x, targets)\n    weight_sum = reduce(mul, shape_dtype_struct.shape, 1)\n    if weights is not None:\n        total_loss = total_loss * weights\n        total_z_loss = total_z_loss * weights\n        weight_sum = jnp.sum(weights)\n\n    # By default, we do not normalize loss based on anything.\n    # We don't normalize based on batch size because the optimizers we use are\n    # pretty much scale invariant, so this simplifies things.\n    # We don't normalize based on number of non-padding tokens in order to treat\n    # each token as equally important regardless of sequence length.\n    if loss_normalizing_factor is not None:\n        total_loss /= loss_normalizing_factor\n        total_z_loss /= loss_normalizing_factor\n    return jnp.sum(total_loss), jnp.sum(total_z_loss), weight_sum\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.compute_weighted_cross_entropy_and_accuracy","title":"<code>compute_weighted_cross_entropy_and_accuracy(logits, targets, weights=None, label_smoothing=0.0, z_loss=0.0, loss_normalizing_factor=None)</code>","text":"<p>Compute weighted cross entropy and entropy for log probs and targets.</p> <p>Args:  logits: [batch, length, num_classes] float array.  targets: categorical targets [batch, length] int array.  weights: None or array of shape [batch, length].  label_smoothing: label smoothing constant, used to determine the on and off    values.  z_loss: coefficient for auxiliary z-loss loss term.  loss_normalizing_factor: Constant to divide loss by. If not specified, loss    will not be normalized. Intended for backward compatibility with T5-MTF    training. Should not normally be used.</p> <p>Returns:   Tuple of scalar loss, z_loss, weight sum and accuracy</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def compute_weighted_cross_entropy_and_accuracy(\n        logits: jnp.ndarray,\n        targets: jnp.ndarray,\n        weights: Optional[jnp.ndarray] = None,\n        label_smoothing: float = 0.0,\n        z_loss: float = 0.0,\n        loss_normalizing_factor: Optional[float] = None,\n) -&gt; Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n    \"\"\"Compute weighted cross entropy and entropy for log probs and targets.\n\n    Args:\n     logits: [batch, length, num_classes] float array.\n     targets: categorical targets [batch, length] int array.\n     weights: None or array of shape [batch, length].\n     label_smoothing: label smoothing constant, used to determine the on and off\n       values.\n     z_loss: coefficient for auxiliary z-loss loss term.\n     loss_normalizing_factor: Constant to divide loss by. If not specified, loss\n       will not be normalized. Intended for backward compatibility with T5-MTF\n       training. Should not normally be used.\n\n    Returns:\n      Tuple of scalar loss, z_loss, weight sum and accuracy\n    \"\"\"\n    total_loss, total_z_loss, weight_sum = compute_weighted_cross_entropy(\n        logits, targets, weights, label_smoothing, z_loss, loss_normalizing_factor)\n\n    predictions = jnp.argmax(logits, axis=-1)\n    correct_predictions = jnp.equal(predictions, targets).astype(jnp.float32)\n    accuracy = jnp.sum(correct_predictions * weights) / weight_sum\n\n    return total_loss, total_z_loss, weight_sum, accuracy\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.convert_special_loss_normalizing_factor_to_enum","title":"<code>convert_special_loss_normalizing_factor_to_enum(x)</code>","text":"<p>Converts stringified version of LNF to an enum.</p> <p>This is useful because gin dynamic registration does not (currently) have support for enum.</p> <p>Args:   x: stringified version of SpecialLossNormalizingFactor enum.</p> <p>Returns:   SpecialLossNormalizingFactor enum instance.</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def convert_special_loss_normalizing_factor_to_enum(\n        x: str) -&gt; SpecialLossNormalizingFactor:\n    \"\"\"Converts stringified version of LNF to an enum.\n\n    This is useful because gin dynamic registration does not (currently)\n    have support for enum.\n\n    Args:\n      x: stringified version of SpecialLossNormalizingFactor enum.\n\n    Returns:\n      SpecialLossNormalizingFactor enum instance.\n    \"\"\"\n    x = x.upper()\n    if x == 'NUM_REAL_TARGET_TOKENS':\n        return SpecialLossNormalizingFactor.NUM_REAL_TARGET_TOKENS\n    if x == 'NUM_TOTAL_TARGET_TOKENS':\n        return SpecialLossNormalizingFactor.NUM_TOTAL_TARGET_TOKENS\n    if x == 'AVERAGE_PER_SEQUENCE':\n        return SpecialLossNormalizingFactor.AVERAGE_PER_SEQUENCE\n    raise ValueError(\n        'Could not convert string \\\"%s\\\" to SpecialLossNormalizingFactor' % x)\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.cross_entropy","title":"<code>cross_entropy(labels, predictions, ignore_index=None)</code>","text":"<p>The cross_entropy function computes the cross entropy loss between a set of labels and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <p>Calculate the cross entropy loss</p> required <code>predictions</code> <p>Calculate the log_softmax</p> required <code>ignore_index</code> <p>Mask out the loss from a specific class</p> <code>None</code> <p>Returns:</p> Type Description <p>The average cross entropy over the batch</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def cross_entropy(labels, predictions, ignore_index=None):\n    \"\"\"\n    The cross_entropy function computes the cross entropy loss between a set of labels and predictions.\n\n    :param labels: Calculate the cross entropy loss\n    :param predictions: Calculate the log_softmax\n    :param ignore_index: Mask out the loss from a specific class\n    :return: The average cross entropy over the batch\n\n    \"\"\"\n    labels = jax.nn.one_hot(labels, predictions.shape[-1])\n    if ignore_index is not None:\n        mask = np.ones_like(labels)\n        mask = np.where(labels == ignore_index, 0, mask)\n        labels = labels * mask\n        predictions = predictions * mask\n    log_softmax = predictions - logsumexp(predictions, axis=-1, keepdims=True)\n    return -np.sum(labels * log_softmax) / labels.shape[0]\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.cross_entropy_loss_and_accuracy","title":"<code>cross_entropy_loss_and_accuracy(logits, tokens, valid=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Array</code> <p>Logits</p> required <code>tokens</code> <code>Array</code> <p>labels</p> required <code>valid</code> <code>Array</code> <p>attention_mask</p> <code>None</code> <p>Returns:</p> Type Description <p>loss and accuracy</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def cross_entropy_loss_and_accuracy(logits: chex.Array, tokens: chex.Array, valid: chex.Array = None):\n    \"\"\"\n\n    :param logits: Logits\n    :param tokens: labels\n    :param valid: attention_mask\n    :return: loss and accuracy\n    \"\"\"\n    if valid is None:\n        valid = jnp.ones(tokens.shape[:2])\n    valid = valid.astype(jnp.float32)\n    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n    logits = logits.astype(jnp.float32)  # for numerical stability\n    token_log_prob = jnp.squeeze(\n        jnp.take_along_axis(\n            jax.nn.log_softmax(logits, axis=-1),\n            jnp.expand_dims(tokens, -1),\n            axis=-1,\n        ),\n        -1,\n    )\n    token_log_prob = jnp.where(valid &gt; 0.0, token_log_prob, jnp.array(0.0))\n    loss = -jnp.mean(jnp.sum(token_log_prob, axis=-1) / valid_text_length)\n    correct = jnp.where(\n        valid &gt; 0.0,\n        jnp.argmax(logits, axis=-1) == tokens,\n        jnp.array(False)\n    )\n    accuracy = jnp.mean(jnp.sum(correct, axis=-1) / valid_text_length)\n    return loss, accuracy\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.cross_entropy_onehot","title":"<code>cross_entropy_onehot(labels, predictions)</code>","text":"<p>The cross_entropy_onehot function takes two arguments: labels - a 1D array of integers in the range [0, num_classes) predictions - a 2D array of floats with shape (num_examples, num_classes) The function returns the cross entropy loss between labels and predictions. The loss is averaged over all examples.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <p>Compute the one-hot encoding of the labels</p> required <code>predictions</code> <p>Calculate the log_softmax</p> required <p>Returns:</p> Type Description <p>The cross entropy loss</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def cross_entropy_onehot(labels, predictions):\n    \"\"\"\n    The cross_entropy_onehot function takes two arguments:\n    labels - a 1D array of integers in the range [0, num_classes)\n    predictions - a 2D array of floats with shape (num_examples, num_classes)\n    The function returns the cross entropy loss between labels and predictions. The loss is averaged over all examples.\n\n    :param labels: Compute the one-hot encoding of the labels\n    :param predictions: Calculate the log_softmax\n    :return: The cross entropy loss\n\n    \"\"\"\n    labels = jax.nn.one_hot(labels, predictions.shape[-1])\n    log_softmax = predictions - logsumexp(predictions, axis=-1, keepdims=True)\n    return -np.sum(labels * log_softmax) / labels.shape[0]\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.cross_entropy_with_logits","title":"<code>cross_entropy_with_logits(logits, targets, z_loss)</code>","text":"<p>Computes cross entropy loss with stable custom gradient.</p> <p>Computes a stabilized-gradient version of:   -jnp.sum(targets * nn.log_softmax(logits), axis=-1)</p> <p>If z_loss &gt; 0, then an auxiliary loss equal to z_loss*log(z)^2 will be added to the cross entropy loss (z = softmax normalization constant). The two uses of z_loss are: 1. To keep the logits from drifting too far from zero, which can cause    unacceptable roundoff errors in bfloat16. 2. To encourage the logits to be normalized log-probabilities.</p> <p>Args:   logits: [batch, length, num_classes] float array.   targets: categorical one-hot targets [batch, length, num_classes] float     array.   z_loss: coefficient for auxilliary z-loss loss term.</p> <p>Returns:   tuple with the total loss and the z_loss, both   float arrays with shape [batch, length].</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>@jax.custom_vjp\ndef cross_entropy_with_logits(logits: jnp.ndarray, targets: jnp.ndarray,\n                              z_loss: float) -&gt; Tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"Computes cross entropy loss with stable custom gradient.\n\n    Computes a stabilized-gradient version of:\n      -jnp.sum(targets * nn.log_softmax(logits), axis=-1)\n\n    If z_loss &gt; 0, then an auxiliary loss equal to z_loss*log(z)^2\n    will be added to the cross entropy loss (z = softmax normalization constant).\n    The two uses of z_loss are:\n    1. To keep the logits from drifting too far from zero, which can cause\n       unacceptable roundoff errors in bfloat16.\n    2. To encourage the logits to be normalized log-probabilities.\n\n    Args:\n      logits: [batch, length, num_classes] float array.\n      targets: categorical one-hot targets [batch, length, num_classes] float\n        array.\n      z_loss: coefficient for auxilliary z-loss loss term.\n\n    Returns:\n      tuple with the total loss and the z_loss, both\n      float arrays with shape [batch, length].\n    \"\"\"\n    logits_sum = jax.scipy.special.logsumexp(logits, axis=-1, keepdims=True)\n    log_softmax = logits - logits_sum\n    loss = -jnp.sum(targets * log_softmax, axis=-1)\n    # Add auxilliary z-loss term.\n    log_z = jnp.squeeze(logits_sum, axis=-1)\n    total_z_loss = z_loss * jax.lax.square(log_z)\n    loss += total_z_loss\n    return loss, total_z_loss\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.fused_cross_entropy_loss_and_accuracy","title":"<code>fused_cross_entropy_loss_and_accuracy(logits, tokens, valid=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Array</code> <p>Logits</p> required <code>tokens</code> <code>Array</code> <p>labels</p> required <code>valid</code> <code>Array</code> <p>attention_mask</p> <code>None</code> <p>Returns:</p> Type Description <p>loss and accuracy</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def fused_cross_entropy_loss_and_accuracy(logits: chex.Array, tokens: chex.Array, valid: chex.Array = None):\n    \"\"\"\n\n    :param logits: Logits\n    :param tokens: labels\n    :param valid: attention_mask\n    :return: loss and accuracy\n    \"\"\"\n    if valid is None:\n        valid = jnp.ones(tokens.shape[:2])\n    valid = valid.astype(jnp.float32)\n    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n    logits = logits.astype(jnp.float32)\n    log_probs = jax.nn.log_softmax(logits, axis=-1)\n    token_log_prob = jnp.squeeze(\n        jnp.take_along_axis(\n            log_probs,\n            jnp.expand_dims(tokens, -1),\n            axis=-1,\n        ),\n        -1,\n    )\n    token_log_prob = jnp.where(valid &gt; 0.0, token_log_prob, jnp.array(0.0))\n    loss = -jnp.mean(jnp.sum(token_log_prob, axis=-1) / valid_text_length)\n    correct = jnp.where(\n        valid &gt; 0.0,\n        jnp.argmax(logits, axis=-1) == tokens,\n        jnp.array(False)\n    )\n    accuracy = jnp.mean(jnp.sum(correct, axis=-1) / valid_text_length)\n    return loss, accuracy\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.get_loss_normalizing_factor_and_weights","title":"<code>get_loss_normalizing_factor_and_weights(loss_normalizing_factor, batch)</code>","text":"<p>Get the float loss_normalizing_factor and loss weights.</p> <p>If loss_normalizing_factor is float or None, this will simply return the input loss_normalizing_factor and batch.</p> <p>If loss_normalizing_factor is a SpecialLossNormalizingFactor, it will return a float loss_normalizing_factor and loss weights corresponding to the special LNF. See SpecialLossNormalizingFactor for more details.</p> <p>Args:   loss_normalizing_factor: The input LNF, which may be a float, None, or     SpecialLossNormalizingFactor (or a stringified SLNF).   batch: Input data batch.</p> <p>Returns:   Tuple of (output_loss_normalizing_factor, loss_weights).     'output_loss_normalizing_factor' is a scalar float (Python float     or jnp float).     'loss_weights' is the per token loss weight JNP array.</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def get_loss_normalizing_factor_and_weights(\n        loss_normalizing_factor: Optional[Union[float, int, str,\n        SpecialLossNormalizingFactor]],\n        batch: Mapping[str, jnp.ndarray]\n):\n    \"\"\"Get the float loss_normalizing_factor and loss weights.\n\n    If loss_normalizing_factor is float or None, this will simply return the\n    input loss_normalizing_factor and batch.\n\n    If loss_normalizing_factor is a SpecialLossNormalizingFactor, it will\n    return a float loss_normalizing_factor and loss weights corresponding to\n    the special LNF. See SpecialLossNormalizingFactor for more details.\n\n    Args:\n      loss_normalizing_factor: The input LNF, which may be a float, None, or\n        SpecialLossNormalizingFactor (or a stringified SLNF).\n      batch: Input data batch.\n\n    Returns:\n      Tuple of (output_loss_normalizing_factor, loss_weights).\n        'output_loss_normalizing_factor' is a scalar float (Python float\n        or jnp float).\n        'loss_weights' is the per token loss weight JNP array.\n    \"\"\"\n\n    loss_weights = batch.get('decoder_loss_weights', None)\n    if (loss_normalizing_factor is None or\n            not isinstance(loss_normalizing_factor,\n                           (str, SpecialLossNormalizingFactor))):\n        return (loss_normalizing_factor, loss_weights)\n\n    if isinstance(loss_normalizing_factor, str):\n        loss_normalizing_factor = convert_special_loss_normalizing_factor_to_enum(\n            loss_normalizing_factor)\n\n    # If `loss_weights` are not provided, we assume that the padding id is 0 and\n    # that non-padding tokens in the decoder all correspond to the positions\n    # where loss should be taken. If more fine-grained behavior (e.g., taking\n    # loss on subset of 'decoder_target_tokens') is desired, provide\n    # `loss_weights` that account for this.\n    if loss_weights is None:\n        loss_weights = jnp.asarray(batch['decoder_target_tokens'] &gt; 0, jnp.float32)\n\n    output_normalizing_factor = None\n    if loss_normalizing_factor == SpecialLossNormalizingFactor.NUM_REAL_TARGET_TOKENS:\n        output_normalizing_factor = jnp.sum(loss_weights)\n    elif loss_normalizing_factor == SpecialLossNormalizingFactor.NUM_TOTAL_TARGET_TOKENS:\n        output_normalizing_factor = np.prod(batch['decoder_target_tokens'].shape)\n    elif loss_normalizing_factor == SpecialLossNormalizingFactor.AVERAGE_PER_SEQUENCE:\n        if 'decoder_segment_ids' in batch:  # is packed\n            norm_vec = _sum_weights_per_segment(batch['decoder_positions'],\n                                                batch['decoder_segment_ids'],\n                                                loss_weights)\n        else:\n            norm_vec = jnp.sum(loss_weights, axis=-1, keepdims=True)\n        # Handle divide-by-zero.\n        loss_weights = jnp.nan_to_num(\n            loss_weights / norm_vec, nan=0, posinf=0, neginf=0)\n        output_normalizing_factor = jnp.sum(loss_weights)\n    else:\n        raise ValueError('Unsupported value of loss_normalizing_factor: %s' %\n                         str(loss_normalizing_factor))\n\n    return output_normalizing_factor, loss_weights\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.hinge","title":"<code>hinge(labels, predictions)</code>","text":"<p>The hinge function is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs). For an intended output t = + 1 or - 1 and a classifier score y, the hinge loss of the prediction y is defined as:</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <p>Calculate the hinge loss</p> required <code>predictions</code> <p>Calculate the hinge loss</p> required <p>Returns:</p> Type Description <p>The average loss over the training set</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def hinge(labels, predictions):\n    \"\"\"\n    The hinge function is a loss function used for training classifiers.\n    The hinge loss is used for &amp;quot;maximum-margin&amp;quot; classification, most notably\n    for support vector machines (SVMs).\n    For an intended output t = + 1 or - 1 and a classifier score y, the hinge loss of the prediction y is defined as:\n\n    :param labels: Calculate the hinge loss\n    :param predictions: Calculate the hinge loss\n    :return: The average loss over the training set\n\n    \"\"\"\n    return np.mean(np.maximum(0, 1 - labels * predictions))\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.l2","title":"<code>l2(labels, predictions)</code>","text":"<p>The l2 function computes the sum of squared differences between labels and predictions.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <p>Calculate the difference between the labels and predictions</p> required <code>predictions</code> <p>Calculate the difference between the labels and predictions</p> required <p>Returns:</p> Type Description <p>The sum of the squared differences between labels and predictions</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def l2(labels, predictions):\n    \"\"\"\n    The l2 function computes the sum of squared differences between labels and predictions.\n\n    :param labels: Calculate the difference between the labels and predictions\n    :param predictions: Calculate the difference between the labels and predictions\n    :return: The sum of the squared differences between labels and predictions\n\n    \"\"\"\n    return np.sum((labels - predictions) ** 2)\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.log_cosh","title":"<code>log_cosh(labels, predictions)</code>","text":"<p>The log_cosh function is a loss function that takes in two arguments: labels and predictions. The log_cosh function returns the mean of the logarithm of the hyperbolic cosine of (predictions - labels). This loss function is used to measure how well our model performs on data it has not seen before.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <p>Pass the actual values of the data</p> required <code>predictions</code> <p>Pass the predictions of your model</p> required <p>Returns:</p> Type Description <p>The logarithm of the hyperbolic cosine of the prediction error</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def log_cosh(labels, predictions):\n    \"\"\"\n    The log_cosh function is a loss function that takes in two arguments: labels and predictions.\n    The log_cosh function returns the mean of the logarithm of the hyperbolic cosine of (predictions - labels).\n    This loss function is used to measure how well our model performs on data it has not seen before.\n\n    :param labels: Pass the actual values of the data\n    :param predictions: Pass the predictions of your model\n    :return: The logarithm of the hyperbolic cosine of the prediction error\n\n    \"\"\"\n\n    def cosh(x):\n        return (np.exp(x) + np.exp(-x)) / 2\n\n    return np.mean(np.log(cosh(predictions - labels)))\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.mae","title":"<code>mae(labels, predictions)</code>","text":"<p>The mae function calculates the mean absolute error between two lists of numbers.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <p>Store the actual values of the data</p> required <code>predictions</code> <p>Store the predictions from the model</p> required <p>Returns:</p> Type Description <p>The mean absolute error between the labels and predictions</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def mae(labels, predictions):\n    \"\"\"\n    The mae function calculates the mean absolute error between two lists of numbers.\n\n    :param labels: Store the actual values of the data\n    :param predictions: Store the predictions from the model\n    :return: The mean absolute error between the labels and predictions\n\n    \"\"\"\n    return np.mean(np.abs(labels - predictions))\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.mse","title":"<code>mse(labels, predictions)</code>","text":"<p>The mse function computes the mean squared error between two arrays.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <p>Pass in the actual values of the data, and predictions is used to pass in our predicted values</p> required <code>predictions</code> <p>Store the predictions made by the model</p> required <p>Returns:</p> Type Description <p>The mean squared error (mse) between the labels and predictions</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def mse(labels, predictions):\n    \"\"\"\n    The mse function computes the mean squared error between two arrays.\n\n    :param labels: Pass in the actual values of the data, and predictions is used to pass in our predicted values\n    :param predictions: Store the predictions made by the model\n    :return: The mean squared error (mse) between the labels and predictions\n\n    \"\"\"\n    return np.mean((labels - predictions) ** 2)\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.mse_loss","title":"<code>mse_loss(val, target, valid=None)</code>","text":"<p>The mse_loss function computes the mean squared error between two arrays.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <p>Store the output of the model</p> required <code>target</code> <p>Calculate the loss</p> required <code>valid</code> <p>Mask out the loss of certain pixels</p> <code>None</code> <p>Returns:</p> Type Description <p>The mean square error of the input and target</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def mse_loss(val, target, valid=None):\n    \"\"\"\n    The mse_loss function computes the mean squared error between two arrays.\n\n    :param val: Store the output of the model\n    :param target: Calculate the loss\n    :param valid: Mask out the loss of certain pixels\n    :return: The mean square error of the input and target\n\n    \"\"\"\n    if valid is None:\n        valid = jnp.ones((*target.shape[:2], 1))\n    valid = valid.astype(jnp.float32)\n    loss = jnp.mean(\n        jnp.where(\n            valid &gt; 0.0,\n            jnp.square(val - target),\n            0.0\n        )\n    )\n    return loss\n</code></pre>"},{"location":"generated-func-loss_func/#src.fjformer.func.loss_func.nll","title":"<code>nll(labels, predictions)</code>","text":"<p>The nll function computes the negative log likelihood of a set of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <p>Calculate the loss</p> required <code>predictions</code> <p>Calculate the loss</p> required <p>Returns:</p> Type Description <p>The negative log likelihood of the predictions</p> Source code in <code>src/fjformer/func/loss_func.py</code> <pre><code>def nll(labels, predictions):\n    \"\"\"\n    The nll function computes the negative log likelihood of a set of predictions.\n\n    :param labels: Calculate the loss\n    :param predictions: Calculate the loss\n    :return: The negative log likelihood of the predictions\n\n    \"\"\"\n    return -np.sum(labels * np.log(predictions + 1e-8))\n</code></pre>"},{"location":"generated-monitor-tracker/","title":"monitor.tracker","text":""},{"location":"generated-monitor-tracker/#src.fjformer.monitor.tracker.get_memory_information","title":"<code>get_memory_information(dir_prefix='/dev/shm')</code>","text":"<p>The get_memory_information function is a wrapper around the go tool pprof command. It takes in an optional argument, dir_prefix, which defaults to /dev/shm. The function then runs the go tool pprof command with arguments -tags and {dir_prefix}/memory.prof. The output of this command is captured and returned as a string.</p> <p>Parameters:</p> Name Type Description Default <code>dir_prefix</code> <code>str</code> <p>str: Specify the directory prefix for</p> <code>'/dev/shm'</code> <p>Returns:</p> Type Description <code>str</code> <p>A string that contains the memory profile</p> Source code in <code>src/fjformer/monitor/tracker.py</code> <pre><code>def get_memory_information(dir_prefix: str = \"/dev/shm\") -&gt; str:\n    \"\"\"\n    The get_memory_information function is a wrapper around the go tool pprof command.\n    It takes in an optional argument, dir_prefix, which defaults to /dev/shm.\n    The function then runs the go tool pprof command with arguments -tags and {dir_prefix}/memory.prof.\n    The output of this command is captured and returned as a string.\n\n    :param dir_prefix: str: Specify the directory prefix for\n    :return: A string that contains the memory profile\n\n    \"\"\"\n    return subprocess.run(\n        args=[\"go\", \"tool\", \"pprof\", \"-tags\", f\"{dir_prefix}/memory.prof\"],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.DEVNULL,\n    ).stdout.decode(\"utf-8\")\n</code></pre>"},{"location":"generated-monitor-tracker/#src.fjformer.monitor.tracker.initialise_tracking","title":"<code>initialise_tracking(interval=1.0, dir_prefix='/dev/shm')</code>","text":"<p>The initialise_tracking function starts a daemon thread that periodically saves the memory profile to disk. The outer function starts the daemon thread and returns a context manager that stops it when the context exits.  The inner function uses posix.rename() to atomically replace an existing file, so we can be sure that any given memory profile was taken at some point during the lifetime of its context.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>float</code> <p>float: Set the time interval between each memory profile</p> <code>1.0</code> <code>dir_prefix</code> <code>str</code> <p>str: Specify the directory where the memory profile will be saved</p> <code>'/dev/shm'</code> <p>Returns:</p> Type Description <code>None</code> <p>A thread object</p> Source code in <code>src/fjformer/monitor/tracker.py</code> <pre><code>def initialise_tracking(interval: float = 1., dir_prefix: str = \"/dev/shm\") -&gt; None:\n    \"\"\"\n    The initialise_tracking function starts a daemon thread that periodically saves the memory profile to disk.\n    The outer function starts the daemon thread and returns a context manager that stops it when\n    the context exits.  The inner function uses posix.rename() to atomically replace an existing file,\n    so we can be sure that any given memory profile was taken at some point during the lifetime of its\n    context.\n\n    :param interval: float: Set the time interval between each memory profile\n    :param dir_prefix: str: Specify the directory where the memory profile will be saved\n    :return: A thread object\n\n    \"\"\"\n\n    def inner():\n        \"\"\"\n        The inner function is a daemon thread that periodically saves the memory profile to disk.\n        The outer function starts the daemon thread and returns a context manager that stops it when\n        the context exits.  The inner function uses posix.rename() to atomically replace an existing file,\n        so we can be sure that any given memory profile was taken at some point during the lifetime of its\n        context.\n\n        :return: A thread object\n\n        \"\"\"\n        while True:\n            jax.profiler.save_device_memory_profile(f\"{dir_prefix}/memory.prof.new\")\n            if posix is not None:\n                os.rename(f\"{dir_prefix}/memory.prof.new\", f\"{dir_prefix}/memory.prof\")\n            else:\n                posix.rename(f\"{dir_prefix}/memory.prof.new\", f\"{dir_prefix}/memory.prof\")\n            time.sleep(interval)\n\n    thread = threading.Thread(target=inner, daemon=True)\n    thread.start()\n</code></pre>"},{"location":"generated-monitor-tracker/#src.fjformer.monitor.tracker.is_notebook","title":"<code>is_notebook()</code>","text":"<p>Returns True if the code is being run in a notebook, False otherwise.</p> Source code in <code>src/fjformer/monitor/tracker.py</code> <pre><code>def is_notebook():\n    \"\"\"Returns True if the code is being run in a notebook, False otherwise.\"\"\"\n    return os.environ.get(\"IPYTHON\") is not None\n</code></pre>"},{"location":"generated-monitor-tracker/#src.fjformer.monitor.tracker.run","title":"<code>run(note_book=None, interval=1, dir_prefix='/dev/shm', dpr=True)</code>","text":"<p>The run function is a simple wrapper around the go tool pprof command. It runs the command every interval seconds and prints out its output to stdout. If you are running this in a Jupyter notebook, it will print to an IPython display object instead of stdout.</p> <p>Parameters:</p> Name Type Description Default <code>note_book</code> <p>Determine whether the program is running in a notebook or not</p> <code>None</code> <code>interval</code> <code>float</code> <p>float: Specify the interval between each refresh</p> <code>1</code> <code>dir_prefix</code> <code>str</code> <p>str: Specify the directory where the memory</p> <code>'/dev/shm'</code> <code>dpr</code> <p>Determine whether to display the output in a notebook or not</p> <code>True</code> <p>Returns:</p> Type Description <p>The output of the pprof command</p> Source code in <code>src/fjformer/monitor/tracker.py</code> <pre><code>def run(note_book=None, interval: float = 1, dir_prefix: str = \"/dev/shm\", dpr=True):\n    \"\"\"\n    The run function is a simple wrapper around the go tool pprof command.\n    It runs the command every interval seconds and prints out its output to stdout.\n    If you are running this in a Jupyter notebook, it will print to an IPython display object instead of stdout.\n\n\n    :param note_book: Determine whether the program is running in a notebook or not\n    :param interval: float: Specify the interval between each refresh\n    :param dir_prefix: str: Specify the directory where the memory\n    :param dpr: Determine whether to display the output in a notebook or not\n    :return: The output of the pprof command\n\n    \"\"\"\n    if note_book is None:\n        note_book = is_notebook()\n    std = curses.initscr() if not note_book else None\n    try:\n        while True:\n            if not note_book and dpr:\n                std.clear()\n            output = subprocess.run(\n                args=[\"go\", \"tool\", \"pprof\", \"-tags\", f\"{dir_prefix}/memory.prof\"],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.DEVNULL,\n            ).stdout.decode(\"utf-8\")\n            if not note_book and dpr:\n                std.addstr(output)\n                std.refresh()\n            if note_book and dpr:\n                IPython.display.clear_output(True)\n                print(output)\n\n            with open(f\"{dir_prefix}/memory.json\", \"w\") as fin:\n                json.dump({\n                    \"log\": output\n                }, fin)\n            time.sleep(interval)\n    except KeyboardInterrupt:\n        curses.endwin()\n</code></pre>"},{"location":"generated-monitor-tracker/#src.fjformer.monitor.tracker.threaded_log","title":"<code>threaded_log(interval=1.0, dir_prefix='/dev/shm', save_mem_json=False)</code>","text":"<p>The threaded_log function is a wrapper around the get_memory_information function. It allows you to monitor your memory usage in real time, and optionally save it to a JSON file. The threaded_log function returns a threading.Thread object that can be started with .start() and stopped with .join().</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>float</code> <p>float: Set the time interval between each memory log</p> <code>1.0</code> <code>dir_prefix</code> <code>str</code> <p>str: Specify the directory to save the memory</p> <code>'/dev/shm'</code> <code>save_mem_json</code> <code>bool</code> <p>bool: Save the memory information to a json file</p> <code>False</code> <p>Returns:</p> Type Description <code>Thread</code> <p>A threading</p> Source code in <code>src/fjformer/monitor/tracker.py</code> <pre><code>def threaded_log(interval: float = 1., dir_prefix: str = \"/dev/shm\", save_mem_json: bool = False) -&gt; threading.Thread:\n    \"\"\"\n    The threaded_log function is a wrapper around the get_memory_information function.\n    It allows you to monitor your memory usage in real time, and optionally save it to a JSON file.\n    The threaded_log function returns a threading.Thread object that can be started with .start() and stopped with .join().\n\n\n    :param interval: float: Set the time interval between each memory log\n    :param dir_prefix: str: Specify the directory to save the memory\n    :param save_mem_json: bool: Save the memory information to a json file\n    :return: A threading\n\n    \"\"\"\n    note_book = is_notebook()\n\n    def show_():\n\n        std = curses.initscr() if not note_book else None\n        try:\n            while True:\n                mem_info = get_memory_information()\n                if not note_book:\n                    std.clear()\n                    std.addstr(mem_info)\n                    std.refresh()\n                if note_book:\n                    IPython.display.clear_output(True)\n                    print(mem_info)\n                if save_mem_json:\n                    with open(f\"{dir_prefix}/memory.json\", \"w\") as fin:\n                        json.dump({\n                            \"log\": mem_info\n                        }, fin)\n                time.sleep(interval)\n        except KeyboardInterrupt:\n            curses.endwin()\n\n    thread = threading.Thread(\n        target=show_\n    )\n    return thread\n</code></pre>"},{"location":"generated-optimizers-adafactor/","title":"optimizers.adafactor","text":""},{"location":"generated-optimizers-adafactor/#src.fjformer.optimizers.adafactor.get_adafactor_with_cosine_scheduler","title":"<code>get_adafactor_with_cosine_scheduler(steps, learning_rate=5e-05, min_dim_size_to_factor=128, decay_rate=0.8, decay_offset=0, multiply_by_parameter_scale=True, clipping_threshold=1.0, momentum=None, dtype_momentum=jnp.float32, weight_decay_rate=None, eps=1e-30, factored=True, gradient_accumulation_steps=1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>gradient_accumulation_steps</code> <code>int</code> <code>1</code> <code>steps</code> <code>int</code> required <code>learning_rate</code> <code>5e-05</code> <code>weight_decay</code> required <code>min_dim_size_to_factor</code> <code>int</code> <code>128</code> <code>decay_rate</code> <code>float</code> <code>0.8</code> <code>decay_offset</code> <code>int</code> <code>0</code> <code>multiply_by_parameter_scale</code> <code>float</code> <code>True</code> <code>clipping_threshold</code> <code>Optional[float]</code> <code>1.0</code> <code>momentum</code> <code>Optional[float]</code> <code>None</code> <code>dtype_momentum</code> <code>ArrayDType</code> <code>float32</code> <code>weight_decay_rate</code> <code>Optional[float]</code> <code>None</code> <code>eps</code> <code>float</code> <code>1e-30</code> <code>factored</code> <code>bool</code> <code>True</code> <code>weight_decay_mask</code> required <p>Returns:</p> Type Description <p>Optimizer and Scheduler</p> Source code in <code>src/fjformer/optimizers/adafactor.py</code> <pre><code>def get_adafactor_with_cosine_scheduler(\n        steps: int,\n        learning_rate=5e-5,\n        min_dim_size_to_factor: int = 128,\n        decay_rate: float = 0.8,\n        decay_offset: int = 0,\n        multiply_by_parameter_scale: float = True,\n        clipping_threshold: Optional[float] = 1.0,\n        momentum: Optional[float] = None,\n        dtype_momentum: chex.ArrayDType = jnp.float32,\n        weight_decay_rate: Optional[float] = None,\n        eps: float = 1e-30,\n        factored: bool = True,\n        gradient_accumulation_steps: int = 1\n):\n    \"\"\"\n\n    :param gradient_accumulation_steps:\n    :param steps:\n    :param learning_rate:\n    :param weight_decay:\n    :param min_dim_size_to_factor:\n    :param decay_rate:\n    :param decay_offset:\n    :param multiply_by_parameter_scale:\n    :param clipping_threshold:\n    :param momentum:\n    :param dtype_momentum:\n    :param weight_decay_rate:\n    :param eps:\n    :param factored:\n    :param weight_decay_mask:\n    :param gradient_accumulation_steps\n    :return: Optimizer and Scheduler\n    \"\"\"\n    scheduler = optax.cosine_decay_schedule(\n        init_value=learning_rate,\n        decay_steps=steps\n    )\n    tx = optax.chain(\n        optax.adafactor(\n            learning_rate=scheduler,\n            min_dim_size_to_factor=min_dim_size_to_factor,\n            decay_rate=decay_rate,\n            decay_offset=decay_offset,\n            multiply_by_parameter_scale=multiply_by_parameter_scale,\n            clipping_threshold=clipping_threshold,\n            eps=eps,\n            momentum=momentum,\n            weight_decay_rate=weight_decay_rate,\n            dtype_momentum=dtype_momentum,\n            factored=factored\n        )\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-adafactor/#src.fjformer.optimizers.adafactor.get_adafactor_with_linear_scheduler","title":"<code>get_adafactor_with_linear_scheduler(steps, learning_rate_start=5e-05, learning_rate_end=1e-05, weight_decay=0.1, min_dim_size_to_factor=128, decay_rate=0.8, decay_offset=0, multiply_by_parameter_scale=True, clipping_threshold=1.0, momentum=None, dtype_momentum=jnp.float32, weight_decay_rate=None, eps=1e-30, factored=True, gradient_accumulation_steps=1, weight_decay_mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>gradient_accumulation_steps</code> <code>int</code> <code>1</code> <code>steps</code> <code>int</code> required <code>learning_rate_start</code> <code>float</code> <code>5e-05</code> <code>learning_rate_end</code> <code>float</code> <code>1e-05</code> <code>weight_decay</code> <code>0.1</code> <code>min_dim_size_to_factor</code> <code>int</code> <code>128</code> <code>decay_rate</code> <code>float</code> <code>0.8</code> <code>decay_offset</code> <code>int</code> <code>0</code> <code>multiply_by_parameter_scale</code> <code>float</code> <code>True</code> <code>clipping_threshold</code> <code>Optional[float]</code> <code>1.0</code> <code>momentum</code> <code>Optional[float]</code> <code>None</code> <code>dtype_momentum</code> <code>ArrayDType</code> <code>float32</code> <code>weight_decay_rate</code> <code>Optional[float]</code> <code>None</code> <code>eps</code> <code>float</code> <code>1e-30</code> <code>factored</code> <code>bool</code> <code>True</code> <code>weight_decay_mask</code> <code>None</code> <p>Returns:</p> Type Description <p>Optimizer and Scheduler</p> Source code in <code>src/fjformer/optimizers/adafactor.py</code> <pre><code>def get_adafactor_with_linear_scheduler(\n        steps: int,\n        learning_rate_start: float = 5e-5,\n        learning_rate_end: float = 1e-5,\n        weight_decay=1e-1,\n        min_dim_size_to_factor: int = 128,\n        decay_rate: float = 0.8,\n        decay_offset: int = 0,\n        multiply_by_parameter_scale: float = True,\n        clipping_threshold: Optional[float] = 1.0,\n        momentum: Optional[float] = None,\n        dtype_momentum: chex.ArrayDType = jnp.float32,\n        weight_decay_rate: Optional[float] = None,\n        eps: float = 1e-30,\n        factored: bool = True,\n        gradient_accumulation_steps: int = 1,\n        weight_decay_mask=None,\n\n):\n    \"\"\"\n\n    :param gradient_accumulation_steps:\n    :param steps:\n    :param learning_rate_start:\n    :param learning_rate_end:\n    :param weight_decay:\n    :param min_dim_size_to_factor:\n    :param decay_rate:\n    :param decay_offset:\n    :param multiply_by_parameter_scale:\n    :param clipping_threshold:\n    :param momentum:\n    :param dtype_momentum:\n    :param weight_decay_rate:\n    :param eps:\n    :param factored:\n    :param weight_decay_mask:\n    :return: Optimizer and Scheduler\n    \"\"\"\n    scheduler = optax.linear_schedule(\n        init_value=learning_rate_start,\n        end_value=learning_rate_end,\n        transition_steps=steps\n    )\n\n    tx = optax.chain(\n        optax.adafactor(\n            learning_rate=scheduler,\n            min_dim_size_to_factor=min_dim_size_to_factor,\n            decay_rate=decay_rate,\n            decay_offset=decay_offset,\n            multiply_by_parameter_scale=multiply_by_parameter_scale,\n            clipping_threshold=clipping_threshold,\n            eps=eps,\n            momentum=momentum,\n            weight_decay_rate=weight_decay_rate,\n            dtype_momentum=dtype_momentum,\n            factored=factored\n        ),\n        optax_add_scheduled_weight_decay(\n            lambda step: -scheduler(step) * weight_decay,\n            weight_decay_mask\n        )\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-adafactor/#src.fjformer.optimizers.adafactor.get_adafactor_with_warm_up_cosine_scheduler","title":"<code>get_adafactor_with_warm_up_cosine_scheduler(steps, learning_rate=5e-05, learning_rate_end=1e-05, weight_decay=0.1, min_dim_size_to_factor=128, decay_rate=0.8, decay_offset=0, multiply_by_parameter_scale=True, clipping_threshold=1.0, momentum=None, dtype_momentum=jnp.float32, weight_decay_rate=None, eps=1e-30, factored=True, exponent=1.0, weight_decay_mask=None, gradient_accumulation_steps=1, warmup_steps=500)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> required <code>learning_rate</code> <code>5e-05</code> <code>learning_rate_end</code> <code>1e-05</code> <code>weight_decay</code> <code>0.1</code> <code>min_dim_size_to_factor</code> <code>int</code> <code>128</code> <code>decay_rate</code> <code>float</code> <code>0.8</code> <code>decay_offset</code> <code>int</code> <code>0</code> <code>multiply_by_parameter_scale</code> <code>float</code> <code>True</code> <code>clipping_threshold</code> <code>Optional[float]</code> <code>1.0</code> <code>momentum</code> <code>Optional[float]</code> <code>None</code> <code>dtype_momentum</code> <code>ArrayDType</code> <code>float32</code> <code>weight_decay_rate</code> <code>Optional[float]</code> <code>None</code> <code>eps</code> <code>float</code> <code>1e-30</code> <code>factored</code> <code>bool</code> <code>True</code> <code>exponent</code> <code>float</code> <code>1.0</code> <code>weight_decay_mask</code> <code>None</code> <code>gradient_accumulation_steps</code> <code>int</code> <code>1</code> <code>warmup_steps</code> <code>int</code> <code>500</code> <p>Returns:</p> Type Description Source code in <code>src/fjformer/optimizers/adafactor.py</code> <pre><code>def get_adafactor_with_warm_up_cosine_scheduler(\n        steps: int,\n        learning_rate=5e-5,\n        learning_rate_end=1e-5,\n        weight_decay=1e-1,\n        min_dim_size_to_factor: int = 128,\n        decay_rate: float = 0.8,\n        decay_offset: int = 0,\n        multiply_by_parameter_scale: float = True,\n        clipping_threshold: Optional[float] = 1.0,\n        momentum: Optional[float] = None,\n        dtype_momentum: chex.ArrayDType = jnp.float32,\n        weight_decay_rate: Optional[float] = None,\n        eps: float = 1e-30,\n        factored: bool = True,\n        exponent: float = 1.0,\n        weight_decay_mask=None,\n        gradient_accumulation_steps: int = 1,\n        warmup_steps: int = 500,\n):\n    \"\"\"\n\n    :param steps:\n    :param learning_rate:\n    :param learning_rate_end:\n    :param weight_decay:\n    :param min_dim_size_to_factor:\n    :param decay_rate:\n    :param decay_offset:\n    :param multiply_by_parameter_scale:\n    :param clipping_threshold:\n    :param momentum:\n    :param dtype_momentum:\n    :param weight_decay_rate:\n    :param eps:\n    :param factored:\n    :param exponent:\n    :param weight_decay_mask:\n    :param gradient_accumulation_steps:\n    :param warmup_steps:\n    :return:\n    \"\"\"\n    scheduler = optax.warmup_cosine_decay_schedule(\n        init_value=0.5e-7,\n        peak_value=learning_rate,\n        warmup_steps=warmup_steps,\n        decay_steps=steps,\n        end_value=learning_rate_end,\n        exponent=exponent\n    )\n    tx = optax.chain(\n        optax.adafactor(\n            learning_rate=scheduler,\n            min_dim_size_to_factor=min_dim_size_to_factor,\n            decay_rate=decay_rate,\n            decay_offset=decay_offset,\n            multiply_by_parameter_scale=multiply_by_parameter_scale,\n            clipping_threshold=clipping_threshold,\n            eps=eps,\n            momentum=momentum,\n            weight_decay_rate=weight_decay_rate,\n            dtype_momentum=dtype_momentum,\n            factored=factored\n        ),\n        optax_add_scheduled_weight_decay(\n            lambda step: -scheduler(step) * weight_decay,\n            weight_decay_mask\n        )\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-adafactor/#src.fjformer.optimizers.adafactor.get_adafactor_with_warmup_linear_scheduler","title":"<code>get_adafactor_with_warmup_linear_scheduler(steps, min_dim_size_to_factor=128, decay_rate=0.8, decay_offset=0, multiply_by_parameter_scale=True, clipping_threshold=1.0, momentum=None, dtype_momentum=jnp.float32, weight_decay_rate=None, eps=1e-30, factored=True, gradient_accumulation_steps=1, learning_rate_start=5e-05, learning_rate_end=1e-05, warmup_steps=500)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>min_dim_size_to_factor</code> <code>int</code> <code>128</code> <code>decay_rate</code> <code>float</code> <code>0.8</code> <code>decay_offset</code> <code>int</code> <code>0</code> <code>multiply_by_parameter_scale</code> <code>float</code> <code>True</code> <code>clipping_threshold</code> <code>Optional[float]</code> <code>1.0</code> <code>momentum</code> <code>Optional[float]</code> <code>None</code> <code>dtype_momentum</code> <code>ArrayDType</code> <code>float32</code> <code>weight_decay_rate</code> <code>Optional[float]</code> <code>None</code> <code>factored</code> <code>bool</code> <code>True</code> <code>warmup_steps</code> <code>int</code> <code>500</code> <code>gradient_accumulation_steps</code> <code>int</code> <code>1</code> <code>steps</code> <code>int</code> required <code>learning_rate_start</code> <code>float</code> <code>5e-05</code> <code>learning_rate_end</code> <code>float</code> <code>1e-05</code> <code>eps</code> <code>float</code> <code>1e-30</code> <code>weight_decay</code> required Source code in <code>src/fjformer/optimizers/adafactor.py</code> <pre><code>def get_adafactor_with_warmup_linear_scheduler(\n        steps: int,\n        min_dim_size_to_factor: int = 128,\n        decay_rate: float = 0.8,\n        decay_offset: int = 0,\n        multiply_by_parameter_scale: float = True,\n        clipping_threshold: Optional[float] = 1.0,\n        momentum: Optional[float] = None,\n        dtype_momentum: chex.ArrayDType = jnp.float32,\n        weight_decay_rate: Optional[float] = None,\n        eps: float = 1e-30,\n        factored: bool = True,\n        gradient_accumulation_steps: int = 1,\n        learning_rate_start: float = 5e-5,\n        learning_rate_end: float = 1e-5,\n        warmup_steps: int = 500\n):\n    \"\"\"\n    :param min_dim_size_to_factor:\n    :param decay_rate:\n    :param decay_offset:\n    :param multiply_by_parameter_scale:\n    :param clipping_threshold:\n    :param momentum:\n    :param dtype_momentum:\n    :param weight_decay_rate:\n    :param factored:\n    :param warmup_steps:\n    :param gradient_accumulation_steps:\n    :param steps:\n    :param learning_rate_start:\n    :param learning_rate_end:\n    :param eps:\n    :param weight_decay:\n\n     # New parameter for warmup\n     @warmup_steps (int): Number of steps for the warmup phase\n\n     # return Optimizer and Scheduler with WarmUp feature\n   \"\"\"\n\n    scheduler_warmup = optax.linear_schedule(init_value=5e-8, end_value=learning_rate_start,\n                                             transition_steps=warmup_steps)\n    scheduler_decay = optax.linear_schedule(init_value=learning_rate_start, end_value=learning_rate_end,\n                                            transition_steps=steps - warmup_steps)\n\n    scheduler_combined = optax.join_schedules(schedules=[scheduler_warmup, scheduler_decay], boundaries=[warmup_steps])\n\n    tx = optax.chain(\n        optax.adafactor(\n            learning_rate=scheduler_combined,\n            min_dim_size_to_factor=min_dim_size_to_factor,\n            decay_rate=decay_rate,\n            decay_offset=decay_offset,\n            multiply_by_parameter_scale=multiply_by_parameter_scale,\n            clipping_threshold=clipping_threshold,\n            eps=eps,\n            momentum=momentum,\n            weight_decay_rate=weight_decay_rate,\n            dtype_momentum=dtype_momentum,\n            factored=factored\n        )\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler_combined\n</code></pre>"},{"location":"generated-optimizers-adafactor/#src.fjformer.optimizers.adafactor.get_adafactor_with_warmup_linear_scheduler--new-parameter-for-warmup-warmup_steps-int-number-of-steps-for-the-warmup-phase-return-optimizer-and-scheduler-with-warmup-feature","title":"New parameter for warmup @warmup_steps (int): Number of steps for the warmup phase  # return Optimizer and Scheduler with WarmUp feature","text":""},{"location":"generated-optimizers-adamw/","title":"optimizers.adamw","text":""},{"location":"generated-optimizers-adamw/#src.fjformer.optimizers.adamw.get_adamw_with_cosine_scheduler","title":"<code>get_adamw_with_cosine_scheduler(steps, learning_rate=5e-05, b1=0.9, b2=0.999, eps=1e-08, eps_root=0.0, weight_decay=0.1, gradient_accumulation_steps=1, mu_dtype=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>gradient_accumulation_steps</code> <code>int</code> <code>1</code> <code>steps</code> <code>int</code> required <code>learning_rate</code> <code>float</code> <code>5e-05</code> <code>b1</code> <code>float</code> <code>0.9</code> <code>b2</code> <code>float</code> <code>0.999</code> <code>eps</code> <code>float</code> <code>1e-08</code> <code>eps_root</code> <code>float</code> <code>0.0</code> <code>weight_decay</code> <code>float</code> <code>0.1</code> <code>mu_dtype</code> <code>Optional[ArrayDType]</code> <code>None</code> <p>Returns:</p> Type Description <p>Optimizer and Scheduler</p> Source code in <code>src/fjformer/optimizers/adamw.py</code> <pre><code>def get_adamw_with_cosine_scheduler(\n        steps: int,\n        learning_rate: float = 5e-5,\n        b1: float = 0.9,\n        b2: float = 0.999,\n        eps: float = 1e-8,\n        eps_root: float = 0.0,\n        weight_decay: float = 1e-1,\n        gradient_accumulation_steps: int = 1,\n        mu_dtype: Optional[chex.ArrayDType] = None,\n\n):\n    \"\"\"\n\n    :param gradient_accumulation_steps:\n    :param steps:\n    :param learning_rate:\n    :param b1:\n    :param b2:\n    :param eps:\n    :param eps_root:\n    :param weight_decay:\n    :param mu_dtype:\n    :return: Optimizer and Scheduler\n    \"\"\"\n    scheduler = optax.cosine_decay_schedule(\n        init_value=learning_rate,\n        decay_steps=steps\n    )\n    tx = optax.chain(\n        optax.scale_by_adam(\n            b1=b1,\n            b2=b2,\n            eps=eps,\n            eps_root=eps_root,\n            mu_dtype=mu_dtype\n        ),\n        optax.add_decayed_weights(\n            weight_decay=weight_decay\n        ),\n        optax.scale_by_schedule(scheduler),\n        optax.scale(-1)\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-adamw/#src.fjformer.optimizers.adamw.get_adamw_with_linear_scheduler","title":"<code>get_adamw_with_linear_scheduler(steps, learning_rate_start=5e-05, learning_rate_end=1e-05, b1=0.9, b2=0.999, eps=1e-08, eps_root=0.0, weight_decay=0.1, gradient_accumulation_steps=1, mu_dtype=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>gradient_accumulation_steps</code> <code>int</code> <code>1</code> <code>steps</code> <code>int</code> required <code>learning_rate_start</code> <code>float</code> <code>5e-05</code> <code>learning_rate_end</code> <code>float</code> <code>1e-05</code> <code>b1</code> <code>float</code> <code>0.9</code> <code>b2</code> <code>float</code> <code>0.999</code> <code>eps</code> <code>float</code> <code>1e-08</code> <code>eps_root</code> <code>float</code> <code>0.0</code> <code>weight_decay</code> <code>float</code> <code>0.1</code> <code>mu_dtype</code> <code>Optional[ArrayDType]</code> <code>None</code> <p>Returns:</p> Type Description <p>Optimizer and Scheduler</p> Source code in <code>src/fjformer/optimizers/adamw.py</code> <pre><code>def get_adamw_with_linear_scheduler(\n        steps: int,\n        learning_rate_start: float = 5e-5,\n        learning_rate_end: float = 1e-5,\n        b1: float = 0.9,\n        b2: float = 0.999,\n        eps: float = 1e-8,\n        eps_root: float = 0.0,\n        weight_decay: float = 1e-1,\n        gradient_accumulation_steps: int = 1,\n        mu_dtype: Optional[chex.ArrayDType] = None,\n\n):\n    \"\"\"\n\n    :param gradient_accumulation_steps:\n    :param steps:\n    :param learning_rate_start:\n    :param learning_rate_end:\n    :param b1:\n    :param b2:\n    :param eps:\n    :param eps_root:\n    :param weight_decay:\n    :param mu_dtype:\n    :return: Optimizer and Scheduler\n    \"\"\"\n    scheduler = optax.linear_schedule(\n        init_value=learning_rate_start,\n        end_value=learning_rate_end,\n        transition_steps=steps\n    )\n    tx = optax.chain(\n        optax.scale_by_adam(\n            b1=b1,\n            b2=b2,\n            eps=eps,\n            eps_root=eps_root,\n            mu_dtype=mu_dtype\n        ),\n        optax.add_decayed_weights(\n            weight_decay=weight_decay\n        ),\n        optax.scale_by_schedule(scheduler),\n        optax.scale(-1)\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-adamw/#src.fjformer.optimizers.adamw.get_adamw_with_warm_up_cosine_scheduler","title":"<code>get_adamw_with_warm_up_cosine_scheduler(steps, learning_rate=5e-05, learning_rate_end=1e-05, b1=0.9, b2=0.999, eps=1e-08, eps_root=0.0, weight_decay=0.1, exponent=1.0, gradient_accumulation_steps=1, warmup_steps=500, mu_dtype=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> required <code>learning_rate</code> <code>float</code> <code>5e-05</code> <code>learning_rate_end</code> <code>float</code> <code>1e-05</code> <code>b1</code> <code>float</code> <code>0.9</code> <code>b2</code> <code>float</code> <code>0.999</code> <code>eps</code> <code>float</code> <code>1e-08</code> <code>eps_root</code> <code>float</code> <code>0.0</code> <code>weight_decay</code> <code>float</code> <code>0.1</code> <code>exponent</code> <code>float</code> <code>1.0</code> <code>gradient_accumulation_steps</code> <code>int</code> <code>1</code> <code>warmup_steps</code> <code>int</code> <code>500</code> <code>mu_dtype</code> <code>Optional[ArrayDType]</code> <code>None</code> <p>Returns:</p> Type Description Source code in <code>src/fjformer/optimizers/adamw.py</code> <pre><code>def get_adamw_with_warm_up_cosine_scheduler(\n        steps: int,\n        learning_rate: float = 5e-5,\n        learning_rate_end: float = 1e-5,\n        b1: float = 0.9,\n        b2: float = 0.999,\n        eps: float = 1e-8,\n        eps_root: float = 0.0,\n        weight_decay: float = 1e-1,\n        exponent: float = 1.0,\n        gradient_accumulation_steps: int = 1,\n        warmup_steps: int = 500,\n        mu_dtype: Optional[chex.ArrayDType] = None\n):\n    \"\"\"\n\n    :param steps:\n    :param learning_rate:\n    :param learning_rate_end:\n    :param b1:\n    :param b2:\n    :param eps:\n    :param eps_root:\n    :param weight_decay:\n    :param exponent:\n    :param gradient_accumulation_steps:\n    :param warmup_steps:\n    :param mu_dtype:\n    :return:\n    \"\"\"\n    scheduler = optax.warmup_cosine_decay_schedule(\n        init_value=0.5e-7,\n        peak_value=learning_rate,\n        warmup_steps=warmup_steps,\n        decay_steps=steps,\n        end_value=learning_rate_end,\n        exponent=exponent\n    )\n    tx = optax.chain(\n        optax.scale_by_adam(\n            b1=b1,\n            b2=b2,\n            eps=eps,\n            eps_root=eps_root,\n            mu_dtype=mu_dtype\n        ),\n        optax.add_decayed_weights(\n            weight_decay=weight_decay\n        ),\n        optax.scale_by_schedule(scheduler),\n        optax.scale(-1)\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-adamw/#src.fjformer.optimizers.adamw.get_adamw_with_warmup_linear_scheduler","title":"<code>get_adamw_with_warmup_linear_scheduler(steps, learning_rate_start=5e-05, learning_rate_end=1e-05, b1=0.9, b2=0.999, eps=1e-08, eps_root=0.0, weight_decay=0.1, gradient_accumulation_steps=1, mu_dtype=None, warmup_steps=500)</code>","text":"<p>Thanks TO JinSeoungwoo</p> <p>Parameters:</p> Name Type Description Default <code>warmup_steps</code> <code>int</code> <code>500</code> <code>gradient_accumulation_steps</code> <code>int</code> <code>1</code> <code>steps</code> <code>int</code> required <code>learning_rate_start</code> <code>float</code> <code>5e-05</code> <code>learning_rate_end</code> <code>float</code> <code>1e-05</code> <code>b1</code> <code>float</code> <code>0.9</code> <code>b2</code> <code>float</code> <code>0.999</code> <code>eps</code> <code>float</code> <code>1e-08</code> <code>eps_root</code> <code>float</code> <code>0.0</code> <code>weight_decay</code> <code>float</code> <code>0.1</code> <code>mu_dtype</code> <code>Optional[ArrayDType]</code> <code>None</code> Source code in <code>src/fjformer/optimizers/adamw.py</code> <pre><code>def get_adamw_with_warmup_linear_scheduler(\n        steps: int,\n        learning_rate_start: float = 5e-5,\n        learning_rate_end: float = 1e-5,\n        b1: float = 0.9,\n        b2: float = 0.999,\n        eps: float = 1e-8,\n        eps_root: float = 0.0,\n        weight_decay: float = 1e-1,\n        gradient_accumulation_steps: int = 1,\n        mu_dtype: Optional[chex.ArrayDType] = None,\n        warmup_steps: int = 500\n):\n    \"\"\"\n    Thanks TO [JinSeoungwoo](https://github.com/erfanzar/EasyDeL/issues/32)\n    :param warmup_steps:\n    :param gradient_accumulation_steps:\n    :param steps:\n    :param learning_rate_start:\n    :param learning_rate_end:\n    :param b1:\n    :param b2:\n    :param eps:\n    :param eps_root:\n    :param weight_decay:\n    :param mu_dtype:\n\n     # New parameter for warmup\n     @warmup_steps (int): Number of steps for the warmup phase\n\n     # return Optimizer and Scheduler with WarmUp feature\n   \"\"\"\n\n    scheduler_warmup = optax.linear_schedule(init_value=5e-8, end_value=learning_rate_start,\n                                             transition_steps=warmup_steps)\n    scheduler_decay = optax.linear_schedule(init_value=learning_rate_start, end_value=learning_rate_end,\n                                            transition_steps=steps - warmup_steps)\n\n    scheduler_combined = optax.join_schedules(schedules=[scheduler_warmup, scheduler_decay], boundaries=[warmup_steps])\n\n    tx = optax.chain(\n        optax.scale_by_adam(\n            b1=b1,\n            b2=b2,\n            eps=eps,\n            eps_root=eps_root,\n            mu_dtype=mu_dtype\n        ),\n        optax.add_decayed_weights(\n            weight_decay=weight_decay\n        ),\n        optax.scale_by_schedule(scheduler_combined),\n        optax.scale(-1)\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler_combined\n</code></pre>"},{"location":"generated-optimizers-adamw/#src.fjformer.optimizers.adamw.get_adamw_with_warmup_linear_scheduler--new-parameter-for-warmup-warmup_steps-int-number-of-steps-for-the-warmup-phase-return-optimizer-and-scheduler-with-warmup-feature","title":"New parameter for warmup @warmup_steps (int): Number of steps for the warmup phase  # return Optimizer and Scheduler with WarmUp feature","text":""},{"location":"generated-optimizers-lion/","title":"optimizers.lion","text":""},{"location":"generated-optimizers-lion/#src.fjformer.optimizers.lion.get_lion_with_cosine_scheduler","title":"<code>get_lion_with_cosine_scheduler(steps, learning_rate=5e-05, alpha=0.0, exponent=1.0, b1=0.9, b2=0.99, gradient_accumulation_steps=1, mu_dtype=None)</code>","text":"<p>Args:     learning_rate: An initial value <code>init_v</code>.     steps: Positive integer - the number of steps for which to apply       the decay for.     alpha: Float. The minimum value of the multiplier used to adjust the       learning rate.     exponent: Float. The default decay is 0.5 * (1 + cos(pi * t/T)), where t is       the current timestep and T is the <code>decay_steps</code>. The exponent modifies       this to be (0.5 * (1 + cos(pi * t/T))) ** exponent. Defaults to 1.0.     b1: Rate for combining the momentum and the current grad.     b2: Decay rate for the exponentially weighted average of grads.     mu_dtype: Optional <code>dtype</code> to be used for the momentum; if       <code>None</code> then the <code>dtype is inferred from</code>params<code>and</code>updates`.     gradient_accumulation_steps:gradient_accumulation_steps Return:     Optimizer , Scheduler</p> Source code in <code>src/fjformer/optimizers/lion.py</code> <pre><code>def get_lion_with_cosine_scheduler(\n        steps: int,\n        learning_rate=5e-5,\n        alpha: float = 0.0,\n        exponent: float = 1.0,\n        b1: float = 0.9,\n        b2: float = 0.99,\n        gradient_accumulation_steps: int = 1,\n        mu_dtype: Optional[chex.ArrayDType] = None,\n):\n    \"\"\"\n\n    Args:\n        learning_rate: An initial value `init_v`.\n        steps: Positive integer - the number of steps for which to apply\n          the decay for.\n        alpha: Float. The minimum value of the multiplier used to adjust the\n          learning rate.\n        exponent: Float. The default decay is 0.5 * (1 + cos(pi * t/T)), where t is\n          the current timestep and T is the `decay_steps`. The exponent modifies\n          this to be (0.5 * (1 + cos(pi * t/T))) ** exponent. Defaults to 1.0.\n        b1: Rate for combining the momentum and the current grad.\n        b2: Decay rate for the exponentially weighted average of grads.\n        mu_dtype: Optional `dtype` to be used for the momentum; if\n          `None` then the `dtype is inferred from `params` and `updates`.\n        gradient_accumulation_steps:gradient_accumulation_steps\n    Return:\n        Optimizer , Scheduler\n\n    \"\"\"\n    try:\n        scheduler = optax.cosine_decay_schedule(\n            init_value=learning_rate,\n            decay_steps=steps,\n            alpha=alpha,\n            exponent=exponent\n        )\n    except:\n        scheduler = optax.cosine_decay_schedule(\n            init_value=learning_rate,\n            decay_steps=steps,\n            alpha=alpha,\n            # exponent=exponent\n        )\n    tx = optax.chain(\n        optax.scale_by_lion(\n            b1=b1,\n            b2=b2,\n            mu_dtype=mu_dtype\n        ),\n        optax.scale_by_schedule(scheduler),\n        optax.scale(-1)\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-lion/#src.fjformer.optimizers.lion.get_lion_with_linear_scheduler","title":"<code>get_lion_with_linear_scheduler(steps, learning_rate_start=5e-05, learning_rate_end=1e-05, b1=0.9, b2=0.99, gradient_accumulation_steps=1, mu_dtype=None)</code>","text":"<p>Args:     steps: total train steps (max_steps)     learning_rate_start: start learning rate for sure     learning_rate_end: end learning rate for sure :        b1: Rate for combining the momentum and the current grad.     b2: Decay rate for the exponentially weighted average of grads.     mu_dtype: Optional <code>dtype</code> to be used for the momentum; if       <code>None</code> then the <code>dtype is inferred from</code>params<code>and</code>updates`.     gradient_accumulation_steps:gradient_accumulation_steps Return:     Optimizer , Scheduler</p> Source code in <code>src/fjformer/optimizers/lion.py</code> <pre><code>def get_lion_with_linear_scheduler(\n        steps: int,\n        learning_rate_start: float = 5e-5,\n        learning_rate_end: float = 1e-5,\n        b1: float = 0.9,\n        b2: float = 0.99,\n        gradient_accumulation_steps: int = 1,\n        mu_dtype: Optional[chex.ArrayDType] = None,\n):\n    \"\"\"\n    Args:\n        steps: total train steps (max_steps)\n        learning_rate_start: start learning rate for sure\n        learning_rate_end: end learning rate for sure :\\\n        b1: Rate for combining the momentum and the current grad.\n        b2: Decay rate for the exponentially weighted average of grads.\n        mu_dtype: Optional `dtype` to be used for the momentum; if\n          `None` then the `dtype is inferred from `params` and `updates`.\n        gradient_accumulation_steps:gradient_accumulation_steps\n    Return:\n        Optimizer , Scheduler\n         \"\"\"\n    scheduler = optax.linear_schedule(\n        init_value=learning_rate_start,\n        end_value=learning_rate_end,\n        transition_steps=steps\n    )\n    tx = optax.chain(\n        optax.scale_by_lion(\n            b1=b1,\n            b2=b2,\n            mu_dtype=mu_dtype\n        ),\n        optax.scale_by_schedule(scheduler),\n        optax.scale(-1)\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-lion/#src.fjformer.optimizers.lion.get_lion_with_warm_up_cosine_scheduler","title":"<code>get_lion_with_warm_up_cosine_scheduler(steps, learning_rate=5e-05, learning_rate_end=1e-05, exponent=1.0, b1=0.9, b2=0.99, gradient_accumulation_steps=1, warmup_steps=500, mu_dtype=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> required <code>learning_rate</code> <code>5e-05</code> <code>learning_rate_end</code> <code>1e-05</code> <code>exponent</code> <code>float</code> <code>1.0</code> <code>b1</code> <code>float</code> <code>0.9</code> <code>b2</code> <code>float</code> <code>0.99</code> <code>gradient_accumulation_steps</code> <code>int</code> <code>1</code> <code>warmup_steps</code> <code>int</code> <code>500</code> <code>mu_dtype</code> <code>Optional[ArrayDType]</code> <code>None</code> <p>Returns:</p> Type Description Source code in <code>src/fjformer/optimizers/lion.py</code> <pre><code>def get_lion_with_warm_up_cosine_scheduler(\n        steps: int,\n        learning_rate=5e-5,\n        learning_rate_end=1e-5,\n        exponent: float = 1.0,\n        b1: float = 0.9,\n        b2: float = 0.99,\n        gradient_accumulation_steps: int = 1,\n        warmup_steps: int = 500,\n        mu_dtype: Optional[chex.ArrayDType] = None,\n):\n    \"\"\"\n\n    :param steps:\n    :param learning_rate:\n    :param learning_rate_end:\n    :param exponent:\n    :param b1:\n    :param b2:\n    :param gradient_accumulation_steps:\n    :param warmup_steps:\n    :param mu_dtype:\n    :return:\n    \"\"\"\n    scheduler = optax.warmup_cosine_decay_schedule(\n        init_value=0.5e-7,\n        peak_value=learning_rate,\n        warmup_steps=warmup_steps,\n        decay_steps=steps,\n        end_value=learning_rate_end,\n        exponent=exponent,\n    )\n    tx = optax.chain(\n        optax.scale_by_lion(\n            b1=b1,\n            b2=b2,\n            mu_dtype=mu_dtype\n        ),\n        optax.scale_by_schedule(scheduler),\n        optax.scale(-1)\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-lion/#src.fjformer.optimizers.lion.get_lion_with_with_warmup_linear_scheduler","title":"<code>get_lion_with_with_warmup_linear_scheduler(steps, b1=0.9, b2=0.99, gradient_accumulation_steps=1, mu_dtype=None, learning_rate_start=5e-05, learning_rate_end=1e-05, warmup_steps=500)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>b1</code> <code>float</code> <code>0.9</code> <code>b2</code> <code>float</code> <code>0.99</code> <code>mu_dtype</code> <code>Optional[ArrayDType]</code> <code>None</code> <code>learning_rate_start</code> <code>float</code> <code>5e-05</code> <code>learning_rate_end</code> <code>float</code> <code>1e-05</code> <code>warmup_steps</code> <code>int</code> <code>500</code> <code>gradient_accumulation_steps</code> <code>int</code> <code>1</code> <code>steps</code> <code>int</code> required <p>Returns:</p> Type Description <p>Optimizer and Scheduler</p> Source code in <code>src/fjformer/optimizers/lion.py</code> <pre><code>def get_lion_with_with_warmup_linear_scheduler(\n        steps: int,\n        b1: float = 0.9,\n        b2: float = 0.99,\n        gradient_accumulation_steps: int = 1,\n        mu_dtype: Optional[chex.ArrayDType] = None,\n        learning_rate_start: float = 5e-5,\n        learning_rate_end: float = 1e-5,\n        warmup_steps: int = 500\n):\n    \"\"\"\n\n    :param b1:\n    :param b2:\n    :param mu_dtype:\n    :param learning_rate_start:\n    :param learning_rate_end:\n    :param warmup_steps:\n    :param gradient_accumulation_steps:\n    :param steps:\n    :param gradient_accumulation_steps\n    :return: Optimizer and Scheduler\n    \"\"\"\n    scheduler_warmup = optax.linear_schedule(init_value=5e-8, end_value=learning_rate_start,\n                                             transition_steps=warmup_steps)\n    scheduler_decay = optax.linear_schedule(init_value=learning_rate_start, end_value=learning_rate_end,\n                                            transition_steps=steps - warmup_steps)\n\n    scheduler_combined = optax.join_schedules(schedules=[scheduler_warmup, scheduler_decay], boundaries=[warmup_steps])\n    tx = optax.chain(\n        optax.scale_by_lion(\n            b1=b1,\n            b2=b2,\n            mu_dtype=mu_dtype\n        ),\n        optax.scale_by_schedule(scheduler_combined),\n        optax.scale(-1)\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler_combined\n</code></pre>"},{"location":"generated-optimizers-optimizer_utils/","title":"optimizers.optimizer_utils","text":""},{"location":"generated-optimizers-optimizer_utils/#src.fjformer.optimizers.optimizer_utils.optax_add_scheduled_weight_decay","title":"<code>optax_add_scheduled_weight_decay(schedule_fn, mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>schedule_fn</code> required <code>mask</code> <code>None</code> <p>Returns:</p> Type Description <p>Optax GradientTransformation inited</p> Source code in <code>src/fjformer/optimizers/optimizer_utils.py</code> <pre><code>def optax_add_scheduled_weight_decay(schedule_fn, mask=None):\n    \"\"\"\n\n    :param schedule_fn:\n    :param mask:\n    :return: Optax GradientTransformation inited\n    \"\"\"\n\n    def init_fn(params):\n        del params\n        return OptaxScheduledWeightDecayState(count=jnp.zeros([], jnp.int32))\n\n    def update_fn(updates, state, params):\n        if params is None:\n            raise ValueError('Params cannot be None for weight decay!')\n\n        weight_decay = schedule_fn(state.count)\n        updates = jax.tree_util.tree_map(\n            lambda g, p: g + weight_decay * p, updates, params\n        )\n        return updates, OptaxScheduledWeightDecayState(\n            count=optax.safe_int32_increment(state.count)\n        )\n\n    if mask is not None:\n        return optax.masked(optax.GradientTransformation(init_fn, update_fn), mask)\n    return optax.GradientTransformation(init_fn, update_fn)\n</code></pre>"},{"location":"generated-optimizers-rmsprop/","title":"optimizers.rmsprop","text":""},{"location":"generated-optimizers-rmsprop/#src.fjformer.optimizers.rmsprop.get_rmsprop_with_cosine_scheduler","title":"<code>get_rmsprop_with_cosine_scheduler(steps, learning_rate=5e-05, decay=0.9, initial_scale=0.0, momentum=None, nesterov=False, eps=1e-08, weight_decay=0.1, gradient_accumulation_steps=1)</code>","text":"<p>The get_rmsprop_with_cosine_scheduler function returns a tuple of the optimizer and scheduler. The optimizer is composed of several transformations:     1) scale_by_rms - scales the gradient by RMS (root-mean-square) values, which are calculated using an      exponential moving average with decay rate <code>decay</code> and initial value <code>initial_scale</code>. The epsilon      parameter prevents division by zero.     2) scale_by_schedule - scales the gradient by a schedule, in this case cosine decay with initial value     <code>learning rate</code> and number of steps to complete one cycle equal to total number of training steps.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>int: Set the number of steps in the cosine decay schedule</p> required <code>learning_rate</code> <code>float</code> <p>float: Set the initial learning rate</p> <code>5e-05</code> <code>decay</code> <code>float</code> <p>float: Control the decay rate of the running average</p> <code>0.9</code> <code>initial_scale</code> <code>float</code> <p>float: Set the initial scale of the rmsprop optimizer</p> <code>0.0</code> <code>momentum</code> <code>Optional[float]</code> <p>Optional[float]: Specify the momentum of the optimizer</p> <code>None</code> <code>nesterov</code> <code>bool</code> <p>bool: Determine whether to use the nesterov momentum algorithm</p> <code>False</code> <code>eps</code> <code>float</code> <p>float: Avoid division by zero</p> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <p>float: Add a weight decay to the loss function</p> <code>0.1</code> <code>gradient_accumulation_steps</code> <code>int</code> <p>int: Accumulate the gradients over multiple steps before updating the weights</p> <code>1</code> <code></code> <p>Define the number of steps to be taken before the learning rate is decayed</p> required <p>Returns:</p> Type Description <p>The optimizer and the scheduler</p> Source code in <code>src/fjformer/optimizers/rmsprop.py</code> <pre><code>def get_rmsprop_with_cosine_scheduler(\n        steps: int,\n        learning_rate: float = 5e-5,\n        decay: float = 0.9,\n        initial_scale: float = 0.,\n        momentum: Optional[float] = None,\n        nesterov: bool = False,\n        eps: float = 1e-8,\n        weight_decay: float = 1e-1,\n        gradient_accumulation_steps: int = 1,\n\n):\n\n    \"\"\"\n    The get_rmsprop_with_cosine_scheduler function returns a tuple of the optimizer and scheduler.\n    The optimizer is composed of several transformations:\n        1) scale_by_rms - scales the gradient by RMS (root-mean-square) values, which are calculated using an\n         exponential moving average with decay rate `decay` and initial value `initial_scale`. The epsilon\n         parameter prevents division by zero.\n        2) scale_by_schedule - scales the gradient by a schedule, in this case cosine decay with initial value\n        `learning rate` and number of steps to complete one cycle equal to total number of training steps.\n\n\n    :param steps: int: Set the number of steps in the cosine decay schedule\n    :param learning_rate: float: Set the initial learning rate\n    :param decay: float: Control the decay rate of the running average\n    :param initial_scale: float: Set the initial scale of the rmsprop optimizer\n    :param momentum: Optional[float]: Specify the momentum of the optimizer\n    :param nesterov: bool: Determine whether to use the nesterov momentum algorithm\n    :param eps: float: Avoid division by zero\n    :param weight_decay: float: Add a weight decay to the loss function\n    :param gradient_accumulation_steps: int: Accumulate the gradients over multiple steps before updating the weights\n    :param : Define the number of steps to be taken before the learning rate is decayed\n    :return: The optimizer and the scheduler\n    \"\"\"\n    scheduler = optax.cosine_decay_schedule(\n        init_value=learning_rate,\n        decay_steps=steps\n    )\n\n    tx = optax.chain(\n        optax.scale_by_rms(\n            decay=decay,\n            eps=eps,\n            initial_scale=initial_scale\n        ),\n        optax.scale_by_schedule(scheduler),\n        optax.scale(-1),\n        (\n            optax.trace(\n                decay=momentum,\n                nesterov=nesterov\n            )\n            if momentum is not None else optax.identity()\n        ),\n        optax.add_decayed_weights(\n            weight_decay=weight_decay\n        ),\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-rmsprop/#src.fjformer.optimizers.rmsprop.get_rmsprop_with_linear_scheduler","title":"<code>get_rmsprop_with_linear_scheduler(steps, learning_rate_start=5e-05, learning_rate_end=1e-05, decay=0.9, initial_scale=0.0, momentum=None, nesterov=False, eps=1e-08, weight_decay=0.1, gradient_accumulation_steps=1)</code>","text":"<p>The get_rmsprop_with_linear_scheduler function returns a tuple of two objects:     1. A transformation (tx) that is applied to the gradients before they are used to update the model parameters.     2. A scheduler object that can be used to retrieve the current learning rate at any point during training.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>int: Define how many steps the learning rate will take to transition from learning_rate_start to learning_rate_end</p> required <code>learning_rate_start</code> <code>float</code> <p>float: Set the initial learning rate</p> <code>5e-05</code> <code>learning_rate_end</code> <code>float</code> <p>float: Specify the final learning rate</p> <code>1e-05</code> <code>decay</code> <code>float</code> <p>float: Control the decay rate of the rmsprop algorithm</p> <code>0.9</code> <code>initial_scale</code> <code>float</code> <p>float: Scale the initial gradient</p> <code>0.0</code> <code>momentum</code> <code>Optional[float]</code> <p>Optional[float]: Set the momentum of the optimizer</p> <code>None</code> <code>nesterov</code> <code>bool</code> <p>bool: Determine whether to use nesterov momentum or not</p> <code>False</code> <code>eps</code> <code>float</code> <p>float: Prevent division by zero</p> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <p>float: Apply weight decay to the model weights</p> <code>0.1</code> <code>gradient_accumulation_steps</code> <code>int</code> <p>int: Accumulate the gradients over multiple batches</p> <code>1</code> <code></code> <p>Control the learning rate decay</p> required <p>Returns:</p> Type Description <p>Optimizer,Scheduler</p> Source code in <code>src/fjformer/optimizers/rmsprop.py</code> <pre><code>def get_rmsprop_with_linear_scheduler(\n        steps: int,\n        learning_rate_start: float = 5e-5,\n        learning_rate_end: float = 1e-5,\n        decay: float = 0.9,\n        initial_scale: float = 0.,\n        momentum: Optional[float] = None,\n        nesterov: bool = False,\n        eps: float = 1e-8,\n        weight_decay: float = 1e-1,\n        gradient_accumulation_steps: int = 1,\n\n):\n\n    \"\"\"\n    The get_rmsprop_with_linear_scheduler function returns a tuple of two objects:\n        1. A transformation (tx) that is applied to the gradients before they are used to update the model parameters.\n        2. A scheduler object that can be used to retrieve the current learning rate at any point during training.\n\n    :param steps: int: Define how many steps the learning rate will take to transition from learning_rate_start to learning_rate_end\n    :param learning_rate_start: float: Set the initial learning rate\n    :param learning_rate_end: float: Specify the final learning rate\n    :param decay: float: Control the decay rate of the rmsprop algorithm\n    :param initial_scale: float: Scale the initial gradient\n    :param momentum: Optional[float]: Set the momentum of the optimizer\n    :param nesterov: bool: Determine whether to use nesterov momentum or not\n    :param eps: float: Prevent division by zero\n    :param weight_decay: float: Apply weight decay to the model weights\n    :param gradient_accumulation_steps: int: Accumulate the gradients over multiple batches\n    :param : Control the learning rate decay\n    :return: Optimizer,Scheduler\n    \"\"\"\n    scheduler = optax.linear_schedule(\n        init_value=learning_rate_start,\n        end_value=learning_rate_end,\n        transition_steps=steps\n    )\n\n    tx = optax.chain(\n        optax.scale_by_rms(\n            decay=decay,\n            eps=eps,\n            initial_scale=initial_scale\n        ),\n        optax.scale_by_schedule(scheduler),\n        optax.scale(-1),\n        (\n            optax.trace(\n                decay=momentum,\n                nesterov=nesterov\n            )\n            if momentum is not None else optax.identity()\n        ),\n        optax.add_decayed_weights(\n            weight_decay=weight_decay\n        ),\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-rmsprop/#src.fjformer.optimizers.rmsprop.get_rmsprop_with_warm_up_cosine_scheduler","title":"<code>get_rmsprop_with_warm_up_cosine_scheduler(steps, learning_rate=5e-05, learning_rate_end=1e-05, decay=0.9, initial_scale=0.0, momentum=None, nesterov=False, eps=1e-08, weight_decay=0.1, exponent=1.0, gradient_accumulation_steps=1, warmup_steps=500)</code>","text":"<p>The get_rmsprop_with_warm_up_cosine_scheduler function returns a tuple of two objects:     1. A transformation (tx) that is applied to the gradients before they are used to update the parameters.     2. A scheduler object that can be used to get the current learning rate at any given step in training.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>int: Define the number of steps in the warm up phase</p> required <code>learning_rate</code> <code>float</code> <p>float: Set the learning rate of the optimizer</p> <code>5e-05</code> <code>learning_rate_end</code> <code>float</code> <p>float: Set the final learning rate of the optimizer after decay</p> <code>1e-05</code> <code>decay</code> <code>float</code> <p>float: Control the decay rate of the rmsprop algorithm</p> <code>0.9</code> <code>initial_scale</code> <code>float</code> <p>float: Scale the initial gradient</p> <code>0.0</code> <code>momentum</code> <code>Optional[float]</code> <p>Optional[float]: Define the momentum of the optimizer</p> <code>None</code> <code>nesterov</code> <code>bool</code> <p>bool: Indicate whether to use nesterov momentum</p> <code>False</code> <code>eps</code> <code>float</code> <p>float: Avoid division by zero</p> <code>1e-08</code> <code>weight_decay</code> <code>float</code> <p>float: Add a weight decay to the loss function</p> <code>0.1</code> <code>exponent</code> <code>float</code> <p>float: Control the shape of the cosine curve</p> <code>1.0</code> <code>gradient_accumulation_steps</code> <code>int</code> <p>int: Accumulate gradients over multiple batches</p> <code>1</code> <code>warmup_steps</code> <code>int</code> <p>int: Number of steps of the linear warmup</p> <code>500</code> <code></code> <p>Control the learning rate</p> required <p>Returns:</p> Type Description <p>Optimizer,Scheduler</p> Source code in <code>src/fjformer/optimizers/rmsprop.py</code> <pre><code>def get_rmsprop_with_warm_up_cosine_scheduler(\n        steps: int,\n        learning_rate: float = 5e-5,\n        learning_rate_end: float = 1e-5,\n        decay: float = 0.9,\n        initial_scale: float = 0.,\n        momentum: Optional[float] = None,\n        nesterov: bool = False,\n        eps: float = 1e-8,\n        weight_decay: float = 1e-1,\n        exponent: float = 1.0,\n        gradient_accumulation_steps: int = 1,\n        warmup_steps: int = 500,\n):\n    \"\"\"\n    The get_rmsprop_with_warm_up_cosine_scheduler function returns a tuple of two objects:\n        1. A transformation (tx) that is applied to the gradients before they are used to update the parameters.\n        2. A scheduler object that can be used to get the current learning rate at any given step in training.\n\n    :param steps: int: Define the number of steps in the warm up phase\n    :param learning_rate: float: Set the learning rate of the optimizer\n    :param learning_rate_end: float: Set the final learning rate of the optimizer after decay\n    :param decay: float: Control the decay rate of the rmsprop algorithm\n    :param initial_scale: float: Scale the initial gradient\n    :param momentum: Optional[float]: Define the momentum of the optimizer\n    :param nesterov: bool: Indicate whether to use nesterov momentum\n    :param eps: float: Avoid division by zero\n    :param weight_decay: float: Add a weight decay to the loss function\n    :param exponent: float: Control the shape of the cosine curve\n    :param gradient_accumulation_steps: int: Accumulate gradients over multiple batches\n    :param warmup_steps: int: Number of steps of the linear warmup\n    :param : Control the learning rate\n    :return: Optimizer,Scheduler\n    \"\"\"\n    scheduler = optax.warmup_cosine_decay_schedule(\n        init_value=0.5e-7,\n        peak_value=learning_rate,\n        warmup_steps=warmup_steps,\n        decay_steps=steps,\n        end_value=learning_rate_end,\n        exponent=exponent\n    )\n\n    tx = optax.chain(\n        optax.scale_by_rms(\n            decay=decay,\n            eps=eps,\n            initial_scale=initial_scale\n        ),\n        optax.scale_by_schedule(scheduler),\n        optax.scale(-1),\n        (\n            optax.trace(\n                decay=momentum,\n                nesterov=nesterov\n            )\n            if momentum is not None else optax.identity()\n        ),\n        optax.add_decayed_weights(\n            weight_decay=weight_decay\n        ),\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler\n</code></pre>"},{"location":"generated-optimizers-rmsprop/#src.fjformer.optimizers.rmsprop.get_rmsprop_with_warmup_linear_scheduler","title":"<code>get_rmsprop_with_warmup_linear_scheduler(steps, learning_rate_start=5e-05, learning_rate_end=1e-05, decay=0.9, eps=1e-08, initial_scale=0.0, momentum=None, nesterov=False, weight_decay=0.1, gradient_accumulation_steps=1, warmup_steps=500)</code>","text":"<p>The get_rmsprop_with_warmup_linear_scheduler function returns a tuple of the following:     1. A JAX optimizer transformation (tx) that performs RMSprop with warmup and linear decay, as well as weight decay.     2. A JAX schedule object (scheduler_combined) that can be used to plot the learning rate over time.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>int: Define the number of steps in the training loop</p> required <code>learning_rate_start</code> <code>float</code> <p>float: Set the learning rate at the start of training</p> <code>5e-05</code> <code>learning_rate_end</code> <code>float</code> <p>float: Set the learning rate at the end of training</p> <code>1e-05</code> <code>decay</code> <code>float</code> <p>float: Control the decay rate of the moving average</p> <code>0.9</code> <code>eps</code> <code>float</code> <p>float: Avoid division by zero</p> <code>1e-08</code> <code>initial_scale</code> <code>float</code> <p>float: Set the initial scale of the rmsprop optimizer</p> <code>0.0</code> <code>momentum</code> <code>Optional[float]</code> <p>Optional[float]: Set the momentum of the optimizer</p> <code>None</code> <code>nesterov</code> <code>bool</code> <p>bool: Determine whether to use the nesterov momentum algorithm</p> <code>False</code> <code>weight_decay</code> <code>float</code> <p>float: Add a weight decay to the loss function</p> <code>0.1</code> <code>gradient_accumulation_steps</code> <code>int</code> <p>int: Accumulate the gradients over multiple batches</p> <code>1</code> <code>warmup_steps</code> <code>int</code> <p>int: Set the number of steps to warm up the learning rate</p> <code>500</code> <p>Returns:</p> Type Description <p>Optimizer,Scheduler</p> Source code in <code>src/fjformer/optimizers/rmsprop.py</code> <pre><code>def get_rmsprop_with_warmup_linear_scheduler(\n        steps: int,\n        learning_rate_start: float = 5e-5,\n        learning_rate_end: float = 1e-5,\n        decay: float = 0.9,\n        eps: float = 1e-8,\n        initial_scale: float = 0.,\n        momentum: Optional[float] = None,\n        nesterov: bool = False,\n        weight_decay: float = 1e-1,\n        gradient_accumulation_steps: int = 1,\n        warmup_steps: int = 500\n):\n\n    \"\"\"\n    The get_rmsprop_with_warmup_linear_scheduler function returns a tuple of the following:\n        1. A JAX optimizer transformation (tx) that performs RMSprop with warmup and linear decay, as well as weight decay.\n        2. A JAX schedule object (scheduler_combined) that can be used to plot the learning rate over time.\n\n    :param steps: int: Define the number of steps in the training loop\n    :param learning_rate_start: float: Set the learning rate at the start of training\n    :param learning_rate_end: float: Set the learning rate at the end of training\n    :param decay: float: Control the decay rate of the moving average\n    :param eps: float: Avoid division by zero\n    :param initial_scale: float: Set the initial scale of the rmsprop optimizer\n    :param momentum: Optional[float]: Set the momentum of the optimizer\n    :param nesterov: bool: Determine whether to use the nesterov momentum algorithm\n    :param weight_decay: float: Add a weight decay to the loss function\n    :param gradient_accumulation_steps: int: Accumulate the gradients over multiple batches\n    :param warmup_steps: int: Set the number of steps to warm up the learning rate\n    :return: Optimizer,Scheduler\n    \"\"\"\n    scheduler_warmup = optax.linear_schedule(\n        init_value=5e-8,\n        end_value=learning_rate_start,\n        transition_steps=warmup_steps\n    )\n    scheduler_decay = optax.linear_schedule(\n        init_value=learning_rate_start,\n        end_value=learning_rate_end,\n        transition_steps=steps - warmup_steps\n    )\n\n    scheduler_combined = optax.join_schedules(\n        schedules=[scheduler_warmup, scheduler_decay],\n        boundaries=[warmup_steps]\n    )\n\n    tx = optax.chain(\n        optax.scale_by_rms(\n            decay=decay,\n            eps=eps,\n            initial_scale=initial_scale\n        ),\n        optax.scale_by_schedule(scheduler_combined),\n        optax.scale(-1),\n        (\n            optax.trace(\n                decay=momentum,\n                nesterov=nesterov\n            )\n            if momentum is not None else optax.identity()\n        ),\n        optax.add_decayed_weights(\n            weight_decay=weight_decay\n        ),\n    )\n    if gradient_accumulation_steps &gt; 1:\n        tx = optax.MultiSteps(\n            tx, gradient_accumulation_steps\n        )\n    return tx, scheduler_combined\n</code></pre>"},{"location":"generated-pallas_operations-efficient_attention-efficient_attention/","title":"pallas_operations.efficient_attention.efficient_attention","text":""},{"location":"generated-pallas_operations-efficient_attention-efficient_attention/#src.fjformer.pallas_operations.efficient_attention.efficient_attention.efficient_attention","title":"<code>efficient_attention(query, key, value, bias=None, deterministic=True, dropout_rng=None, attention_drop_rate=0.0, causal=True, query_chunk_size=1024, key_chunk_size=1024, dtype=jnp.float32, policy=jax.checkpoint_policies.nothing_saveable(), precision=None, float32_logits=True, prevent_cse=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>query</code> <code>Array</code> <p>Array Shape [batch,Q Sequence length,num attention heads, head dims]</p> required <code>key</code> <code>Array</code> <p>Array Shape [batch,KV Sequence length,num KV attention heads, head dims]</p> required <code>value</code> <code>Array</code> <p>Array Shape [batch,KV Sequence length,num KV attention heads, head dims]</p> required <code>bias</code> <code>Array</code> <p>Bias To be added</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool (whenever use dropout or no)</p> <code>True</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>RNG Dropout</p> <code>None</code> <code>attention_drop_rate</code> <code>float</code> <code>0.0</code> <code>causal</code> <code>bool</code> <p>Is Decoder or Causal</p> <code>True</code> <code>query_chunk_size</code> <code>int</code> <p>Chunk size used for query</p> <code>1024</code> <code>key_chunk_size</code> <code>int</code> <p>Chunk size used for key</p> <code>1024</code> <code>dtype</code> <code>ArrayDType</code> <p>DataType</p> <code>float32</code> <code>policy</code> <p>Gradient Checkpoint Policy</p> <code>nothing_saveable()</code> <code>precision</code> <p>PrecisionLike</p> <code>None</code> <code>float32_logits</code> <code>bool</code> <code>True</code> <code>prevent_cse</code> <code>bool</code> <code>True</code> <p>Returns:</p> Type Description Source code in <code>src/fjformer/pallas_operations/efficient_attention/efficient_attention.py</code> <pre><code>def efficient_attention(\n        query: chex.Array,\n        key: chex.Array,\n        value: chex.Array,\n        bias: chex.Array = None,\n        deterministic: bool = True,\n        dropout_rng: chex.PRNGKey = None,\n        attention_drop_rate: float = 0.0,\n        causal: bool = True,\n        query_chunk_size: int = 1024,\n        key_chunk_size: int = 1024,\n        dtype: chex.ArrayDType = jnp.float32,\n        policy=jax.checkpoint_policies.nothing_saveable(),\n        precision=None,\n        float32_logits: bool = True,\n        prevent_cse: bool = True,\n):\n    \"\"\"\n\n    :param query: Array Shape [batch,Q Sequence length,num attention heads, head dims]\n    :param key: Array Shape [batch,KV Sequence length,num KV attention heads, head dims]\n    :param value: Array Shape [batch,KV Sequence length,num KV attention heads, head dims]\n    :param bias: Bias To be added\n    :param deterministic: bool (whenever use dropout or no)\n    :param dropout_rng: RNG Dropout\n    :param attention_drop_rate:\n    :param causal: Is Decoder or Causal\n    :param query_chunk_size: Chunk size used for query\n    :param key_chunk_size: Chunk size used for key\n    :param dtype: DataType\n    :param policy: Gradient Checkpoint Policy\n    :param precision: PrecisionLike\n    :param float32_logits:\n    :param prevent_cse:\n    :return:\n    \"\"\"\n    query = query / jnp.sqrt(query.shape[-1]).astype(dtype)\n    if float32_logits:\n        query = query.astype(jnp.float32)\n        key = key.astype(jnp.float32)\n\n    batch, q_len, num_heads, dim_per_head = query.shape\n    batch, kv_len, kv_heads, dim_per_head = key.shape\n    batch, kv_len, kv_heads, dim_per_head = value.shape\n\n    num_q = q_len // query_chunk_size\n    num_kv = kv_len // key_chunk_size\n    query = query.reshape((batch, num_q, query_chunk_size, num_heads, dim_per_head))\n    key = key.reshape((batch, num_kv, key_chunk_size, kv_heads, dim_per_head))\n    value = value.reshape((batch, num_kv, key_chunk_size, kv_heads, dim_per_head))\n\n    query = jnp.moveaxis(query, 1, 0)\n    key = jnp.moveaxis(key, 1, 0)\n    value = jnp.moveaxis(value, 1, 0)\n\n    if bias is not None:\n        for bias_dim, broadcast_dim in zip(bias.shape, (batch, num_heads, q_len, kv_len)):\n            assert bias_dim == 1 or bias_dim == broadcast_dim\n    if not deterministic and attention_drop_rate &gt; 0.0:\n        attn_dropout_rng, dropout_rng = jax.random.split(dropout_rng)\n        attn_dropout = jax.random.bernoulli(attn_dropout_rng, attention_drop_rate, (batch, num_heads, q_len, kv_len))\n    else:\n        attn_dropout = None\n\n    _chunk_bias_fn = functools.partial(\n        _chunk_attention_bias,\n        query_chunk_size, key_chunk_size, bias, deterministic,\n        attn_dropout, attention_drop_rate, causal, dtype)\n\n    def scan_attention(args):\n        query_chunk, query_chunk_idx = args\n\n        @functools.partial(jax.checkpoint, prevent_cse=prevent_cse, policy=policy)\n        def scan_kv_block(carry, args):\n            key_chunk, value_chunk, key_chunk_idx = args\n            (numerator, denominator, prev_max_score) = carry\n            attn_weights = jnp.einsum('bqhd,bkhd-&gt;bqhk', query_chunk, key_chunk, precision=precision)\n            bias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n            bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n            attn_weights = attn_weights + bias_chunk\n\n            max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n            max_score = jnp.maximum(prev_max_score, max_score)\n            max_score = jax.lax.stop_gradient(max_score)\n            exp_weights = jnp.exp(attn_weights - max_score)\n            exp_values = jnp.einsum(\n                'bqhv,bvhd-&gt;bqhd', exp_weights, value_chunk, precision=precision\n            )\n            correction = jnp.exp(prev_max_score - max_score)\n            numerator = numerator * correction + exp_values\n            denominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)\n            return Carry(numerator, denominator, max_score), None\n\n        def skip_upper_half(carry, args):\n            key_chunk, value_chunk, key_chunk_idx = args\n            skip_block = jnp.array(False)\n            if causal:\n                skip_block = query_chunk_idx &lt; key_chunk_idx\n            return jax.lax.cond(\n                skip_block,\n                lambda carry, args: (carry, None),\n                scan_kv_block,\n                carry,\n                args,\n            )\n\n        init_carry = Carry(\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=query.dtype),\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=query.dtype),\n            (-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=query.dtype),\n        )\n        (numerator, denominator, max_score), _ = lax.scan(\n            skip_upper_half, init_carry, xs=(key, value, jnp.arange(0, num_kv))\n        )\n        outputs = (numerator / denominator).astype(dtype)\n        return outputs\n\n    _, res = lax.scan(\n        lambda _, x: ((), scan_attention(x)),\n        (), xs=(query, jnp.arange(0, num_q))\n    )\n    res = rearrange(res, 'n b c h d -&gt; b (n c) h d')\n    return res\n</code></pre>"},{"location":"generated-pallas_operations-flash_attention-gpu-jax_flash_attn_gpu/","title":"pallas_operations.flash_attention.gpu.jax_flash_attn_gpu","text":"<p>Module containing fused attention forward and backward pass.</p>"},{"location":"generated-pallas_operations-flash_attention-tpu-jax_flash_attn_tpu/","title":"pallas_operations.flash_attention.tpu.jax_flash_attn_tpu","text":"<p>Flash Attention TPU kernel.</p>"},{"location":"generated-pallas_operations-flash_attention-tpu-jax_flash_attn_tpu/#src.fjformer.pallas_operations.flash_attention.tpu.jax_flash_attn_tpu.BlockSizes","title":"<code>BlockSizes</code>  <code>dataclass</code>","text":"<p>Tile sizes parameterizing FlashAttention kernels.</p> <p>Those parameters have negligible effect on numerics, but affect performance greatly.</p> Source code in <code>src/fjformer/pallas_operations/flash_attention/tpu/jax_flash_attn_tpu.py</code> <pre><code>@dataclasses.dataclass(frozen=True)\nclass BlockSizes:\n    \"\"\"Tile sizes parameterizing FlashAttention kernels.\n\n    Those parameters have negligible effect on numerics, but affect performance\n    greatly.\n    \"\"\"\n    block_q: int\n    block_k_major: int\n    block_k: int\n    block_b: int\n\n    block_q_major_dkv: Optional[int] = None\n    block_k_major_dkv: Optional[int] = None\n    block_k_dkv: Optional[int] = None\n    block_q_dkv: Optional[int] = None\n\n    block_k_major_dq: Optional[int] = None\n    block_k_dq: Optional[int] = None\n    block_q_dq: Optional[int] = None\n\n    def __post_init__(self):\n        def verify_major_minor(prefix, suffix, major, minor):\n            if minor &gt; major:\n                raise ValueError(\n                    f\"{prefix}{suffix}={minor} should be smaller than\"\n                    f\" {prefix}_major{suffix}={major}\"\n                )\n            if major % minor != 0:\n                raise ValueError(\n                    f\"{prefix}{suffix}={minor} should divide\"\n                    f\" {prefix}_major{suffix}={major}\"\n                )\n\n        verify_major_minor(\"block_k\", \"\", self.block_k_major, self.block_k)\n        if self.block_q_major_dkv is not None and self.block_q_dkv is not None:\n            verify_major_minor(\n                \"block_q\", \"_dkv\", self.block_q_major_dkv, self.block_q_dkv\n            )\n        if self.block_k_major_dkv is not None and self.block_k_dkv is not None:\n            verify_major_minor(\n                \"block_k\", \"_dkv\", self.block_k_major_dkv, self.block_k_dkv\n            )\n        if self.block_k_major_dq is not None and self.block_k_dq is not None:\n            verify_major_minor(\n                \"block_k\", \"_dq\", self.block_k_major_dq, self.block_k_dq\n            )\n\n    @property\n    def has_backward_blocks(self) -&gt; bool:\n        backward_blocks = (\n            self.block_q_major_dkv,\n            self.block_k_major_dkv,\n            self.block_q_dkv,\n            self.block_k_dkv,\n            self.block_k_major_dq,\n            self.block_k_dq,\n            self.block_q_dq,\n        )\n        return all(b is not None for b in backward_blocks)\n\n    @classmethod\n    def get_default(cls, batch_size, num_heads, q_seq_len, kv_len, d_model):\n        # TODO(apaszke,sharadmv): Select better parameters based on a heuristic.\n        del batch_size, num_heads, q_seq_len, kv_len, d_model  # Unused.\n        return BlockSizes(\n            block_q=128,\n            block_k_major=128,\n            block_k=128,\n            block_b=1,\n            block_q_major_dkv=128,\n            block_k_major_dkv=128,\n            block_k_dkv=128,\n            block_q_dkv=128,\n            block_k_major_dq=128,\n            block_k_dq=128,\n            block_q_dq=128,\n        )\n</code></pre>"},{"location":"generated-pallas_operations-flash_attention-tpu-jax_flash_attn_tpu/#src.fjformer.pallas_operations.flash_attention.tpu.jax_flash_attn_tpu.SegmentIds","title":"<code>SegmentIds</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>SegmentIds for Q and KV sequences.</p> <p>SegmentIds are used to generate segment mask, which prevents attention between different segments in the input sequence. Each array is a list of ids (integers). Only the token with the same id can attend to each other.</p> <p>Attributes:   q: segment ids along the Q sequence.   kv: segment ids along the KV sequence.</p> Source code in <code>src/fjformer/pallas_operations/flash_attention/tpu/jax_flash_attn_tpu.py</code> <pre><code>class SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n\n    SegmentIds are used to generate segment mask, which prevents attention between\n    different segments in the input sequence. Each array is a list of ids\n    (integers).\n    Only the token with the same id can attend to each other.\n\n    Attributes:\n      q: segment ids along the Q sequence.\n      kv: segment ids along the KV sequence.\n    \"\"\"\n\n    q: jax.Array  # [batch_size, q_seq_len]\n    kv: jax.Array  # [batch_size, kv_seq_len]\n</code></pre>"},{"location":"generated-pallas_operations-layer_norm-gpu-layer_norm/","title":"pallas_operations.layer_norm.gpu.layer_norm","text":"<p>Module containing fused layer norm forward and backward pass.</p>"},{"location":"generated-pallas_operations-ring_attention-ring_attention/","title":"pallas_operations.ring_attention.ring_attention","text":"<p>This module contains ring attention forward and backward pass, supporting both blockwise computation and TPU-compatible fused attention. It features blockwise computation for feedforward networks to reduce memory cost. For more details, refer to 'RingAttention' at https://arxiv.org/abs/2305.19370 and 'Blockwise Parallel Transformers' at https://arxiv.org/abs/2310.01889.</p>"},{"location":"generated-pallas_operations-ring_attention-ring_attention/#src.fjformer.pallas_operations.ring_attention.ring_attention.SegmentIds","title":"<code>SegmentIds</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>SegmentIds for Q and KV sequences.</p> <p>SegmentIds are used to generate segment mask, which prevents attention between different segments in the input sequence. Each array is a list of ids (integers). Only the token with the same id can attend to each other.</p> <p>Attributes:   q: segment ids along the Q sequence.   kv: segment ids along the KV sequence.</p> Source code in <code>src/fjformer/pallas_operations/ring_attention/ring_attention.py</code> <pre><code>class SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n\n    SegmentIds are used to generate segment mask, which prevents attention between\n    different segments in the input sequence. Each array is a list of ids\n    (integers).\n    Only the token with the same id can attend to each other.\n\n    Attributes:\n      q: segment ids along the Q sequence.\n      kv: segment ids along the KV sequence.\n    \"\"\"\n\n    q: jax.Array  # [q_seq_len]\n    kv: jax.Array  # [kv_seq_len]\n</code></pre>"},{"location":"generated-pallas_operations-rms_norm-gpu-rms_norm/","title":"pallas_operations.rms_norm.gpu.rms_norm","text":"<p>Module containing rms forward and backward pass.</p>"},{"location":"generated-pallas_operations-softmax-gpu-softmax/","title":"pallas_operations.softmax.gpu.softmax","text":"<p>Pallas softmax kernel.</p>"},{"location":"generated-pallas_operations-softmax-gpu-softmax/#src.fjformer.pallas_operations.softmax.gpu.softmax.softmax","title":"<code>softmax(x, *, axis=-1, num_warps=4, interpret=False, debug=False)</code>","text":"<p>Computes the softmax of the input array along the specified axis.</p> <p>Args:   x: input array   axis: the axis along which to perform the computation   num_warps: the number of warps to use for executing the Triton kernel   interpret: whether to interpret the kernel using pallas   debug: whether to use pallas in debug mode</p> <p>Returns:   The result of the softmax operation over the specified axis of x.</p> Source code in <code>src/fjformer/pallas_operations/softmax/gpu/softmax.py</code> <pre><code>@functools.partial(jax.jit, static_argnames=[\"axis\", \"num_warps\", \"interpret\",\n                                             \"debug\"])\ndef softmax(\n        x: jax.Array, *, axis: int = -1, num_warps: int = 4,\n        interpret: bool = False, debug: bool = False\n) -&gt; jax.Array:\n    \"\"\"Computes the softmax of the input array along the specified axis.\n\n    Args:\n      x: input array\n      axis: the axis along which to perform the computation\n      num_warps: the number of warps to use for executing the Triton kernel\n      interpret: whether to interpret the kernel using pallas\n      debug: whether to use pallas in debug mode\n\n    Returns:\n      The result of the softmax operation over the specified axis of x.\n    \"\"\"\n    axis = axis if axis &gt;= 0 else len(x.shape) + axis\n    if axis != len(x.shape) - 1:\n        raise NotImplementedError(\n            \"reductions along non-trailing dimension unsupported\")\n\n    row_len = x.shape[-1]\n\n    block_row = pl.next_power_of_2(row_len)\n    out_shape = jax.ShapeDtypeStruct(shape=(row_len,), dtype=x.dtype)\n\n    kernel = functools.partial(_vmappable_softmax_kernel, block_row=block_row)\n    f = pl.pallas_call(kernel, num_warps=num_warps, num_stages=1, grid=(),\n                       out_shape=out_shape, debug=debug, interpret=interpret)\n\n    for _ in range(len(x.shape) - 1):\n        f = jax.vmap(f)\n\n    return f(x)\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_kernel/","title":"pallas_operations.splash_attention.tpu.splash_attention_kernel","text":"<p>Implementation of Sparse Flash Attention, a.k.a. \"Splash\" attention.</p>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_kernel/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_kernel.BlockSizes","title":"<code>BlockSizes</code>  <code>dataclass</code>","text":"<p>Tile sizes parameterizing SplashAttention kernels.</p> <p>Those parameters have negligible effect on numerics, but affect performance greatly.</p> <p>Note that changing the layouts only influences the physical layout that the kernel will enforce. The logical interface to splash attention always takes the head dimension as the minormost one.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_kernel.py</code> <pre><code>@dataclasses.dataclass(unsafe_hash=True)\nclass BlockSizes:\n    \"\"\"Tile sizes parameterizing SplashAttention kernels.\n\n    Those parameters have negligible effect on numerics, but affect performance\n    greatly.\n\n    Note that changing the layouts only influences the physical layout that the\n    kernel will enforce. The logical interface to splash attention always takes\n    the head dimension as the minormost one.\n    \"\"\"\n    block_q: int\n    block_kv: int\n    block_kv_compute: int | None = None\n\n    block_q_dkv: int | None = None\n    block_kv_dkv: int | None = None\n    block_kv_dkv_compute: int | None = None\n\n    block_q_dq: int | None = None\n    block_kv_dq: int | None = None\n\n    use_fused_bwd_kernel: bool = False\n\n    q_layout: QKVLayout = QKVLayout.HEAD_DIM_MINOR\n    k_layout: QKVLayout = QKVLayout.HEAD_DIM_MINOR\n    v_layout: QKVLayout = QKVLayout.HEAD_DIM_MINOR\n\n    def __post_init__(self):\n        if self.block_kv_compute is None:\n            self.block_kv_compute = self.block_kv\n        if self.block_kv_dkv_compute is None:\n            self.block_kv_dkv_compute = self.block_kv_dkv\n        if self.use_fused_bwd_kernel:\n            if self.block_q_dq is not None or self.block_kv_dq is not None:\n                raise ValueError(\n                    \"Block sizes for dq kernel are not needed with a fused kernel.\"\n                )\n\n    @property\n    def has_backward_blocks(self) -&gt; bool:\n        backward_blocks = (\n            self.block_q_dkv, self.block_kv_dkv, self.block_kv_dkv_compute,\n        )\n        if not self.use_fused_bwd_kernel:\n            backward_blocks += (self.block_q_dq, self.block_kv_dq)\n        return all(b is not None for b in backward_blocks)\n\n    @classmethod\n    def get_default(cls):\n        # TODO(apaszke,sharadmv): Select better parameters based on a heuristic.\n        return BlockSizes(\n            block_q=128,\n            block_kv=128,\n            block_kv_compute=128,\n            block_q_dkv=128,\n            block_kv_dkv=128,\n            block_kv_dkv_compute=128,\n            block_q_dq=128,\n            block_kv_dq=128,\n        )\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_kernel/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_kernel.SegmentIds","title":"<code>SegmentIds</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>SegmentIds for Q and KV sequences.</p> <p>SegmentIds are a mechanims to ensure that there is no cross-attention between segments (fraction of a sequence) that have been concatenated together into a sequence. Each array is a list of ids (integers). Only tokens with the same id are allowed to attend to each other.</p> <p>The static mask (e.g. causal) is \"and-ed\" with the segment id mask to form the actual attention mask. It is important that the latter does not have any all-zero rows (along dimension kv). Otherwise it would result in a invalid softmax (the denominator would be 0). This condition holds for causal self-attention because in this case segment ids form a block diagonal matrix so at least one element in each row is set. It is easy to break this condition with non-self-attention configurations. Attributes:   q: segment ids along the Q sequence   kv: segment ids along the KV sequence</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_kernel.py</code> <pre><code>class SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n\n    SegmentIds are a mechanims to ensure that there is no cross-attention between\n    segments (fraction of a sequence) that have been concatenated together into a\n    sequence. Each array is a list of ids (integers). Only tokens with the same\n    id are allowed to attend to each other.\n\n    The static mask (e.g. causal) is \"and-ed\" with the segment id mask to form\n    the actual attention mask. It is important that the latter does not have any\n    all-zero rows (along dimension kv). Otherwise it would result in a invalid\n    softmax (the denominator would be 0).\n    This condition holds for causal self-attention because in this case segment\n    ids form a block diagonal matrix so at least one element in each row is set.\n    It is easy to break this condition with non-self-attention configurations.\n    Attributes:\n      q: segment ids along the Q sequence\n      kv: segment ids along the KV sequence\n    \"\"\"\n\n    q: jax.Array  # [q_seq_len]\n    kv: jax.Array  # [kv_seq_len]\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_kernel/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_kernel.SplashAttentionKernel","title":"<code>SplashAttentionKernel</code>","text":"Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_kernel.py</code> <pre><code>@jax.tree_util.register_pytree_node_class\nclass SplashAttentionKernel:\n\n    def __init__(\n            self,\n            fwd_mask_info: mask_info_lib.MaskInfo,\n            dq_mask_info: mask_info_lib.MaskInfo | None,\n            dkv_mask_info: mask_info_lib.MaskInfo | None,\n            **kwargs,\n    ):\n        self.kwargs = kwargs\n        self.fwd_mask_info = fwd_mask_info\n        self.dq_mask_info = dq_mask_info\n        self.dkv_mask_info = dkv_mask_info\n\n    def __call__(self, *args, **kwargs) -&gt; SplashCustomReturnType:\n        return _splash_attention(\n            self.fwd_mask_info,\n            self.dq_mask_info,\n            self.dkv_mask_info,\n            *args,\n            **kwargs,\n            **self.kwargs,\n        )\n\n    def manual_sharding_spec(self, sharding: jax.sharding.NamedSharding):\n        \"\"\"Returns a value that can be used as a shard_map partition spec for the kernel.\"\"\"\n        if self.fwd_mask_info.data_next is not None:\n            block_mask_shape = self.fwd_mask_info.data_next.shape\n            try:\n                shard_shape = sharding.shard_shape(block_mask_shape)\n            except ValueError as exc:\n                raise ValueError(\n                    \"The sharding must divide the mask blocks evenly between devices\"\n                ) from exc\n            if block_mask_shape[-1] != shard_shape[-1]:\n                raise ValueError(\"Sharding the kv sequence dimension is not supported\")\n        spec = sharding.spec\n        assert len(spec) == 2\n        replicated = jax.sharding.PartitionSpec()\n        # Shard q_sequence over the sequence dimension only.\n        q_sequence_spec = jax.sharding.PartitionSpec(spec[1])\n        mask_info_specs = mask_info_lib.MaskInfo(  # pytype: disable=wrong-arg-types\n            data_next=spec if self.fwd_mask_info.data_next is not None else None,\n            mask_next=spec if self.fwd_mask_info.mask_next is not None else None,\n            block_mask=spec if self.fwd_mask_info.block_mask is not None else None,\n            partial_mask_blocks=replicated\n            if self.fwd_mask_info.partial_mask_blocks is not None\n            else None,\n            q_sequence=q_sequence_spec\n            if self.fwd_mask_info.q_sequence is not None\n            else None,\n        )\n        return SplashAttentionKernel(\n            mask_info_specs,\n            mask_info_specs if self.dq_mask_info is not None else None,\n            mask_info_specs if self.dkv_mask_info is not None else None,\n            **self.kwargs,\n        )\n\n    def tree_flatten(self):\n        return (\n            (self.fwd_mask_info, self.dq_mask_info, self.dkv_mask_info),\n            self.kwargs,\n        )\n\n    @classmethod\n    def tree_unflatten(cls, kwargs, values):\n        fwd_mask_info, dq_mask_info, dkv_mask_info = values\n        # NamedTuples are not preserved during pytree serialization.\n        dq_mask_info = (\n            mask_info_lib.MaskInfo(*dq_mask_info)\n            if dq_mask_info is not None\n            else None\n        )\n        dkv_mask_info = (\n            mask_info_lib.MaskInfo(*dkv_mask_info)\n            if dkv_mask_info is not None\n            else None\n        )\n        return SplashAttentionKernel(\n            mask_info_lib.MaskInfo(*fwd_mask_info),\n            dq_mask_info,\n            dkv_mask_info,\n            **kwargs,\n        )\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_kernel/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_kernel.SplashAttentionKernel.manual_sharding_spec","title":"<code>manual_sharding_spec(sharding)</code>","text":"<p>Returns a value that can be used as a shard_map partition spec for the kernel.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_kernel.py</code> <pre><code>def manual_sharding_spec(self, sharding: jax.sharding.NamedSharding):\n    \"\"\"Returns a value that can be used as a shard_map partition spec for the kernel.\"\"\"\n    if self.fwd_mask_info.data_next is not None:\n        block_mask_shape = self.fwd_mask_info.data_next.shape\n        try:\n            shard_shape = sharding.shard_shape(block_mask_shape)\n        except ValueError as exc:\n            raise ValueError(\n                \"The sharding must divide the mask blocks evenly between devices\"\n            ) from exc\n        if block_mask_shape[-1] != shard_shape[-1]:\n            raise ValueError(\"Sharding the kv sequence dimension is not supported\")\n    spec = sharding.spec\n    assert len(spec) == 2\n    replicated = jax.sharding.PartitionSpec()\n    # Shard q_sequence over the sequence dimension only.\n    q_sequence_spec = jax.sharding.PartitionSpec(spec[1])\n    mask_info_specs = mask_info_lib.MaskInfo(  # pytype: disable=wrong-arg-types\n        data_next=spec if self.fwd_mask_info.data_next is not None else None,\n        mask_next=spec if self.fwd_mask_info.mask_next is not None else None,\n        block_mask=spec if self.fwd_mask_info.block_mask is not None else None,\n        partial_mask_blocks=replicated\n        if self.fwd_mask_info.partial_mask_blocks is not None\n        else None,\n        q_sequence=q_sequence_spec\n        if self.fwd_mask_info.q_sequence is not None\n        else None,\n    )\n    return SplashAttentionKernel(\n        mask_info_specs,\n        mask_info_specs if self.dq_mask_info is not None else None,\n        mask_info_specs if self.dkv_mask_info is not None else None,\n        **self.kwargs,\n    )\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_kernel/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_kernel.get_kernel_name","title":"<code>get_kernel_name(is_mqa, save_residuals, is_segmented, phase)</code>","text":"<p>Returns a unique name for all SplashAttention kernel variants.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_kernel.py</code> <pre><code>def get_kernel_name(\n        is_mqa: bool, save_residuals: bool, is_segmented: bool, phase: str\n) -&gt; str:\n    \"\"\"Returns a unique name for all SplashAttention kernel variants.\"\"\"\n\n    assert phase == \"dq\" or phase == \"dkv\" or phase == \"fwd\"\n    # Saving residuals is supported only for the fwd phase.\n    assert not save_residuals or phase == \"fwd\"\n    residuals = \"\"\n    if save_residuals:\n        residuals = \"_residuals\"\n    elif phase == \"fwd\":\n        residuals = \"_no_residuals\"\n    attention_type = \"mqa\" if is_mqa else \"mha\"\n    segments = \"_segmented\" if is_segmented else \"\"\n    return f\"splash_{attention_type}_{phase}{segments}{residuals}\"\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask/","title":"pallas_operations.splash_attention.tpu.splash_attention_mask","text":"<p>Mini-mask creation library.</p>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_mask.CausalMask","title":"<code>CausalMask</code>","text":"<p>             Bases: <code>_ComputableMask</code></p> <p>Lazy causal mask, prevents the model from attending to future tokens.</p> <p>Attributes:   offset: Offset of q start wrt kv. A positive offset shifts the bottom     triangle upward, a negative one shifts it downward. A negative offset     makes the first 'offset' rows of the attention matrix all 0s which leads     to undefined softmax.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_mask.py</code> <pre><code>class CausalMask(_ComputableMask):\n    \"\"\"Lazy causal mask, prevents the model from attending to future tokens.\n\n    Attributes:\n      offset: Offset of q start wrt kv. A positive offset shifts the bottom\n        triangle upward, a negative one shifts it downward. A negative offset\n        makes the first 'offset' rows of the attention matrix all 0s which leads\n        to undefined softmax.\n    \"\"\"\n\n    offset: int\n\n    def __init__(\n            self,\n            shape: Tuple[int, int],\n            offset: int = 0,\n            shard_count: int = 1,\n    ):\n        self.offset = offset\n\n        def causal_mask_function(q_ids, kv_ids):\n            # When evaluating the mask in _process_mask we typically work with numpy\n            # array views.\n            # Avoid the addition when possible to avoid instantiating an actual array.\n            if self.offset == 0:\n                return q_ids &gt;= kv_ids\n            else:\n                return q_ids + self.offset &gt;= kv_ids\n\n        mask_function = causal_mask_function\n\n        super().__init__(\n            shape=shape,\n            mask_function=mask_function,\n            shard_count=shard_count,\n        )\n\n    def __eq__(self, other: object):\n        if not isinstance(other, type(self)):\n            return NotImplemented\n\n        return (\n                self.shape == other.shape\n                and self.offset == other.offset\n                and np.array_equal(self.q_sequence, other.q_sequence)\n        )\n\n    def __hash__(self):\n        return hash((\n            type(self),\n            self.shape,\n            self.offset,\n            self.q_sequence.tobytes() if self.q_sequence is not None else None,\n        ))\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_mask.FullMask","title":"<code>FullMask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Mask</code></p> <p>Lazy full mask, allows all tokens to attend to all other tokens.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_mask.py</code> <pre><code>@dataclasses.dataclass(frozen=True)\nclass FullMask(Mask):\n    \"\"\"Lazy full mask, allows all tokens to attend to all other tokens.\"\"\"\n\n    # TODO(amagni): Transform FullMask into a _ComputableMask.\n\n    _shape: tuple[int, int]\n\n    def __post_init__(self):\n        if not isinstance(self.shape, tuple):\n            raise ValueError(f'Unsupported shape type: {type(self.shape)}')\n\n    @property\n    def shape(self) -&gt; Tuple[int, ...]:\n        return self._shape\n\n    def __getitem__(self, idx) -&gt; np.ndarray:\n        if len(idx) != 2:\n            raise NotImplementedError(f'Unsupported slice: {idx}')\n        i, j = idx\n        if not isinstance(i, slice) or not isinstance(j, slice):\n            raise NotImplementedError(f'Unsupported slice: {idx}')\n        i = _fill_slice(i, self.shape[0])\n        j = _fill_slice(j, self.shape[1])\n        return np.ones((i.stop - i.start, j.stop - j.start), dtype=np.bool_)\n\n    def __eq__(self, other: object):\n        if not isinstance(other, type(self)):\n            return NotImplemented\n\n        return self.shape == other.shape\n\n    def __hash__(self):\n        return hash((type(self), self.shape))\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_mask.LocalMask","title":"<code>LocalMask</code>","text":"<p>             Bases: <code>Mask</code></p> <p>Lazy local mask, prevents model from attending to tokens outside window.</p> <p>Attributes:   _shape: Shape of the 2-dim mask: (q_seq_len, kv_seq_len).   window_size: Size of the two sides of the local window (None identifes no     limit for the given side).   offset: Offset of q start wrt kv. A positive offset shifts the bottom     triangle upward, a negative one shifts it downward. A negative offset     makes the first 'offset' rows of the attention matrix all 0s which leads     to undefined softmax.   _q_sequence: Important for performance.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_mask.py</code> <pre><code>class LocalMask(Mask):\n    \"\"\"Lazy local mask, prevents model from attending to tokens outside window.\n\n    Attributes:\n      _shape: Shape of the 2-dim mask: (q_seq_len, kv_seq_len).\n      window_size: Size of the two sides of the local window (None identifes no\n        limit for the given side).\n      offset: Offset of q start wrt kv. A positive offset shifts the bottom\n        triangle upward, a negative one shifts it downward. A negative offset\n        makes the first 'offset' rows of the attention matrix all 0s which leads\n        to undefined softmax.\n      _q_sequence: Important for performance.\n    \"\"\"\n\n    # TODO(amagni): Transform LocalMask into a _ComputableMask.\n\n    _shape: Tuple[int, int]\n    window_size: Tuple[int | None, int | None]\n    offset: int\n    _q_sequence: np.ndarray | None = None\n\n    def __init__(\n            self,\n            shape: Tuple[int, int],\n            window_size: Tuple[int | None, int | None],\n            offset: int,\n            shard_count: int = 1,\n    ):\n        self._shape = shape\n        self.window_size = window_size\n        self.offset = offset\n\n        if self.shape[0] % (shard_count * shard_count) != 0:\n            raise ValueError(\n                f'Shard count squared ({shard_count * shard_count}) must'\n                f' divide Q seq_len ({self.shape[0]}) evenly.'\n            )\n\n    @property\n    def shape(self) -&gt; Tuple[int, int]:\n        return self._shape\n\n    def __getitem__(self, idx) -&gt; np.ndarray:\n        if len(idx) != 2:\n            raise NotImplementedError(f'Unsupported slice: {idx}')\n        q_slice, kv_slice = idx\n        if not isinstance(q_slice, slice) or not isinstance(kv_slice, slice):\n            raise NotImplementedError(f'Unsupported slice: {idx}')\n\n        q_slice = _fill_slice(q_slice, self.shape[0])\n        kv_slice = _fill_slice(kv_slice, self.shape[1])\n\n        if self._q_sequence is None:\n            rows = np.arange(q_slice.start, q_slice.stop)\n        else:\n            rows = self._q_sequence[q_slice]\n\n        cols = np.arange(kv_slice.start, kv_slice.stop)\n\n        left_size, right_size = self.window_size\n\n        if left_size is None and right_size is None:\n            return np.ones((rows.shape[0], cols.shape[0]), dtype=np.bool_)\n        else:\n            expanded_cols = cols[None, :]\n            if self.offset != 0:\n                expanded_rows = rows[:, None] + self.offset\n            else:\n                expanded_rows = rows[:, None]\n            if left_size is not None and right_size is not None:\n                return (expanded_rows &lt;= expanded_cols + left_size) &amp; (\n                        expanded_cols - right_size &lt;= expanded_rows\n                )\n\n            elif left_size is not None and right_size is None:\n                return expanded_rows &lt;= expanded_cols + left_size\n            else:\n                assert left_size is None and right_size is not None\n                return expanded_cols - right_size &lt;= expanded_rows\n\n    def __eq__(self, other: object):\n        if not isinstance(other, type(self)):\n            return NotImplemented\n\n        return (\n                self.shape == other.shape\n                and self.window_size == other.window_size\n                and self.offset == other.offset\n                and (True if self._q_sequence is None else\n                     np.array_equal(self._q_sequence, other._q_sequence))\n        )\n\n    def __hash__(self):\n        return hash((\n            type(self),\n            self.shape,\n            self.window_size,\n            self.offset,\n            self._q_sequence.tobytes() if self._q_sequence is not None else None,\n        ))\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_mask.Mask","title":"<code>Mask</code>","text":"<p>A base class for splash attention masks.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_mask.py</code> <pre><code>class Mask:\n    \"\"\"A base class for splash attention masks.\"\"\"\n\n    @property\n    def shape(self) -&gt; Tuple[int, ...]:\n        raise NotImplementedError\n\n    def __getitem__(self, idx) -&gt; np.ndarray:\n        raise NotImplementedError\n\n    def __bool__(self) -&gt; bool:\n        raise NotImplementedError(\n            'Conversion to bool is unsupported. Could be caused by using logical'\n            ' instead of bitwise operations on masks.'\n        )\n\n    def __or__(self, other: 'Mask') -&gt; 'Mask':\n        if self.shape != other.shape:\n            raise ValueError(\n                f'Invalid shape for other: {other.shape}, expected: {self.shape}'\n            )\n        return LogicalOr(self, other)\n\n    def __and__(self, other: 'Mask') -&gt; 'Mask':\n        if self.shape != other.shape:\n            raise ValueError(\n                f'Invalid shape for other: {other.shape}, expected: {self.shape}'\n            )\n        return LogicalAnd(self, other)\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_mask.MultiHeadMask","title":"<code>MultiHeadMask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Mask</code></p> <p>Lazy multihead mask, combines multiple lazy masks one per head.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_mask.py</code> <pre><code>@dataclasses.dataclass\nclass MultiHeadMask(Mask):\n    \"\"\"Lazy multihead mask, combines multiple lazy masks one per head.\"\"\"\n\n    masks: Sequence[Mask]\n\n    def __post_init__(self):\n        if not self.masks:\n            raise ValueError('Unsupported empty tuple of masks')\n\n        shape = self.masks[0].shape\n        for mask in self.masks[1:]:\n            if shape != mask.shape:\n                raise ValueError(\n                    f'Unexpected mask shape, got: {mask.shape}, expected: {shape}'\n                )\n\n        if not all(isinstance(mask, Mask) for mask in self.masks):\n            raise ValueError('masks should be of type Mask')\n\n        if any(isinstance(mask, MultiHeadMask) for mask in self.masks):\n            raise ValueError('Nesting MultiHeadMasks is not supported')\n\n    @property\n    def shape(self) -&gt; Tuple[int, ...]:\n        return (len(self.masks),) + self.masks[0].shape\n\n    def __getitem__(self, idx) -&gt; np.ndarray:\n        if len(idx) != 3:\n            raise NotImplementedError(f'Unsupported slice: {idx}')\n\n        head_slice = idx[0]\n        if isinstance(head_slice, int):\n            assert head_slice &gt;= 0 and head_slice &lt;= len(self.masks)\n            return self.masks[head_slice][idx[1:]]\n        else:\n            slice_masks = [mask[idx[1:]] for mask in self.masks[head_slice]]\n            return np.stack(slice_masks)\n\n    def __eq__(self, other: object):\n        if not isinstance(other, type(self)):\n            return NotImplemented\n\n        return self.masks == other.masks\n\n    def __hash__(self):\n        return hash((type(self),) + tuple(hash(mask) for mask in self.masks))\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_mask.NumpyMask","title":"<code>NumpyMask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Mask</code></p> <p>A mask backed by a dense numpy array.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_mask.py</code> <pre><code>@dataclasses.dataclass\nclass NumpyMask(Mask):\n    \"\"\"A mask backed by a dense numpy array.\"\"\"\n\n    array: np.ndarray\n\n    def __post_init__(self):\n        if self.array.ndim != 2:\n            raise ValueError('Expected a 2-dim array')\n\n        if self.array.dtype != np.bool_:\n            raise ValueError('Mask must be a boolean array')\n\n    @property\n    def shape(self) -&gt; Tuple[int, ...]:\n        return self.array.shape\n\n    def __getitem__(self, idx) -&gt; np.ndarray:\n        return self.array[idx]\n\n    def __eq__(self, other: object):\n        if not isinstance(other, type(self)):\n            return NotImplemented\n\n        return np.array_equal(self.array, other.array, equal_nan=True)\n\n    def __hash__(self):\n        return hash((type(self), self.array.tobytes()))\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_mask.make_causal_mask","title":"<code>make_causal_mask(shape, offset=0)</code>","text":"<p>Makes a causal attention mask.</p> <p>Args:   shape: Shape of the 2-dim mask: (q_seq_len, kv_seq_len).   offset: Offset of q start wrt kv. A positive offset shifts the bottom     triangle upward, a negative one shifts it downward. A negative offset     makes the first 'offset' rows of the attention matrix all 0s which leads     to undefined softmax.</p> <p>Returns:   The causal mask.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_mask.py</code> <pre><code>def make_causal_mask(shape: Tuple[int, int], offset: int = 0) -&gt; np.ndarray:\n    \"\"\"Makes a causal attention mask.\n\n    Args:\n      shape: Shape of the 2-dim mask: (q_seq_len, kv_seq_len).\n      offset: Offset of q start wrt kv. A positive offset shifts the bottom\n        triangle upward, a negative one shifts it downward. A negative offset\n        makes the first 'offset' rows of the attention matrix all 0s which leads\n        to undefined softmax.\n\n    Returns:\n      The causal mask.\n    \"\"\"\n    q_seq_len, kv_seq_len = shape\n    q_idx = np.arange(q_seq_len, dtype=np.int32)\n    kv_idx = np.arange(kv_seq_len, dtype=np.int32)\n    return (q_idx[:, None] + offset &gt;= kv_idx[None, :]).astype(np.bool_)\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_mask.make_local_attention_mask","title":"<code>make_local_attention_mask(shape, window_size, *, offset=0)</code>","text":"<p>Makes a local attention mask.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_mask.py</code> <pre><code>def make_local_attention_mask(\n        shape: Tuple[int, int],\n        window_size: Tuple[int | None, int | None],\n        *,\n        offset: int = 0,\n) -&gt; np.ndarray:\n    \"\"\"Makes a local attention mask.\"\"\"\n    q_seq_len, kv_seq_len = shape\n    q_idx = np.arange(q_seq_len, dtype=np.int32)\n    kv_idx = np.arange(kv_seq_len, dtype=np.int32)\n    mask = np.ones((q_seq_len, kv_seq_len), dtype=np.bool_)\n    left, right = window_size\n    if left is not None:\n        mask = mask &amp; (q_idx[:, None] - left + offset &lt;= kv_idx[None, :])\n    if right is not None:\n        mask = mask &amp; (q_idx[:, None] + right + offset &gt;= kv_idx[None, :])\n    return mask.astype(np.bool_)\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_mask.make_random_mask","title":"<code>make_random_mask(shape, sparsity, seed)</code>","text":"<p>Makes a random attention mask.</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_mask.py</code> <pre><code>def make_random_mask(\n        shape: Tuple[int, int], sparsity: float, seed: int\n) -&gt; np.ndarray:\n    \"\"\"Makes a random attention mask.\"\"\"\n    np.random.seed(seed)\n    return np.random.binomial(n=1, p=1.0 - sparsity, size=shape).astype(np.bool_)\n</code></pre>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask_info/","title":"pallas_operations.splash_attention.tpu.splash_attention_mask_info","text":"<p>Mini-mask creation library.</p>"},{"location":"generated-pallas_operations-splash_attention-tpu-splash_attention_mask_info/#src.fjformer.pallas_operations.splash_attention.tpu.splash_attention_mask_info.MaskInfo","title":"<code>MaskInfo</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Contains runtime masking information for the Splash attention kernel.</p> <p>The arrays data_next, mask_next and block_mask are placed in TPU scalar-memory. This is a scarse resource so the mask creation logic attempts to shrink the data-type of these arrays to the smallest possible one. This can be: np.int32, np.int16 or np.int8.</p> <p>For the arrays data_next, mask_next and block_mask the size of the first dimension can be one of the two following values: num_head or num_head_shards. The first dimension has size: * num_head_shards when there is only one unique mask for each head in a shard. In this case the three arrays are broadcasted to all the heads in the shard. * num_heads when there is more than one unique mask for each head in the shard.</p> <p>Attributes:   data_next: An integer[num_heads_or_shards, num_q_blocks, num_kv_blocks]     NumPy array where each entry contains the next <code>kv</code> block index to     prefetch.   mask_next: An integer[num_heads_or_shards, num_q_blocks, num_kv_blocks]     NumPy array where each entry contains the next mask block index in     <code>partial_mask_blocks</code> to prefetch.   block_mask: An integer[num_heads_or_shards, num_q_blocks, num_kv_blocks]     NumPy array whose entries can be 0, 1 or 2. An entry of 0 indicates that     the corresponding block in the full mask was all zeros. An entry of 1     indicates that the corresponding block in the full mask contained both     zeros and ones. An entry of 2 indicates the corresponding block was     entirely ones.   partial_mask_blocks: A i32[num_partial_blocks, block_q, block_kv] NumPy     array that contains the blocks of the original mask that contained both     zeros and ones. The entries in <code>mask_next</code> point to indices in the first     axis of this array.   q_sequence: A i32[q_sequence_length] NumPy array. When using causal masking,     this contains the list of indices that correspond to q tokens. For plain     causal this is just np.arange(q_sequence_length).</p> Source code in <code>src/fjformer/pallas_operations/splash_attention/tpu/splash_attention_mask_info.py</code> <pre><code>class MaskInfo(NamedTuple):\n    \"\"\"Contains runtime masking information for the Splash attention kernel.\n\n    The arrays data_next, mask_next and block_mask are placed in TPU\n    scalar-memory. This is a scarse resource so the mask creation logic attempts\n    to shrink the data-type of these arrays to the smallest possible one.\n    This can be: np.int32, np.int16 or np.int8.\n\n    For the arrays data_next, mask_next and block_mask the size of the first\n    dimension can be one of the two following values: num_head or\n    num_head_shards.\n    The first dimension has size:\n    * num_head_shards when there is only one unique mask for each head in a shard.\n    In this case the three arrays are broadcasted to all the heads in the shard.\n    * num_heads when there is more than one unique mask for each head in the\n    shard.\n\n    Attributes:\n      data_next: An integer[num_heads_or_shards, num_q_blocks, num_kv_blocks]\n        NumPy array where each entry contains the next `kv` block index to\n        prefetch.\n      mask_next: An integer[num_heads_or_shards, num_q_blocks, num_kv_blocks]\n        NumPy array where each entry contains the next mask block index in\n        `partial_mask_blocks` to prefetch.\n      block_mask: An integer[num_heads_or_shards, num_q_blocks, num_kv_blocks]\n        NumPy array whose entries can be 0, 1 or 2. An entry of 0 indicates that\n        the corresponding block in the full mask was all zeros. An entry of 1\n        indicates that the corresponding block in the full mask contained both\n        zeros and ones. An entry of 2 indicates the corresponding block was\n        entirely ones.\n      partial_mask_blocks: A i32[num_partial_blocks, block_q, block_kv] NumPy\n        array that contains the blocks of the original mask that contained both\n        zeros and ones. The entries in `mask_next` point to indices in the first\n        axis of this array.\n      q_sequence: A i32[q_sequence_length] NumPy array. When using causal masking,\n        this contains the list of indices that correspond to q tokens. For plain\n        causal this is just np.arange(q_sequence_length).\n    \"\"\"\n\n    data_next: np.ndarray | None\n    mask_next: np.ndarray | None\n    block_mask: np.ndarray | None\n    partial_mask_blocks: np.ndarray | None\n    q_sequence: np.ndarray | None\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/","title":"partition_utils.mesh_utils","text":""},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.create_mesh","title":"<code>create_mesh(axis_dims=(1, -1, 1, 1), axis_names=('dp', 'fsdp', 'tp', 'sp'), backend='')</code>","text":"<p>The create_mesh function creates a mesh object that can be used to shard arrays.</p> <p>Parameters:</p> Name Type Description Default <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the dimensions of the mesh</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Name the axes of the mesh</p> <code>('dp', 'fsdp', 'tp', 'sp')</code> <code>backend</code> <p>Specify the backend to use</p> <code>''</code> <p>Returns:</p> Type Description <p>A mesh object</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def create_mesh(\n        axis_dims: Sequence[int] = (1, -1, 1, 1), axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"), backend=\"\"\n):\n    \"\"\"\n    The create_mesh function creates a mesh object that can be used to shard arrays.\n\n    :param axis_dims: Sequence[int]: Specify the dimensions of the mesh\n    :param axis_names: Sequence[str]: Name the axes of the mesh\n    :param backend: Specify the backend to use\n    :return: A mesh object\n\n    \"\"\"\n    array_devices = jax.numpy.ones((len(jax.devices() if backend == \"\" else jax.devices(backend)), 1))\n    resh = array_devices.reshape(axis_dims).shape\n\n    return jax.sharding.Mesh(\n        create_device_mesh(resh), axis_names\n    )\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.flatten_tree","title":"<code>flatten_tree(xs, is_leaf=None, sep=None)</code>","text":"<p>The flatten_tree function takes a nested structure of arrays and returns a dictionary mapping from string keys to the corresponding array values. The string keys are derived from the tree path to each value, with <code>sep</code> used as the separator between levels in the tree. For example:</p> <p>Parameters:</p> Name Type Description Default <code>xs</code> <p>Store the tree structure</p> required <code>is_leaf</code> <p>Determine if a node is a leaf</p> <code>None</code> <code>sep</code> <p>Specify the separator between each key in the path</p> <code>None</code> <p>Returns:</p> Type Description <p>A dict of flattened tree paths to values</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def flatten_tree(xs, is_leaf=None, sep=None):\n    \"\"\"\n    The flatten_tree function takes a nested structure of arrays and returns a\n    dictionary mapping from string keys to the corresponding array values. The\n    string keys are derived from the tree path to each value, with `sep` used as\n    the separator between levels in the tree. For example:\n\n    :param xs: Store the tree structure\n    :param is_leaf: Determine if a node is a leaf\n    :param sep: Specify the separator between each key in the path\n    :return: A dict of flattened tree paths to values\n\n    \"\"\"\n    flattened, _ = jax.tree_util.tree_flatten_with_path(xs, is_leaf=is_leaf)\n    output = {}\n    for key, val in flattened:\n        output[tree_path_to_string(key, sep=sep)] = val\n    return output\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.get_jax_mesh","title":"<code>get_jax_mesh(axis_dims, names)</code>","text":"<p>The get_jax_mesh function takes a string of the form:     &lt;axis_dims&gt; where axis_dims is a comma-separated list of dimensions, each dimension being either:     &lt;name&gt;:&lt;dim&gt;  or  &lt;dim&gt; If there are no names, then the default names 'x', 'y', and 'z' will be used. If there are fewer than three dimensions, then the remaining dimensions will be set to 1. For example:</p> <p>Parameters:</p> Name Type Description Default <code>axis_dims</code> <p>Specify the dimensions of the mesh</p> required <code>names</code> <p>Specify the names of the dimensions in</p> required <p>Returns:</p> Type Description <p>A mesh object</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def get_jax_mesh(axis_dims, names):\n    \"\"\"\n    The get_jax_mesh function takes a string of the form:\n        &amp;lt;axis_dims&amp;gt;\n    where axis_dims is a comma-separated list of dimensions, each dimension being either:\n        &amp;lt;name&amp;gt;:&amp;lt;dim&amp;gt;  or  &amp;lt;dim&amp;gt;\n    If there are no names, then the default names 'x', 'y', and 'z' will be used. If there are fewer than three dimensions, then the remaining dimensions will be set to 1. For example:\n\n    :param axis_dims: Specify the dimensions of the mesh\n    :param names: Specify the names of the dimensions in\n    :return: A mesh object\n\n    \"\"\"\n    if axis_dims.startswith('!'):\n        mesh_axis_splitting = True\n        axis_dims = axis_dims[1:]\n    else:\n        mesh_axis_splitting = False\n\n    if ':' in axis_dims:\n        dims = []\n        dim_names = []\n        for axis in axis_dims.split(','):\n            name, dim = axis.split(':')\n            assert name in names\n            dims.append(int(dim))\n            dim_names.append(name)\n        assert (set(dim_names) == set(names))\n    else:\n        dims = [int(x) for x in axis_dims.split(',')]\n        dim_names = names\n    assert len(dims) == len(names)\n    mesh_shape = np.arange(jax.device_count()).reshape(dims).shape\n    if mesh_axis_splitting:\n        physical_mesh = np.array(jax.devices()).reshape(mesh_shape)\n    else:\n        physical_mesh = mesh_utils.create_device_mesh(mesh_shape)\n    return Mesh(physical_mesh, dim_names)\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.get_metrics","title":"<code>get_metrics(metrics, unreplicate=False, stack=False)</code>","text":"<p>The get_metrics function is a helper function that takes the metrics dictionary returned by the training loop and converts it to a format that can be used for plotting. It does this in two ways:</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <p>Store the metrics that we want to track</p> required <code>unreplicate</code> <p>Convert the metrics from a replicated</p> <code>False</code> <code>stack</code> <p>Stack the metrics in a list</p> <code>False</code> <p>Returns:</p> Type Description <p>A dictionary of metrics</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def get_metrics(metrics, unreplicate=False, stack=False):\n    \"\"\"\n    The get_metrics function is a helper function that takes the metrics dictionary\n    returned by the training loop and converts it to a format that can be used for\n    plotting. It does this in two ways:\n\n    :param metrics: Store the metrics that we want to track\n    :param unreplicate: Convert the metrics from a replicated\n    :param stack: Stack the metrics in a list\n    :return: A dictionary of metrics\n\n    \"\"\"\n    if unreplicate:\n        metrics = flax.jax_utils.unreplicate(metrics)\n    metrics = jax.device_get(metrics)\n    if stack:\n        return jax.tree_map(lambda *args: np.stack(args), *metrics)\n    else:\n        return {key: float(val) for key, val in metrics.items()}\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.get_names_from_partition_spec","title":"<code>get_names_from_partition_spec(partition_specs)</code>","text":"<p>The get_names_from_partition_spec function takes a partition_specs argument, which is either a dictionary or list. If it's a dictionary, the function converts it to a list of values. Then for each item in the partition_specs list:     If the item is None, continue (do nothing) and move on to next iteration of loop.     If the item is an instance of str (i.e., if it's just one string), add that string to names set and move on to next iteration of loop.     Otherwise (if not None or str), call get_names_from_partition_spec recurs</p> <p>Parameters:</p> Name Type Description Default <code>partition_specs</code> <p>Specify the partitioning of the data</p> required <p>Returns:</p> Type Description <p>A list of names</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def get_names_from_partition_spec(partition_specs):\n    \"\"\"\n    The get_names_from_partition_spec function takes a partition_specs argument, which is either a dictionary or list.\n    If it's a dictionary, the function converts it to a list of values. Then for each item in the partition_specs list:\n        If the item is None, continue (do nothing) and move on to next iteration of loop.\n        If the item is an instance of str (i.e., if it's just one string), add that string to names set and move on to next iteration of loop.\n        Otherwise (if not None or str), call get_names_from_partition_spec recurs\n\n    :param partition_specs: Specify the partitioning of the data\n    :return: A list of names\n\n    \"\"\"\n    names = set()\n    if isinstance(partition_specs, dict):\n        partition_specs = partition_specs.values()\n    for item in partition_specs:\n        if item is None:\n            continue\n        elif isinstance(item, str):\n            names.add(item)\n        else:\n            names.update(get_names_from_partition_spec(item))\n\n    return list(names)\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.get_weight_decay_mask","title":"<code>get_weight_decay_mask(exclusions)</code>","text":"<p>Return a weight decay mask function that computes the pytree masks according to the given exclusion rules.</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def get_weight_decay_mask(exclusions):\n    \"\"\" Return a weight decay mask function that computes the pytree masks\n        according to the given exclusion rules.\n    \"\"\"\n\n    def decay(name, _):\n        for rule in exclusions:\n            if re.search(rule, name) is not None:\n                return False\n        return True\n\n    def weight_decay_mask(params):\n        return named_tree_map(decay, params, sep='/')\n\n    return weight_decay_mask\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.make_shard_and_gather_fns","title":"<code>make_shard_and_gather_fns(partition_specs, dtype_specs=None)</code>","text":"<p>The make_shard_and_gather_fns function takes in a partition_specs and dtype_specs, and returns two functions: shard_fns and gather_fns. The shard function is used to shard the input tensor into the specified partitions. The gather function is used to gather all the shards back together into one tensor.</p> <p>Parameters:</p> Name Type Description Default <code>partition_specs</code> <p>Specify the sharding of the input tensor</p> required <code>dtype_specs</code> <p>Specify the dtype of the tensor</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple of functions</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def make_shard_and_gather_fns(partition_specs, dtype_specs=None):\n    \"\"\"\n    The make_shard_and_gather_fns function takes in a partition_specs and dtype_specs,\n    and returns two functions: shard_fns and gather_fns. The shard function is used to\n    shard the input tensor into the specified partitions. The gather function is used to\n    gather all the shards back together into one tensor.\n\n    :param partition_specs: Specify the sharding of the input tensor\n    :param dtype_specs: Specify the dtype of the tensor\n    :return: A tuple of functions\n\n    \"\"\"\n    float_dtypes = (jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64)\n\n    def make_to_dtype_fn(dtype_spec):\n        def to_dtype(tensor):\n            if dtype_specs in float_dtypes and getattr(tensor, 'dtype', None) in float_dtypes:\n                # force np array to jax numpy array\n                return jnp.asarray(tensor).astype(dtype_specs)\n            elif hasattr(dtype_spec, 'dtype') and hasattr(tensor, 'dtype'):\n                return jnp.asarray(tensor).astype(dtype_spec.dtype)\n            return jnp.asarray(tensor)\n\n        return to_dtype\n\n    def make_shard_fn(partition_spec, dtype_spec=None):\n        jax_shard_function = pjit(\n            make_to_dtype_fn(dtype_spec),\n            in_shardings=None,\n            out_shardings=partition_spec\n        )\n\n        def shard_fn(tensor):\n            return jax_shard_function(tensor).block_until_ready()\n\n        return shard_fn\n\n    def make_gather_fn(partition_spec, dtype_spec=None):\n        jax_gather_fn = pjit(\n            make_to_dtype_fn(dtype_spec),\n            in_shardings=partition_spec,\n            out_shardings=None\n        )\n\n        def gather_fn(tensor):\n            return jax.device_get(jax_gather_fn(tensor))\n\n        return gather_fn\n\n    if dtype_specs is None or dtype_specs in float_dtypes:\n        shard_fns = jax.tree_util.tree_map(make_shard_fn, partition_specs)\n        gather_fns = jax.tree_util.tree_map(make_gather_fn, partition_specs)\n    else:\n        shard_fns = jax.tree_util.tree_map(\n            make_shard_fn, partition_specs, dtype_specs\n        )\n        gather_fns = jax.tree_util.tree_map(\n            make_gather_fn, partition_specs, dtype_specs\n        )\n    return shard_fns, gather_fns\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.match_partition_rules","title":"<code>match_partition_rules(rules, params)</code>","text":"<p>Returns a pytree of PartitionSpec according to rules. Supports handling Flax TrainState and Optax optimizer state.</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def match_partition_rules(rules, params):\n    \"\"\" Returns a pytree of PartitionSpec according to rules. Supports handling\n        Flax TrainState and Optax optimizer state.\n    \"\"\"\n\n    def get_partition_spec(name, leaf):\n        if len(leaf.shape) == 0 or np.prod(leaf.shape) == 1:\n            \"\"\" Don't partition scalar values. \"\"\"\n            return PS()\n        for rule, ps in rules:\n            if re.search(rule, name) is not None:\n                return ps\n        raise ValueError(f'Partition rule not found for param: {name}')\n\n    return named_tree_map(get_partition_spec, params, sep='/')\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.named_tree_map","title":"<code>named_tree_map(f, tree, *rest, is_leaf=None, sep=None)</code>","text":"<p>An extended version of jax.tree_util.tree_map, where the mapped function f takes both the name (path) and the tree leaf as input.</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def named_tree_map(f, tree, *rest, is_leaf=None, sep=None):\n    \"\"\" An extended version of jax.tree_util.tree_map, where the mapped function\n        f takes both the name (path) and the tree leaf as input.\n    \"\"\"\n    return jax.tree_util.tree_map_with_path(\n        lambda path, x, *r: f(tree_path_to_string(path, sep=sep), x, *r),\n        tree, *rest,\n        is_leaf=is_leaf\n    )\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.names_in_current_mesh","title":"<code>names_in_current_mesh(*names)</code>","text":"<p>The names_in_current_mesh function is used to check if a set of names are in the current mesh.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <p>Pass in a list of names to the function</p> <code>()</code> <p>Returns:</p> Type Description <p>A boolean indicating whether</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def names_in_current_mesh(*names):\n    \"\"\"\n    The names_in_current_mesh function is used to check if a set of names are in the current mesh.\n\n    :param *names: Pass in a list of names to the function\n    :return: A boolean indicating whether\n\n    \"\"\"\n    mesh_axis_names = pxla.thread_resources.env.physical_mesh.axis_names\n    return set(names) &lt;= set(mesh_axis_names)\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.tree_apply","title":"<code>tree_apply(fns, tree)</code>","text":"<p>The tree_apply function is a generalization of the map function. It takes two arguments: a pytree of functions and a pytree of values. The tree_apply function applies each function in the first argument to its corresponding value in the second argument, and returns a new pytree with these results.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <p>Apply the functions to the tree</p> required <code>tree</code> <p>Apply the function to each element in the tree</p> required <p>Returns:</p> Type Description <p>A pytree of the same structure as the input</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def tree_apply(fns, tree):\n    \"\"\"\n    The tree_apply function is a generalization of the map function.\n    It takes two arguments: a pytree of functions and a pytree of values.\n    The tree_apply function applies each function in the first argument to its corresponding value in the second argument,\n    and returns a new pytree with these results.\n\n    :param fns: Apply the functions to the tree\n    :param tree: Apply the function to each element in the tree\n    :return: A pytree of the same structure as the input\n    \"\"\"\n    return jax.tree_util.tree_map(lambda fn, x: fn(x), fns, tree)\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.tree_path_to_string","title":"<code>tree_path_to_string(path, sep=None)</code>","text":"<p>The tree_path_to_string function takes a tree path and returns a string representation of it.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Specify the path of the tree</p> required <code>sep</code> <p>Join the keys with a separator</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple of strings</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def tree_path_to_string(path, sep=None):\n    \"\"\"\n    The tree_path_to_string function takes a tree path and returns a string representation of it.\n\n    :param path: Specify the path of the tree\n    :param sep: Join the keys with a separator\n    :return: A tuple of strings\n\n    \"\"\"\n    keys = []\n    for key in path:\n        if isinstance(key, jax.tree_util.SequenceKey):\n            keys.append(str(key.idx))\n        elif isinstance(key, jax.tree_util.DictKey):\n            keys.append(str(key.key))\n        elif isinstance(key, jax.tree_util.GetAttrKey):\n            keys.append(str(key.name))\n        elif isinstance(key, jax.tree_util.FlattenedIndexKey):\n            keys.append(str(key.key))\n        else:\n            keys.append(str(key))\n    if sep is None:\n        return tuple(keys)\n    return sep.join(keys)\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.with_sharding_constraint","title":"<code>with_sharding_constraint(x, partition_specs)</code>","text":"<p>A smarter version of with_sharding_constraint that only applies the constraint if the current mesh contains the axes in the partition specs.</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def with_sharding_constraint(x, partition_specs):\n    \"\"\" A smarter version of with_sharding_constraint that only applies the\n        constraint if the current mesh contains the axes in the partition specs.\n    \"\"\"\n    axis_names = get_names_from_partition_spec(partition_specs)\n    if names_in_current_mesh(*axis_names):\n        x = _with_sharding_constraint(x, partition_specs)\n    return x\n</code></pre>"},{"location":"generated-partition_utils-mesh_utils/#src.fjformer.partition_utils.mesh_utils.wrap_function_with_rng","title":"<code>wrap_function_with_rng(rng)</code>","text":"<p>To be used as decorator, automatically bookkeep a RNG for the wrapped function.</p> Source code in <code>src/fjformer/partition_utils/mesh_utils.py</code> <pre><code>def wrap_function_with_rng(rng):\n    \"\"\" To be used as decorator, automatically bookkeep a RNG for the wrapped function. \"\"\"\n\n    def wrap_function(function):\n        def wrapped(*args, **kwargs):\n            nonlocal rng\n            rng, split_rng = jax.random.split(rng)\n            return function(split_rng, *args, **kwargs)\n\n        return wrapped\n\n    return wrap_function\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/","title":"partition_utils.t5x_partitioning","text":"<p>Utilities for partitioning.</p>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.AxisNames","title":"<code>AxisNames</code>","text":"<p>             Bases: <code>tuple</code></p> <p>Tuple of strings specifying name for each axis.</p> <p>We create a separate class for this so JAX's pytree utilities can distinguish it from a tuple that should be treated as a pytree, instead treating it as a leaf.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>class AxisNames(tuple):\n    \"\"\"Tuple of strings specifying name for each axis.\n\n    We create a separate class for this so JAX's pytree utilities can distinguish\n    it from a tuple that should be treated as a pytree, instead treating it as a\n    leaf.\n    \"\"\"\n\n    def __new__(cls, *names):\n        return tuple.__new__(AxisNames, names)\n\n    def __repr__(self):\n        return 'AxisNames%s' % tuple.__repr__(self)\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner","title":"<code>BasePartitioner</code>","text":"<p>Interface for partitioning computations across hardware devices.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>class BasePartitioner(metaclass=abc.ABCMeta):\n    \"\"\"Interface for partitioning computations across hardware devices.\"\"\"\n\n    def __init__(\n            self,\n            num_partitions: Optional[int] = None,\n            model_parallel_submesh: Optional[HardwareMesh] = None,\n            params_on_devices: bool = True,\n            backend: Optional[str] = None,\n            ici_mesh_shape: Optional[HardwareMesh] = None,\n            dcn_mesh_shape: Optional[HardwareMesh] = None,\n    ):\n        \"\"\"Configures the partitioner.\n\n        Args:\n          num_partitions: the number of partitions to use. Ignored if\n            `model_parallel_submesh` is provided.\n          model_parallel_submesh: 4-tuple that specifies the x,y,z,c submesh to use\n            as the model-parallel device tile. This submesh is used for the larger\n            of the two parameter dimensions, and, if 2-D activation sharding is\n            enabled, for the model dimension of activations. The rest of the mesh is\n            used for data parallelism and, if 2-D parameter sharding is enabled, the\n            other parameter dimension.\n          params_on_devices: whether to keep the params on devices, if False -\n            params stay in the host memory. Note that some partitioners might ignore\n            this setting, for example if they don't support storing all params on\n            device memory.\n          backend: get devices from the pinned backend, if specified. This is useful\n            for explicitly specifying the devices other than relying on\n            jax_platform_name.\n          ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in\n            each slice. The meaning of each mesh axis is defined by mesh_axis_names,\n            so these two params must be the same length. If dcn_mesh_shape is\n            present, the overall mesh is the product of ici_mesh_shape and\n            dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with\n            mesh_axis_names ['replica', 'data', 'mdl'] indicates 2-way replica\n            parallelism, 3-way data parallelism, and 4-way model parallelism over 24\n            devices. None, the default, is equivalent to a sequence of ones and\n            means that the model is placed on a single device.\n          dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over\n            multiple slices. The overall mesh is the product of ici_mesh_shape and\n            dcn_mesh_shape, and the meaning of each mesh axis is defined by\n            mesh_axis_names, so these three params must be the same length.\n        \"\"\"\n\n        if not num_partitions and not model_parallel_submesh:\n            raise ValueError('At least one of `num_partitions` or '\n                             '`model_parallel_submesh` must be set.')\n\n        if model_parallel_submesh is not None and len(model_parallel_submesh) != 4:\n            logging.error(\n                (\n                    '`model_parallel_submesh` must be either None or a 4-tuple. Got'\n                    ' `model_parallel_submesh`=%r. A ValueError will be raised'\n                    ' beginning March 1, 2022.'\n                ),\n                model_parallel_submesh,\n            )\n\n        if bool(num_partitions) and bool(model_parallel_submesh):\n            logging.error(\n                'At most one of `num_partitions` or `model_parallel_submesh` can be '\n                'set. Got `num_partitions=%r` and `model_parallel_submesh`=%r. A '\n                'ValueError will be raised beginning March 21, 2022.',\n                num_partitions,\n                model_parallel_submesh,\n            )\n\n        self._num_partitions = num_partitions\n        self._model_parallel_submesh = model_parallel_submesh\n        self._params_on_devices = params_on_devices\n        if ici_mesh_shape is None or dcn_mesh_shape is None:\n            self._data_axis = 'data'\n        else:\n            self._data_axis = ('replica', 'data')\n        self._backend = backend\n        self._ici_mesh_shape = ici_mesh_shape\n        self._dcn_mesh_shape = dcn_mesh_shape\n\n    @property\n    def mesh(self) -&gt; Mesh:\n        raise NotImplementedError\n\n    @property\n    def data_partition_spec(self) -&gt; PartitionSpec:\n        return PartitionSpec(self._data_axis)\n\n    @property\n    def data_mesh_size(self) -&gt; int:\n        \"\"\"Data mesh size.\n\n        Data mesh size is defined as the number of global devices involved to\n        carry out data parallel. Let's say we have a global mesh: ('replica': 2,\n        'data': 4, 'model': 2), and axes 'replica' and 'data' are responsible for\n        the data parallel, that means we have 2*4 = 8 devices involved - i.e., data\n        mesh size is 8.\n\n        Returns:\n          the id of the shard for the axes being replicated among the devices used\n          to shard the sharded_mesh_axes.\n        \"\"\"\n        data_submesh_sizes = (\n            [self.mesh.shape[self._data_axis]]\n            if isinstance(self._data_axis, str)\n            else [self.mesh.shape[axis] for axis in self._data_axis]\n        )\n        data_mesh_size = functools.reduce(lambda x, y: x * y, data_submesh_sizes)\n        return data_mesh_size\n\n    @property\n    def data_shards(self) -&gt; int:\n        \"\"\"Number of data shards.\n\n        Let's say we are dealing with 2 slices of df4x2 TPUs. In data pipeline\n        we need prepare / send one data shard to each local host. This means, we\n        need 4 shards since we have 4 local hosts. How to infer the number of hosts\n        from mesh information? In this case, we have a global mesh: ('replica': 2,\n        'data': 8, 'model': 2). Each local host (i.e., df2x2) has this local mesh:\n        ('replica': 1, 'data': 4, 'model': 2). By dividing global mesh with local\n        mesh, we can get the count of hosts.\n\n        Returns:\n          Number of data shards. Each shard will be sent to one local host.\n        \"\"\"\n        data_chunks = (\n            [self._local_chunker.num_chunks[self._data_axis]]\n            if isinstance(self._data_axis, str)\n            else [self._local_chunker.num_chunks[axis] for axis in self._data_axis]\n        )\n        data_shards = functools.reduce(lambda x, y: x * y, data_chunks)\n        return data_shards\n\n    @property\n    def data_shard_id(self) -&gt; int:\n        \"\"\"Data shard id for the current host.\n\n        Returns:\n          Index of data shard that will be sent to the current local host.\n        \"\"\"\n        return self._local_chunker.get_shard_id(self._data_axis)\n\n    def get_data_layout(\n            self, batch_size: Optional[int] = None, host_index: Optional[int] = None\n    ) -&gt; DataLayout:\n        \"\"\"Returns filled `DataLayout` based on the partitioned model layout.\n\n        Args:\n          batch_size: if set, indicates the requested batch size. The exception will\n            be raised if this batch size is not compatible with the layout. If not\n            set, the batch size is inferred from the layout.\n          host_index: indicates the host index to use for the calculations, if not\n            set - use JAX-provided one. Should be in [0, num_hosts) interval and the\n            order should match the order of corresponding CPU devices in\n            `jax.devices()`.\n\n        Returns:\n          Filled `DataLayout` structure.\n        \"\"\"\n        if host_index is not None:\n            raise NotImplementedError('Explicit host_index is not yet implemented.')\n        if self._data_axis is None:\n            return DataLayout(\n                batch_size=batch_size,\n                shard_id=0,\n                num_shards=1,\n                is_first_host_in_replica_set=(jax.process_index() == 0))\n\n        batch_size = batch_size or self.data_mesh_size\n        if batch_size % self.data_mesh_size:\n            raise ValueError(\n                f'Batch size ({batch_size}) must be divisible by corresponding '\n                f'data mesh size ({self.data_mesh_size}).'\n            )\n\n        if batch_size % self.data_shards:\n            raise ValueError(\n                f'Batch size ({batch_size}) must be divisible by number of '\n                f'data shards ({self.data_shards}).'\n            )\n        replica_id = self._local_chunker.get_replica_id(self._data_axis)\n        return DataLayout(\n            batch_size=int(batch_size),\n            shard_id=int(self.data_shard_id),\n            num_shards=int(self.data_shards),\n            is_first_host_in_replica_set=(replica_id == 0),\n        )\n\n    def get_local_chunk_info(\n            self, global_shape: Tuple[int, ...],\n            mesh_axes: Sequence[Optional[str]]) -&gt; LocalChunkInfo:\n        \"\"\"Returns the local chunk info for a given array shape and sharded axes.\"\"\"\n        return self._local_chunker.get_local_chunk_info(global_shape, mesh_axes)\n\n    @property\n    def params_on_devices(self):\n        return self._params_on_devices\n\n    @params_on_devices.setter\n    def params_on_devices(self, value):\n        self._params_on_devices = value\n\n    def move_params_to_devices(self, train_state,\n                               train_state_axes):\n        \"\"\"Moves the optimizer parameters to devices.\"\"\"\n        p_id_fn = self.partition(\n            _id_fn,\n            in_axis_resources=(train_state_axes, None),\n            out_axis_resources=(train_state_axes, None),\n            donate_argnums=(0,))\n        if jax.process_count() &gt; 1:\n            train_state = host_local_array_to_global_array(\n                train_state, self.mesh, train_state_axes\n            )\n        train_state, _ = p_id_fn(train_state, jnp.ones((), dtype=jnp.uint32))\n        return train_state\n\n    @property\n    @abc.abstractmethod\n    def _local_chunker(self):\n        \"\"\"Returns the chunker that matches the parameters of this partitioner.\"\"\"\n        raise NotImplementedError\n\n    def get_logical_axes(self, train_state):\n        \"\"\"Returns a copy of TrainState with Optional[AxisNames] as leaves.\"\"\"\n        # By default, return None for the logical axes.\n        return train_state.restore_state(\n            jax.tree_map(lambda x: None, train_state.state_dict()))\n\n    def get_mesh_axes(self, train_state):\n        \"\"\"Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.\"\"\"\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def partition(\n            self,\n            fn: Callable,  # pylint: disable=g-bare-generic\n            in_axis_resources,\n            out_axis_resources,\n            static_argnums: Union[int, Sequence[int]] = (),\n            donate_argnums: Union[int, Sequence[int]] = ()\n    ) -&gt; PartitionedCallable:\n        raise NotImplementedError\n\n    @abc.abstractmethod\n    def compile(self, partitioned_fn: PartitionedCallable,\n                *args) -&gt; CompiledPartitionedCallable:\n\n        raise NotImplementedError\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_mesh_size","title":"<code>data_mesh_size: int</code>  <code>property</code>","text":"<p>Data mesh size.</p> <p>Data mesh size is defined as the number of global devices involved to carry out data parallel. Let's say we have a global mesh: ('replica': 2, 'data': 4, 'model': 2), and axes 'replica' and 'data' are responsible for the data parallel, that means we have 2*4 = 8 devices involved - i.e., data mesh size is 8.</p> <p>Returns:   the id of the shard for the axes being replicated among the devices used   to shard the sharded_mesh_axes.</p>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_shard_id","title":"<code>data_shard_id: int</code>  <code>property</code>","text":"<p>Data shard id for the current host.</p> <p>Returns:   Index of data shard that will be sent to the current local host.</p>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.data_shards","title":"<code>data_shards: int</code>  <code>property</code>","text":"<p>Number of data shards.</p> <p>Let's say we are dealing with 2 slices of df4x2 TPUs. In data pipeline we need prepare / send one data shard to each local host. This means, we need 4 shards since we have 4 local hosts. How to infer the number of hosts from mesh information? In this case, we have a global mesh: ('replica': 2, 'data': 8, 'model': 2). Each local host (i.e., df2x2) has this local mesh: ('replica': 1, 'data': 4, 'model': 2). By dividing global mesh with local mesh, we can get the count of hosts.</p> <p>Returns:   Number of data shards. Each shard will be sent to one local host.</p>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.__init__","title":"<code>__init__(num_partitions=None, model_parallel_submesh=None, params_on_devices=True, backend=None, ici_mesh_shape=None, dcn_mesh_shape=None)</code>","text":"<p>Configures the partitioner.</p> <p>Args:   num_partitions: the number of partitions to use. Ignored if     <code>model_parallel_submesh</code> is provided.   model_parallel_submesh: 4-tuple that specifies the x,y,z,c submesh to use     as the model-parallel device tile. This submesh is used for the larger     of the two parameter dimensions, and, if 2-D activation sharding is     enabled, for the model dimension of activations. The rest of the mesh is     used for data parallelism and, if 2-D parameter sharding is enabled, the     other parameter dimension.   params_on_devices: whether to keep the params on devices, if False -     params stay in the host memory. Note that some partitioners might ignore     this setting, for example if they don't support storing all params on     device memory.   backend: get devices from the pinned backend, if specified. This is useful     for explicitly specifying the devices other than relying on     jax_platform_name.   ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in     each slice. The meaning of each mesh axis is defined by mesh_axis_names,     so these two params must be the same length. If dcn_mesh_shape is     present, the overall mesh is the product of ici_mesh_shape and     dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with     mesh_axis_names ['replica', 'data', 'mdl'] indicates 2-way replica     parallelism, 3-way data parallelism, and 4-way model parallelism over 24     devices. None, the default, is equivalent to a sequence of ones and     means that the model is placed on a single device.   dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over     multiple slices. The overall mesh is the product of ici_mesh_shape and     dcn_mesh_shape, and the meaning of each mesh axis is defined by     mesh_axis_names, so these three params must be the same length.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def __init__(\n        self,\n        num_partitions: Optional[int] = None,\n        model_parallel_submesh: Optional[HardwareMesh] = None,\n        params_on_devices: bool = True,\n        backend: Optional[str] = None,\n        ici_mesh_shape: Optional[HardwareMesh] = None,\n        dcn_mesh_shape: Optional[HardwareMesh] = None,\n):\n    \"\"\"Configures the partitioner.\n\n    Args:\n      num_partitions: the number of partitions to use. Ignored if\n        `model_parallel_submesh` is provided.\n      model_parallel_submesh: 4-tuple that specifies the x,y,z,c submesh to use\n        as the model-parallel device tile. This submesh is used for the larger\n        of the two parameter dimensions, and, if 2-D activation sharding is\n        enabled, for the model dimension of activations. The rest of the mesh is\n        used for data parallelism and, if 2-D parameter sharding is enabled, the\n        other parameter dimension.\n      params_on_devices: whether to keep the params on devices, if False -\n        params stay in the host memory. Note that some partitioners might ignore\n        this setting, for example if they don't support storing all params on\n        device memory.\n      backend: get devices from the pinned backend, if specified. This is useful\n        for explicitly specifying the devices other than relying on\n        jax_platform_name.\n      ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in\n        each slice. The meaning of each mesh axis is defined by mesh_axis_names,\n        so these two params must be the same length. If dcn_mesh_shape is\n        present, the overall mesh is the product of ici_mesh_shape and\n        dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with\n        mesh_axis_names ['replica', 'data', 'mdl'] indicates 2-way replica\n        parallelism, 3-way data parallelism, and 4-way model parallelism over 24\n        devices. None, the default, is equivalent to a sequence of ones and\n        means that the model is placed on a single device.\n      dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over\n        multiple slices. The overall mesh is the product of ici_mesh_shape and\n        dcn_mesh_shape, and the meaning of each mesh axis is defined by\n        mesh_axis_names, so these three params must be the same length.\n    \"\"\"\n\n    if not num_partitions and not model_parallel_submesh:\n        raise ValueError('At least one of `num_partitions` or '\n                         '`model_parallel_submesh` must be set.')\n\n    if model_parallel_submesh is not None and len(model_parallel_submesh) != 4:\n        logging.error(\n            (\n                '`model_parallel_submesh` must be either None or a 4-tuple. Got'\n                ' `model_parallel_submesh`=%r. A ValueError will be raised'\n                ' beginning March 1, 2022.'\n            ),\n            model_parallel_submesh,\n        )\n\n    if bool(num_partitions) and bool(model_parallel_submesh):\n        logging.error(\n            'At most one of `num_partitions` or `model_parallel_submesh` can be '\n            'set. Got `num_partitions=%r` and `model_parallel_submesh`=%r. A '\n            'ValueError will be raised beginning March 21, 2022.',\n            num_partitions,\n            model_parallel_submesh,\n        )\n\n    self._num_partitions = num_partitions\n    self._model_parallel_submesh = model_parallel_submesh\n    self._params_on_devices = params_on_devices\n    if ici_mesh_shape is None or dcn_mesh_shape is None:\n        self._data_axis = 'data'\n    else:\n        self._data_axis = ('replica', 'data')\n    self._backend = backend\n    self._ici_mesh_shape = ici_mesh_shape\n    self._dcn_mesh_shape = dcn_mesh_shape\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_data_layout","title":"<code>get_data_layout(batch_size=None, host_index=None)</code>","text":"<p>Returns filled <code>DataLayout</code> based on the partitioned model layout.</p> <p>Args:   batch_size: if set, indicates the requested batch size. The exception will     be raised if this batch size is not compatible with the layout. If not     set, the batch size is inferred from the layout.   host_index: indicates the host index to use for the calculations, if not     set - use JAX-provided one. Should be in [0, num_hosts) interval and the     order should match the order of corresponding CPU devices in     <code>jax.devices()</code>.</p> <p>Returns:   Filled <code>DataLayout</code> structure.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_data_layout(\n        self, batch_size: Optional[int] = None, host_index: Optional[int] = None\n) -&gt; DataLayout:\n    \"\"\"Returns filled `DataLayout` based on the partitioned model layout.\n\n    Args:\n      batch_size: if set, indicates the requested batch size. The exception will\n        be raised if this batch size is not compatible with the layout. If not\n        set, the batch size is inferred from the layout.\n      host_index: indicates the host index to use for the calculations, if not\n        set - use JAX-provided one. Should be in [0, num_hosts) interval and the\n        order should match the order of corresponding CPU devices in\n        `jax.devices()`.\n\n    Returns:\n      Filled `DataLayout` structure.\n    \"\"\"\n    if host_index is not None:\n        raise NotImplementedError('Explicit host_index is not yet implemented.')\n    if self._data_axis is None:\n        return DataLayout(\n            batch_size=batch_size,\n            shard_id=0,\n            num_shards=1,\n            is_first_host_in_replica_set=(jax.process_index() == 0))\n\n    batch_size = batch_size or self.data_mesh_size\n    if batch_size % self.data_mesh_size:\n        raise ValueError(\n            f'Batch size ({batch_size}) must be divisible by corresponding '\n            f'data mesh size ({self.data_mesh_size}).'\n        )\n\n    if batch_size % self.data_shards:\n        raise ValueError(\n            f'Batch size ({batch_size}) must be divisible by number of '\n            f'data shards ({self.data_shards}).'\n        )\n    replica_id = self._local_chunker.get_replica_id(self._data_axis)\n    return DataLayout(\n        batch_size=int(batch_size),\n        shard_id=int(self.data_shard_id),\n        num_shards=int(self.data_shards),\n        is_first_host_in_replica_set=(replica_id == 0),\n    )\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_local_chunk_info","title":"<code>get_local_chunk_info(global_shape, mesh_axes)</code>","text":"<p>Returns the local chunk info for a given array shape and sharded axes.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_local_chunk_info(\n        self, global_shape: Tuple[int, ...],\n        mesh_axes: Sequence[Optional[str]]) -&gt; LocalChunkInfo:\n    \"\"\"Returns the local chunk info for a given array shape and sharded axes.\"\"\"\n    return self._local_chunker.get_local_chunk_info(global_shape, mesh_axes)\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_logical_axes","title":"<code>get_logical_axes(train_state)</code>","text":"<p>Returns a copy of TrainState with Optional[AxisNames] as leaves.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_logical_axes(self, train_state):\n    \"\"\"Returns a copy of TrainState with Optional[AxisNames] as leaves.\"\"\"\n    # By default, return None for the logical axes.\n    return train_state.restore_state(\n        jax.tree_map(lambda x: None, train_state.state_dict()))\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.get_mesh_axes","title":"<code>get_mesh_axes(train_state)</code>","text":"<p>Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_mesh_axes(self, train_state):\n    \"\"\"Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.BasePartitioner.move_params_to_devices","title":"<code>move_params_to_devices(train_state, train_state_axes)</code>","text":"<p>Moves the optimizer parameters to devices.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def move_params_to_devices(self, train_state,\n                           train_state_axes):\n    \"\"\"Moves the optimizer parameters to devices.\"\"\"\n    p_id_fn = self.partition(\n        _id_fn,\n        in_axis_resources=(train_state_axes, None),\n        out_axis_resources=(train_state_axes, None),\n        donate_argnums=(0,))\n    if jax.process_count() &gt; 1:\n        train_state = host_local_array_to_global_array(\n            train_state, self.mesh, train_state_axes\n        )\n    train_state, _ = p_id_fn(train_state, jnp.ones((), dtype=jnp.uint32))\n    return train_state\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.BasePjitPartitioner","title":"<code>BasePjitPartitioner</code>","text":"<p>             Bases: <code>BasePartitioner</code></p> <p>Partitioner that uses T5X version of jax.pjit.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>class BasePjitPartitioner(BasePartitioner):\n    \"\"\"Partitioner that uses T5X version of jax.pjit.\"\"\"\n\n    @cached_property\n    def _local_chunker(self) -&gt; LocalChunker:\n        return LocalChunker(self.mesh)\n\n    @cached_property\n    def mesh(self) -&gt; Mesh:\n        return default_mesh(\n            self._num_partitions,\n            self._model_parallel_submesh,\n            self._backend,\n            self._ici_mesh_shape,\n            self._dcn_mesh_shape,\n        )\n\n    def partition(\n            self,\n            fn: Callable,  # pylint: disable=g-bare-generic\n            in_axis_resources,\n            out_axis_resources,\n            static_argnums: Union[int, Sequence[int]] = (),\n            donate_argnums: Union[int, Sequence[int]] = (),\n    ) -&gt; PjittedFnWithContext:\n        pjitted = pjit(\n            fn,\n            in_shardings=in_axis_resources,\n            out_shardings=out_axis_resources,\n            static_argnums=static_argnums,\n            donate_argnums=donate_argnums,\n        )\n\n        return PjittedFnWithContext(pjitted, self.mesh)\n\n    def compile(self, partitioned_fn: PjittedFnWithContext,\n                *args) -&gt; CompiledPartitionedCallable:\n        return partitioned_fn.lower(*args).compile()\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.DataLayout","title":"<code>DataLayout</code>  <code>dataclass</code>","text":"<p>Represents data layout for the partitioned model.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>@dataclasses.dataclass\nclass DataLayout:\n    \"\"\"Represents data layout for the partitioned model.\"\"\"\n    batch_size: int\n    shard_id: int\n    num_shards: int\n    is_first_host_in_replica_set: bool\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.LocalChunker","title":"<code>LocalChunker</code>","text":"<p>Utility class to aid chunking of sharded arrays in multihost settings.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>class LocalChunker:\n    \"\"\"Utility class to aid chunking of sharded arrays in multihost settings.\"\"\"\n\n    def __init__(self, global_mesh: Mesh):\n        self.global_mesh = global_mesh\n        local_mesh = global_mesh.local_mesh\n        first_local_device = local_mesh.devices.reshape(-1)[0]\n        host_location = collections.OrderedDict(\n            zip(\n                global_mesh.shape.keys(),\n                list(zip(*np.nonzero(\n                    global_mesh.devices == first_local_device)))[0]))\n        self.num_chunks = collections.OrderedDict()\n        self.chunk_ids = collections.OrderedDict()\n        self.mesh_axes = list(global_mesh.shape.keys())\n        for mesh_axis in self.mesh_axes:\n            num_devices_per_chunk = local_mesh.shape[mesh_axis]\n            self.num_chunks[mesh_axis] = (\n                    global_mesh.shape[mesh_axis] // num_devices_per_chunk)\n            self.chunk_ids[mesh_axis] = (\n                    host_location[mesh_axis] // num_devices_per_chunk)\n\n    def get_local_chunk_info(\n            self, global_shape: Tuple[int, ...],\n            mesh_axes: Sequence[Optional[str]]) -&gt; LocalChunkInfo:\n        \"\"\"Get the local chunk info for a given array shape and sharded axes.\n\n        Args:\n          global_shape: the global, unsharded shape of the array to chunk.\n          mesh_axes: a sequence of names (or None) of equal rank to `global_shape`\n            that specifies which mesh dimensions the array is sharded along.\n\n        Returns:\n          LocalChunkInfo containing the logical slices of the array found on this\n          host's local devices, as well as the replica index for this chunk among\n          chunks with the same slice. The latter is used to determine which\n          host should write this chunk during checkpointing.\n        \"\"\"\n        local_slice = [slice(None) for dim in global_shape]\n        sharded_mesh_axes = set()\n        for i, (mesh_axis, size) in enumerate(zip(mesh_axes, global_shape)):\n            if not mesh_axis:\n                continue\n            sharded_mesh_axes.add(mesh_axis)\n            if not isinstance(mesh_axis, str):\n                raise NotImplementedError('TODO(jekbradbury)')\n            chunk_id = self.chunk_ids[mesh_axis]\n            chunk_size = size // self.num_chunks[mesh_axis]\n            local_slice[i] = slice(chunk_id * chunk_size, (chunk_id + 1) * chunk_size)\n\n        replica_id = self.get_replica_id(sharded_mesh_axes)\n\n        return LocalChunkInfo(tuple(local_slice), replica_id)\n\n    def get_shard_id(self, sharded_mesh_axes: Union[str, Set[Optional[str]]]) -&gt; int:\n        \"\"\"Given mesh axes used for sharding, computes current host's shard id.\n\n        To give an example, let's say there are two axes globally: replica, data,\n        and model, the mesh axes for sharding is ('replica', 'data'), which means we\n        are going to partition an array along 'replica' and 'data' axes.\n        The shard_id is to show the index of the current local host along the\n        sharding axes (in this example, it's 'replica' and 'data' axes).\n\n        More concretely, let's say we have 4 local hosts, and we use 'replica' and\n        'data' axes for data parallel (2 hosts along the replica axis, and 2 host\n        along the data axis). The host located in ('replica': 0, 'data': 0), we\n        should assign data shard-0 to it. For host ('replica': 0, 'data': 1), we\n        assign shard-1. For host ('replica': 1, 'data': 0), we assign shard-2.\n        For host ('replica': 1, 'data': 1), we assign shard-3.\n\n        Note: the host location along 'replica' and 'data' axes, e.g.,\n        ('replica': 0, 'data': 0) is named chunk_id and stored in\n        self._local_chunker.chunk_ids[axis].\n\n        Args:\n          sharded_mesh_axes: the mesh axes for sharding.\n\n        Returns:\n          the index of the current local host along the sharding axes.\n        \"\"\"\n        if isinstance(sharded_mesh_axes, str):\n            sharded_mesh_axes = (sharded_mesh_axes,)\n\n        shard_id = 0\n        for mesh_axis in sharded_mesh_axes:\n            chunk_id = self.chunk_ids[mesh_axis]\n            shard_id = shard_id * self.num_chunks[mesh_axis] + chunk_id\n\n        return shard_id\n\n    def get_replica_id(self, sharded_mesh_axes: Union[str, Set[Optional[str]]]) -&gt; int:\n        \"\"\"Given mesh axes used for sharding, computes current host's replica id.\n\n        To give an example, let's say there are two axes globally: data, and model,\n        the mesh axes for sharding is ('data', ), which means we are going to\n        partition an array along 'data' axis and replicate it along 'model' axis.\n        The replica_id is to show the index of the current local host along the\n        'model' axis.\n\n        Args:\n          sharded_mesh_axes: the mesh axes for sharding.\n\n        Returns:\n          the index of the current local host along the non-sharding axes (i.e.,\n          replicating axes).\n        \"\"\"\n        if isinstance(sharded_mesh_axes, str):\n            sharded_mesh_axes = (sharded_mesh_axes,)\n\n        replicated_mesh_axes = [\n            mesh_axis for mesh_axis in self.mesh_axes\n            if mesh_axis not in sharded_mesh_axes\n        ]\n        replica_id = 0\n        for mesh_axis in replicated_mesh_axes:\n            chunk_id = self.chunk_ids[mesh_axis]\n            replica_id = replica_id * self.num_chunks[mesh_axis] + chunk_id\n\n        return replica_id\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_local_chunk_info","title":"<code>get_local_chunk_info(global_shape, mesh_axes)</code>","text":"<p>Get the local chunk info for a given array shape and sharded axes.</p> <p>Args:   global_shape: the global, unsharded shape of the array to chunk.   mesh_axes: a sequence of names (or None) of equal rank to <code>global_shape</code>     that specifies which mesh dimensions the array is sharded along.</p> <p>Returns:   LocalChunkInfo containing the logical slices of the array found on this   host's local devices, as well as the replica index for this chunk among   chunks with the same slice. The latter is used to determine which   host should write this chunk during checkpointing.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_local_chunk_info(\n        self, global_shape: Tuple[int, ...],\n        mesh_axes: Sequence[Optional[str]]) -&gt; LocalChunkInfo:\n    \"\"\"Get the local chunk info for a given array shape and sharded axes.\n\n    Args:\n      global_shape: the global, unsharded shape of the array to chunk.\n      mesh_axes: a sequence of names (or None) of equal rank to `global_shape`\n        that specifies which mesh dimensions the array is sharded along.\n\n    Returns:\n      LocalChunkInfo containing the logical slices of the array found on this\n      host's local devices, as well as the replica index for this chunk among\n      chunks with the same slice. The latter is used to determine which\n      host should write this chunk during checkpointing.\n    \"\"\"\n    local_slice = [slice(None) for dim in global_shape]\n    sharded_mesh_axes = set()\n    for i, (mesh_axis, size) in enumerate(zip(mesh_axes, global_shape)):\n        if not mesh_axis:\n            continue\n        sharded_mesh_axes.add(mesh_axis)\n        if not isinstance(mesh_axis, str):\n            raise NotImplementedError('TODO(jekbradbury)')\n        chunk_id = self.chunk_ids[mesh_axis]\n        chunk_size = size // self.num_chunks[mesh_axis]\n        local_slice[i] = slice(chunk_id * chunk_size, (chunk_id + 1) * chunk_size)\n\n    replica_id = self.get_replica_id(sharded_mesh_axes)\n\n    return LocalChunkInfo(tuple(local_slice), replica_id)\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_replica_id","title":"<code>get_replica_id(sharded_mesh_axes)</code>","text":"<p>Given mesh axes used for sharding, computes current host's replica id.</p> <p>To give an example, let's say there are two axes globally: data, and model, the mesh axes for sharding is ('data', ), which means we are going to partition an array along 'data' axis and replicate it along 'model' axis. The replica_id is to show the index of the current local host along the 'model' axis.</p> <p>Args:   sharded_mesh_axes: the mesh axes for sharding.</p> <p>Returns:   the index of the current local host along the non-sharding axes (i.e.,   replicating axes).</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_replica_id(self, sharded_mesh_axes: Union[str, Set[Optional[str]]]) -&gt; int:\n    \"\"\"Given mesh axes used for sharding, computes current host's replica id.\n\n    To give an example, let's say there are two axes globally: data, and model,\n    the mesh axes for sharding is ('data', ), which means we are going to\n    partition an array along 'data' axis and replicate it along 'model' axis.\n    The replica_id is to show the index of the current local host along the\n    'model' axis.\n\n    Args:\n      sharded_mesh_axes: the mesh axes for sharding.\n\n    Returns:\n      the index of the current local host along the non-sharding axes (i.e.,\n      replicating axes).\n    \"\"\"\n    if isinstance(sharded_mesh_axes, str):\n        sharded_mesh_axes = (sharded_mesh_axes,)\n\n    replicated_mesh_axes = [\n        mesh_axis for mesh_axis in self.mesh_axes\n        if mesh_axis not in sharded_mesh_axes\n    ]\n    replica_id = 0\n    for mesh_axis in replicated_mesh_axes:\n        chunk_id = self.chunk_ids[mesh_axis]\n        replica_id = replica_id * self.num_chunks[mesh_axis] + chunk_id\n\n    return replica_id\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.LocalChunker.get_shard_id","title":"<code>get_shard_id(sharded_mesh_axes)</code>","text":"<p>Given mesh axes used for sharding, computes current host's shard id.</p> <p>To give an example, let's say there are two axes globally: replica, data, and model, the mesh axes for sharding is ('replica', 'data'), which means we are going to partition an array along 'replica' and 'data' axes. The shard_id is to show the index of the current local host along the sharding axes (in this example, it's 'replica' and 'data' axes).</p> <p>More concretely, let's say we have 4 local hosts, and we use 'replica' and 'data' axes for data parallel (2 hosts along the replica axis, and 2 host along the data axis). The host located in ('replica': 0, 'data': 0), we should assign data shard-0 to it. For host ('replica': 0, 'data': 1), we assign shard-1. For host ('replica': 1, 'data': 0), we assign shard-2. For host ('replica': 1, 'data': 1), we assign shard-3.</p> <p>Note: the host location along 'replica' and 'data' axes, e.g., ('replica': 0, 'data': 0) is named chunk_id and stored in self._local_chunker.chunk_ids[axis].</p> <p>Args:   sharded_mesh_axes: the mesh axes for sharding.</p> <p>Returns:   the index of the current local host along the sharding axes.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_shard_id(self, sharded_mesh_axes: Union[str, Set[Optional[str]]]) -&gt; int:\n    \"\"\"Given mesh axes used for sharding, computes current host's shard id.\n\n    To give an example, let's say there are two axes globally: replica, data,\n    and model, the mesh axes for sharding is ('replica', 'data'), which means we\n    are going to partition an array along 'replica' and 'data' axes.\n    The shard_id is to show the index of the current local host along the\n    sharding axes (in this example, it's 'replica' and 'data' axes).\n\n    More concretely, let's say we have 4 local hosts, and we use 'replica' and\n    'data' axes for data parallel (2 hosts along the replica axis, and 2 host\n    along the data axis). The host located in ('replica': 0, 'data': 0), we\n    should assign data shard-0 to it. For host ('replica': 0, 'data': 1), we\n    assign shard-1. For host ('replica': 1, 'data': 0), we assign shard-2.\n    For host ('replica': 1, 'data': 1), we assign shard-3.\n\n    Note: the host location along 'replica' and 'data' axes, e.g.,\n    ('replica': 0, 'data': 0) is named chunk_id and stored in\n    self._local_chunker.chunk_ids[axis].\n\n    Args:\n      sharded_mesh_axes: the mesh axes for sharding.\n\n    Returns:\n      the index of the current local host along the sharding axes.\n    \"\"\"\n    if isinstance(sharded_mesh_axes, str):\n        sharded_mesh_axes = (sharded_mesh_axes,)\n\n    shard_id = 0\n    for mesh_axis in sharded_mesh_axes:\n        chunk_id = self.chunk_ids[mesh_axis]\n        shard_id = shard_id * self.num_chunks[mesh_axis] + chunk_id\n\n    return shard_id\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner","title":"<code>PjitPartitioner</code>","text":"<p>             Bases: <code>BasePjitPartitioner</code></p> <p>Partitioner that uses named axes and jax.pjit.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>class PjitPartitioner(BasePjitPartitioner):\n    \"\"\"Partitioner that uses named axes and jax.pjit.\"\"\"\n\n    def __init__(\n            self,\n            num_partitions: Optional[int] = None,\n            model_parallel_submesh: Optional[HardwareMesh] = None,\n            params_on_devices: bool = True,\n            backend: Optional[str] = None,\n            ici_mesh_shape: Optional[HardwareMesh] = None,\n            dcn_mesh_shape: Optional[HardwareMesh] = None,\n            logical_axis_rules: Optional[LogicalAxisRules] = None,\n    ):\n        \"\"\"PjitPartitioner constructor.\n\n        See https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.mdx/usage/partitioning for details.\n\n        Args:\n          num_partitions: an integer that specifies the size of the model parallel\n            submesh to be automatically selected for the current topology. See\n            `model_parallel_submesh` for details on how this submesh is used.\n            Mutually exclusive with `model_parallel_submesh`.\n          model_parallel_submesh: is a 4-tuple that specifies the `(x, y, z, c)`\n            submesh model-parallel device tile, an axis of accelerator parallelism\n            orthogonal to data parallelism. Array axes in a model's parameters or\n            activations can be sharded over this submesh using axis rules (see\n            `logical_axis_rules`) that map them to 'model'. The effective number of\n            model sub-partitions is equal to `np.prod(model_parallel_submesh)` and\n            must evenly divide the total number of devices (i.e.,\n            `jax.device_count() % np.prod(model_parallel_submesh) == 0`). The rest\n            of the TPU mesh is the data parallel submesh, providing\n            `jax.device_count() // np.prod(model_parallel_submesh)` partitions. It\n            is used for data (batch) parallelism and to shard other array axes that\n            are mapped to 'data'. This argument is mutually exclusive with\n            `num_partitions`.\n          params_on_devices: whether to keep the params on devices, if False -\n            params stay in the host memory. Note that some partitioners might ignore\n            this setting, for example if they don't support storing all params on\n            device memory.\n          backend: get devices from the pinned backend, if specified. This is useful\n            for explicitly specifying the devices other than relying on\n            jax_platform_name.\n          ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in\n            each slice. The meaning of each mesh axis is defined by mesh_axis_names,\n            so these two params must be the same length. If dcn_mesh_shape is\n            present, the overall mesh is the product of ici_mesh_shape and\n            dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with\n            mesh_axis_names ['replica', 'data', 'model'] indicates 2-way replica\n            parallelism, 3-way data parallelism, and 4-way model parallelism over 24\n            devices. None, the default, is equivalent to a sequence of ones and\n            means that the model is placed on a single device.\n          dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over\n            multiple slices. The overall mesh is the product of ici_mesh_shape and\n            dcn_mesh_shape, and the meaning of each mesh axis is defined by\n            mesh_axis_names, so these three params must be the same length.\n          logical_axis_rules: a priority-ordered sequence of KV tuples that maps\n            logical axis names to either `None` (not sharded), 'model' (to shard\n            across the model-parallel submesh), or 'data' (to shard across the\n            data-parallel submesh).\n        \"\"\"\n        super().__init__(\n            num_partitions=num_partitions,\n            model_parallel_submesh=model_parallel_submesh,\n            params_on_devices=params_on_devices,\n            backend=backend,\n            ici_mesh_shape=ici_mesh_shape,\n            dcn_mesh_shape=dcn_mesh_shape,\n        )\n        if logical_axis_rules is None:\n            logical_axis_rules = standard_logical_axis_rules()\n        if ici_mesh_shape is not None and dcn_mesh_shape is not None:\n            # Split batch over new replica axis.\n            logical_axis_rules = (\n                (k, ('replica', 'data') if k == 'batch' else v)\n                for k, v in logical_axis_rules\n            )\n        self._logical_axis_rules = tuple(logical_axis_rules)\n        (self._data_axis,) = flax_partitioning.logical_to_mesh_axes(\n            ['batch'], self._logical_axis_rules\n        )\n\n    def partition(\n            self,\n            fn: Callable,  # pylint: disable=g-bare-generic\n            in_axis_resources,\n            out_axis_resources,\n            static_argnums: Union[int, Sequence[int]] = (),\n            donate_argnums: Union[int, Sequence[int]] = ()\n    ) -&gt; PjittedFnWithContext:\n        \"\"\"Partitions the function using jax.pjit.\"\"\"\n        pjitted = pjit(\n            fn,\n            in_shardings=in_axis_resources,\n            out_shardings=out_axis_resources,\n            static_argnums=static_argnums,\n            donate_argnums=donate_argnums,\n        )\n\n        return PjittedFnWithContext(pjitted, self.mesh, self._logical_axis_rules)\n\n    @property\n    def logical_axis_rules(self):\n        \"\"\"Returns the logical axis rules.\"\"\"\n        return self._logical_axis_rules\n\n    def get_logical_axes(self, train_state):\n        \"\"\"Returns a copy of TrainState with Optional[AxisNames] as leaves.\"\"\"\n        return train_state.as_logical_axes()\n\n    def get_mesh_axes(self, train_state):\n        \"\"\"Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.\"\"\"\n        logical_axes = self.get_logical_axes(train_state)\n\n        def _logical_to_mesh_axes(param_name, logical_axes):\n            if logical_axes is None:\n                return None\n            elif logical_axes is traverse_util.empty_node:\n                return traverse_util.empty_node\n            try:\n                return flax_partitioning.logical_to_mesh_axes(logical_axes,\n                                                              self._logical_axis_rules)\n            except ValueError as e:\n                raise ValueError(f'Failed to map logical axes for {param_name}') from e\n\n        flat_logical_axes = traverse_util.flatten_dict(\n            logical_axes.state_dict(), keep_empty_nodes=True, sep='/')\n        flat_mesh_axes = {\n            k: _logical_to_mesh_axes(k, v) for k, v in flat_logical_axes.items()\n        }\n\n        return logical_axes.restore_state(\n            traverse_util.unflatten_dict(flat_mesh_axes, sep='/'))\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.logical_axis_rules","title":"<code>logical_axis_rules</code>  <code>property</code>","text":"<p>Returns the logical axis rules.</p>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.__init__","title":"<code>__init__(num_partitions=None, model_parallel_submesh=None, params_on_devices=True, backend=None, ici_mesh_shape=None, dcn_mesh_shape=None, logical_axis_rules=None)</code>","text":"<p>PjitPartitioner constructor.</p> <p>See https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.mdx/usage/partitioning for details.</p> <p>Args:   num_partitions: an integer that specifies the size of the model parallel     submesh to be automatically selected for the current topology. See     <code>model_parallel_submesh</code> for details on how this submesh is used.     Mutually exclusive with <code>model_parallel_submesh</code>.   model_parallel_submesh: is a 4-tuple that specifies the <code>(x, y, z, c)</code>     submesh model-parallel device tile, an axis of accelerator parallelism     orthogonal to data parallelism. Array axes in a model's parameters or     activations can be sharded over this submesh using axis rules (see     <code>logical_axis_rules</code>) that map them to 'model'. The effective number of     model sub-partitions is equal to <code>np.prod(model_parallel_submesh)</code> and     must evenly divide the total number of devices (i.e.,     <code>jax.device_count() % np.prod(model_parallel_submesh) == 0</code>). The rest     of the TPU mesh is the data parallel submesh, providing     <code>jax.device_count() // np.prod(model_parallel_submesh)</code> partitions. It     is used for data (batch) parallelism and to shard other array axes that     are mapped to 'data'. This argument is mutually exclusive with     <code>num_partitions</code>.   params_on_devices: whether to keep the params on devices, if False -     params stay in the host memory. Note that some partitioners might ignore     this setting, for example if they don't support storing all params on     device memory.   backend: get devices from the pinned backend, if specified. This is useful     for explicitly specifying the devices other than relying on     jax_platform_name.   ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in     each slice. The meaning of each mesh axis is defined by mesh_axis_names,     so these two params must be the same length. If dcn_mesh_shape is     present, the overall mesh is the product of ici_mesh_shape and     dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with     mesh_axis_names ['replica', 'data', 'model'] indicates 2-way replica     parallelism, 3-way data parallelism, and 4-way model parallelism over 24     devices. None, the default, is equivalent to a sequence of ones and     means that the model is placed on a single device.   dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over     multiple slices. The overall mesh is the product of ici_mesh_shape and     dcn_mesh_shape, and the meaning of each mesh axis is defined by     mesh_axis_names, so these three params must be the same length.   logical_axis_rules: a priority-ordered sequence of KV tuples that maps     logical axis names to either <code>None</code> (not sharded), 'model' (to shard     across the model-parallel submesh), or 'data' (to shard across the     data-parallel submesh).</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def __init__(\n        self,\n        num_partitions: Optional[int] = None,\n        model_parallel_submesh: Optional[HardwareMesh] = None,\n        params_on_devices: bool = True,\n        backend: Optional[str] = None,\n        ici_mesh_shape: Optional[HardwareMesh] = None,\n        dcn_mesh_shape: Optional[HardwareMesh] = None,\n        logical_axis_rules: Optional[LogicalAxisRules] = None,\n):\n    \"\"\"PjitPartitioner constructor.\n\n    See https://github.com/google-research/text-to-text-transfer-transformer/blob/main/README.mdx/usage/partitioning for details.\n\n    Args:\n      num_partitions: an integer that specifies the size of the model parallel\n        submesh to be automatically selected for the current topology. See\n        `model_parallel_submesh` for details on how this submesh is used.\n        Mutually exclusive with `model_parallel_submesh`.\n      model_parallel_submesh: is a 4-tuple that specifies the `(x, y, z, c)`\n        submesh model-parallel device tile, an axis of accelerator parallelism\n        orthogonal to data parallelism. Array axes in a model's parameters or\n        activations can be sharded over this submesh using axis rules (see\n        `logical_axis_rules`) that map them to 'model'. The effective number of\n        model sub-partitions is equal to `np.prod(model_parallel_submesh)` and\n        must evenly divide the total number of devices (i.e.,\n        `jax.device_count() % np.prod(model_parallel_submesh) == 0`). The rest\n        of the TPU mesh is the data parallel submesh, providing\n        `jax.device_count() // np.prod(model_parallel_submesh)` partitions. It\n        is used for data (batch) parallelism and to shard other array axes that\n        are mapped to 'data'. This argument is mutually exclusive with\n        `num_partitions`.\n      params_on_devices: whether to keep the params on devices, if False -\n        params stay in the host memory. Note that some partitioners might ignore\n        this setting, for example if they don't support storing all params on\n        device memory.\n      backend: get devices from the pinned backend, if specified. This is useful\n        for explicitly specifying the devices other than relying on\n        jax_platform_name.\n      ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in\n        each slice. The meaning of each mesh axis is defined by mesh_axis_names,\n        so these two params must be the same length. If dcn_mesh_shape is\n        present, the overall mesh is the product of ici_mesh_shape and\n        dcn_mesh_shape. For example, an ici_mesh_shape of [2, 3, 4] with\n        mesh_axis_names ['replica', 'data', 'model'] indicates 2-way replica\n        parallelism, 3-way data parallelism, and 4-way model parallelism over 24\n        devices. None, the default, is equivalent to a sequence of ones and\n        means that the model is placed on a single device.\n      dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over\n        multiple slices. The overall mesh is the product of ici_mesh_shape and\n        dcn_mesh_shape, and the meaning of each mesh axis is defined by\n        mesh_axis_names, so these three params must be the same length.\n      logical_axis_rules: a priority-ordered sequence of KV tuples that maps\n        logical axis names to either `None` (not sharded), 'model' (to shard\n        across the model-parallel submesh), or 'data' (to shard across the\n        data-parallel submesh).\n    \"\"\"\n    super().__init__(\n        num_partitions=num_partitions,\n        model_parallel_submesh=model_parallel_submesh,\n        params_on_devices=params_on_devices,\n        backend=backend,\n        ici_mesh_shape=ici_mesh_shape,\n        dcn_mesh_shape=dcn_mesh_shape,\n    )\n    if logical_axis_rules is None:\n        logical_axis_rules = standard_logical_axis_rules()\n    if ici_mesh_shape is not None and dcn_mesh_shape is not None:\n        # Split batch over new replica axis.\n        logical_axis_rules = (\n            (k, ('replica', 'data') if k == 'batch' else v)\n            for k, v in logical_axis_rules\n        )\n    self._logical_axis_rules = tuple(logical_axis_rules)\n    (self._data_axis,) = flax_partitioning.logical_to_mesh_axes(\n        ['batch'], self._logical_axis_rules\n    )\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.get_logical_axes","title":"<code>get_logical_axes(train_state)</code>","text":"<p>Returns a copy of TrainState with Optional[AxisNames] as leaves.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_logical_axes(self, train_state):\n    \"\"\"Returns a copy of TrainState with Optional[AxisNames] as leaves.\"\"\"\n    return train_state.as_logical_axes()\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.get_mesh_axes","title":"<code>get_mesh_axes(train_state)</code>","text":"<p>Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_mesh_axes(self, train_state):\n    \"\"\"Returns a copy of TrainState with Optional[PartitionSpecs] as leaves.\"\"\"\n    logical_axes = self.get_logical_axes(train_state)\n\n    def _logical_to_mesh_axes(param_name, logical_axes):\n        if logical_axes is None:\n            return None\n        elif logical_axes is traverse_util.empty_node:\n            return traverse_util.empty_node\n        try:\n            return flax_partitioning.logical_to_mesh_axes(logical_axes,\n                                                          self._logical_axis_rules)\n        except ValueError as e:\n            raise ValueError(f'Failed to map logical axes for {param_name}') from e\n\n    flat_logical_axes = traverse_util.flatten_dict(\n        logical_axes.state_dict(), keep_empty_nodes=True, sep='/')\n    flat_mesh_axes = {\n        k: _logical_to_mesh_axes(k, v) for k, v in flat_logical_axes.items()\n    }\n\n    return logical_axes.restore_state(\n        traverse_util.unflatten_dict(flat_mesh_axes, sep='/'))\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.PjitPartitioner.partition","title":"<code>partition(fn, in_axis_resources, out_axis_resources, static_argnums=(), donate_argnums=())</code>","text":"<p>Partitions the function using jax.pjit.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def partition(\n        self,\n        fn: Callable,  # pylint: disable=g-bare-generic\n        in_axis_resources,\n        out_axis_resources,\n        static_argnums: Union[int, Sequence[int]] = (),\n        donate_argnums: Union[int, Sequence[int]] = ()\n) -&gt; PjittedFnWithContext:\n    \"\"\"Partitions the function using jax.pjit.\"\"\"\n    pjitted = pjit(\n        fn,\n        in_shardings=in_axis_resources,\n        out_shardings=out_axis_resources,\n        static_argnums=static_argnums,\n        donate_argnums=donate_argnums,\n    )\n\n    return PjittedFnWithContext(pjitted, self.mesh, self._logical_axis_rules)\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.PjittedFnWithContext","title":"<code>PjittedFnWithContext</code>","text":"<p>             Bases: <code>PartitionedCallable</code></p> <p>Wraps pjitted function to apply the appropriate contexts.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>class PjittedFnWithContext(PartitionedCallable):\n    \"\"\"Wraps pjitted function to apply the appropriate contexts.\"\"\"\n\n    def __init__(self,\n                 pjitted_fn,\n                 partition_mesh: Mesh,\n                 logical_axis_rules: flax_partitioning.LogicalRules = ()):\n        self._pjitted_fn = pjitted_fn\n        self._mesh = partition_mesh\n        self._logical_axis_rules = logical_axis_rules\n\n    def __call__(self, *args, **kwargs):\n        with Mesh(self._mesh.devices,\n                  self._mesh.axis_names), flax_partitioning.axis_rules(self._logical_axis_rules):\n            return self._pjitted_fn(*args, **kwargs)\n\n    def lower(self, *args, **kwargs):\n        with Mesh(self._mesh.devices,\n                  self._mesh.axis_names), flax_partitioning.axis_rules(self._logical_axis_rules):\n            return self._pjitted_fn.lower(*args, **kwargs)\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.bounds_from_last_device","title":"<code>bounds_from_last_device(last_device)</code>","text":"<p>Get the bound from the given last device.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def bounds_from_last_device(last_device: jax.Device) -&gt; HardwareMesh:\n    \"\"\"Get the bound from the given last device.\"\"\"\n    # Must be passed the device at the highest-coordinate corner of the\n    # relevant mesh, which is a requirement we know is satisfied by the last\n    # device in jax.devices().\n    if hasattr(last_device, 'coords'):\n        x, y, z = last_device.coords\n        return x + 1, y + 1, z + 1, last_device.core_on_chip + 1\n    else:\n        # On non-TPU platforms, the \"mesh\" is hosts x devices per host in order\n        # to take advantage of faster within-host interconnect.\n        return jax.process_count(), jax.local_device_count()\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.default_mesh","title":"<code>default_mesh(num_partitions, model_parallel_submesh=None, backend=None, ici_mesh_shape=None, dcn_mesh_shape=None)</code>","text":"<p>Attempt to return a default mesh for simple cases.</p> <p>Args:   num_partitions: number of partitions to use, will be ignored if     model_parallel_submesh is provided.   model_parallel_submesh: 4-tuple that specifies the x,y,z,c submesh to use as     the model-parallel device tile.   backend: get devices from the pinned backend, if specified. This is useful     for explicitly specifying the devices other than relying on     jax_platform_name.   ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in each     slice. The meaning of each mesh axis is defined by mesh_axis_names, so     these two params must be the same length. If dcn_mesh_shape is present,     the overall mesh is the product of ici_mesh_shape and dcn_mesh_shape. For     example, an ici_mesh_shape of [2, 3, 4] with mesh_axis_names ['replica',     'data', 'model'] indicates 2-way replica parallelism, 3-way data     parallelism, and 4-way model parallelism over 24 devices. None, the     default, is equivalent to a sequence of ones and means that the model is     placed on a single device.   dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over     multiple slices. The overall mesh is the product of ici_mesh_shape and     dcn_mesh_shape, and the meaning of each mesh axis is defined by     mesh_axis_names, so these three params must be the same length.</p> <p>Returns:   xmap/pjit 2D Mesh with 'data', 'model' mesh axes if single-slice, otherwise   3D Mesh with 'replica', 'data', and 'model' mesh axes.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def default_mesh(\n        num_partitions: int,\n        model_parallel_submesh: Optional[HardwareMesh] = None,\n        backend: Optional[str] = None,\n        ici_mesh_shape: Optional[HardwareMesh] = None,\n        dcn_mesh_shape: Optional[HardwareMesh] = None,\n) -&gt; Mesh:\n    \"\"\"Attempt to return a default mesh for simple cases.\n\n    Args:\n      num_partitions: number of partitions to use, will be ignored if\n        model_parallel_submesh is provided.\n      model_parallel_submesh: 4-tuple that specifies the x,y,z,c submesh to use as\n        the model-parallel device tile.\n      backend: get devices from the pinned backend, if specified. This is useful\n        for explicitly specifying the devices other than relying on\n        jax_platform_name.\n      ici_mesh_shape: Shape of the logical mesh used for SPMD parallelism in each\n        slice. The meaning of each mesh axis is defined by mesh_axis_names, so\n        these two params must be the same length. If dcn_mesh_shape is present,\n        the overall mesh is the product of ici_mesh_shape and dcn_mesh_shape. For\n        example, an ici_mesh_shape of [2, 3, 4] with mesh_axis_names ['replica',\n        'data', 'model'] indicates 2-way replica parallelism, 3-way data\n        parallelism, and 4-way model parallelism over 24 devices. None, the\n        default, is equivalent to a sequence of ones and means that the model is\n        placed on a single device.\n      dcn_mesh_shape: Shape of the logical mesh used for SPMD parallelism over\n        multiple slices. The overall mesh is the product of ici_mesh_shape and\n        dcn_mesh_shape, and the meaning of each mesh axis is defined by\n        mesh_axis_names, so these three params must be the same length.\n\n    Returns:\n      xmap/pjit 2D Mesh with 'data', 'model' mesh axes if single-slice, otherwise\n      3D Mesh with 'replica', 'data', and 'model' mesh axes.\n    \"\"\"\n    devices = jax.devices(backend)\n    last_device = devices[-1]\n    platform = last_device.platform\n    device_kind = last_device.device_kind\n    bounds = bounds_from_last_device(last_device)\n\n    if ici_mesh_shape is not None and dcn_mesh_shape is not None:\n        device_mesh = create_hybrid_device_mesh(\n            ici_mesh_shape,\n            dcn_mesh_shape,\n            devices=devices,\n        )\n        multi_slice_global_mesh = Mesh(device_mesh, ['replica', 'data', 'model'])\n        logging.info(\n            'multi_slice_global_mesh axis_names: %s',\n            multi_slice_global_mesh.axis_names,\n        )\n        logging.info(\n            'multi_slice_global_mesh devices: %s', multi_slice_global_mesh.devices\n        )\n        logging.info(\n            'multi_slice_global_mesh devices shape: %s',\n            multi_slice_global_mesh.devices.shape,\n        )\n        return multi_slice_global_mesh\n\n    if model_parallel_submesh:\n        return get_mesh(model_parallel_submesh, backend=backend)\n\n    if platform == 'cpu':\n        return get_cpu_mesh()\n    elif platform == 'gpu':\n        return get_gpu_mesh(num_partitions)\n\n    mps = None\n    if device_kind in ('TPU v2', 'TPU v3'):\n        if num_partitions == 1:\n            mps = (1, 1, 1, 1)\n        elif num_partitions == 2:\n            mps = (1, 1, 1, 2)\n        elif num_partitions == 4:\n            mps = (2, 1, 1, 2)\n        elif num_partitions == 8:\n            mps = (2, 2, 1, 2)\n        elif num_partitions == 16:\n            mps = (4, 2, 1, 2)\n    # assume the use of megacore on TPU v4\n    elif (device_kind == 'TPU v4' or\n          device_kind == 'TPU v4 lite') and bounds[3] == 1:\n        if num_partitions == 1:\n            mps = (1, 1, 1, 1)\n        elif num_partitions == 2:\n            mps = (1, 2, 1, 1)\n        elif num_partitions == 4:\n            if bounds[0] &gt;= 4:\n                mps = (4, 1, 1, 1)\n            else:\n                mps = (2, 2, 1, 1)\n        elif num_partitions == 8:\n            if bounds[2] &gt;= 8:\n                mps = (1, 1, 8, 1)\n            else:\n                mps = (4, 2, 1, 1)\n        elif num_partitions == 16:\n            if bounds[2] &gt;= 16:\n                mps = (1, 1, 16, 1)\n            elif bounds[0] &gt;= 8:\n                mps = (8, 2, 1, 1)\n            elif bounds[0] &gt;= 4:\n                mps = (4, 4, 1, 1)\n            else:\n                mps = (2, 2, 4, 1)\n\n    if mps is None:\n        raise ValueError(\n            'No default mesh for this configuration: specify '\n            'config.model_parallel_submesh explicitly. \\n'\n            f'Platform: {platform}\\n'\n            f'Device kind: {device_kind}\\n'\n            f'Num partitions: {num_partitions}\\n'\n            f'Bounds: {bounds}'\n        )\n    return get_mesh(mps, backend=backend)\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.get_coords","title":"<code>get_coords(device)</code>","text":"<p>Returns the coordinates of the given device.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_coords(device: jax.Device) -&gt; HardwareMesh:\n    \"\"\"Returns the coordinates of the given device.\"\"\"\n    if hasattr(device, 'coords'):\n        return *device.coords, device.core_on_chip\n    return device.process_index, device.id % jax.local_device_count()\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.get_cpu_mesh","title":"<code>get_cpu_mesh()</code>","text":"<p>Trivial mesh for CPU Testing.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_cpu_mesh() -&gt; Mesh:\n    \"\"\"Trivial mesh for CPU Testing.\"\"\"\n    devices = np.empty(\n        (jax.process_count(), jax.local_device_count()), dtype=object\n    )\n    for device in jax.devices():\n        devices[device.process_index, device.id % jax.local_device_count()] = device\n    return Mesh(devices, ['data', 'model'])\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.get_gpu_mesh","title":"<code>get_gpu_mesh(num_partitions)</code>","text":"<p>Mesh for GPUs that preferentially places 'model' on NVLink.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_gpu_mesh(num_partitions: int) -&gt; Mesh:\n    \"\"\"Mesh for GPUs that preferentially places 'model' on NVLink.\"\"\"\n    nvlink_size = jax.local_device_count()\n    dcn_size = jax.process_count()\n    nvlink_mp = min(num_partitions, nvlink_size)\n    nvlink_dp, extra1 = divmod(nvlink_size, nvlink_mp)\n    dcn_mp, extra2 = divmod(num_partitions, nvlink_mp)\n    assert not (extra1 or extra2), ('number of partitions on GPU must be a factor'\n                                    ' or multiple of the number of local devices')\n    dcn_dp = dcn_size // dcn_mp\n\n    devices = create_hybrid_device_mesh(\n        mesh_shape=[nvlink_dp, nvlink_mp],\n        dcn_mesh_shape=[dcn_dp, dcn_mp],\n        process_is_granule=True)\n\n    global_mesh = Mesh(devices, ['data', 'model'])\n    logging.info('global_mesh axis_names: %s', global_mesh.axis_names)\n    logging.info('global_mesh devices: %s', global_mesh.devices)\n    return global_mesh\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.get_mesh","title":"<code>get_mesh(model_parallel_submesh, input_devices=(), input_local_devices=(), tile_by_host_if_needed=True, backend=None)</code>","text":"<p>Construct an xmap/pjit Mesh for the given model-parallel submesh.</p> <p>The resulting mesh has two resource axes: 'model', with the provided submesh shape, and 'data', which covers the rest of the mesh.</p> <p>Args:   model_parallel_submesh: a HardwareMesh spec, namely (x,y,z,core) on TPU for     a single model-parallel replica's \"tile\" in the physical device mesh. The     first three elements (<code>x</code>, <code>y</code>, and <code>z</code>) should be factors of the pod     slice; e.g., if you are using df_4x8, then <code>x</code> should be a factor of 4     (one of 1, 2, 4), <code>y</code> should be a factor of 8 (one of 1, 2, 4, 8), and <code>z</code>     must be 1, because TPU v3 slices are only 2D. <code>z</code> can be &gt;1 for TPU v4     (and maybe later TPUs) that allow 3D slices. <code>core</code> is the number of cores     to use from each TPU node. As communication is usually fastest inside the     same node, if you need a tile of more than 1 core, then     you should first increase <code>core</code>: e.g., for TPU v3, (1,1,1,2) is better       than (2,1,1,1). To pick a good spec, try a few possible values until you       get high TPU utilization.   input_devices: the devices to use, will use jax.devices() if this is not     set.   input_local_devices: the local devices to use, will use jax.local_devices()     if this is not set.   tile_by_host_if_needed: JAX currently requires that the parts of any sharded     array that are located on one host's local devices form a single     contiguous slice. A best effort will be made to achieve this without     \"tiling\" the device assignment over hosts (which can reduce XLA collective     performance). If this flag is True, then the device assignment will be     tiled over hosts if necessary to satisfy this constraint and create a     buildable mesh; if false, mesh construction will fail instead.   backend: get devices from the pinned backend, if specified. This is     useful for explicitly specifying the devices other than relying on     jax_platform_name.</p> <p>Returns:   A xmap / pjit Mesh containing the virtual device mesh with data, model axes.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def get_mesh(model_parallel_submesh: HardwareMesh,\n             input_devices: Sequence[JaxDevice] = (),\n             input_local_devices: Sequence[JaxDevice] = (),\n             tile_by_host_if_needed: bool = True,\n             backend: Optional[str] = None) -&gt; Mesh:\n    \"\"\"Construct an xmap/pjit Mesh for the given model-parallel submesh.\n\n    The resulting mesh has two resource axes: 'model', with the provided submesh\n    shape, and 'data', which covers the rest of the mesh.\n\n    Args:\n      model_parallel_submesh: a HardwareMesh spec, namely (x,y,z,core) on TPU for\n        a single model-parallel replica's \"tile\" in the physical device mesh. The\n        first three elements (`x`, `y`, and `z`) should be factors of the pod\n        slice; e.g., if you are using df_4x8, then `x` should be a factor of 4\n        (one of 1, 2, 4), `y` should be a factor of 8 (one of 1, 2, 4, 8), and `z`\n        must be 1, because TPU v3 slices are only 2D. `z` can be &gt;1 for TPU v4\n        (and maybe later TPUs) that allow 3D slices. `core` is the number of cores\n        to use from each TPU node. As communication is usually fastest inside the\n        same node, if you need a tile of more than 1 core, then\n        you should first increase `core`: e.g., for TPU v3, (1,1,1,2) is better\n          than (2,1,1,1). To pick a good spec, try a few possible values until you\n          get high TPU utilization.\n      input_devices: the devices to use, will use jax.devices() if this is not\n        set.\n      input_local_devices: the local devices to use, will use jax.local_devices()\n        if this is not set.\n      tile_by_host_if_needed: JAX currently requires that the parts of any sharded\n        array that are located on one host's local devices form a single\n        contiguous slice. A best effort will be made to achieve this without\n        \"tiling\" the device assignment over hosts (which can reduce XLA collective\n        performance). If this flag is True, then the device assignment will be\n        tiled over hosts if necessary to satisfy this constraint and create a\n        buildable mesh; if false, mesh construction will fail instead.\n      backend: get devices from the pinned backend, if specified. This is\n        useful for explicitly specifying the devices other than relying on\n        jax_platform_name.\n\n    Returns:\n      A xmap / pjit Mesh containing the virtual device mesh with data, model axes.\n    \"\"\"\n    input_devices = input_devices or jax.devices(backend)\n    input_local_devices = input_local_devices or jax.local_devices(0, backend)\n    # Sort input_devices based on coords, as backends might not return devices\n    # in order.\n    last_device = sorted(input_devices, key=get_coords)[-1]\n    last_input_local_devices = sorted(input_local_devices, key=get_coords)[-1]\n    logging.info('last device coords : %r\\nlast local device coords: %r',\n                 get_coords(last_device), get_coords(last_input_local_devices))\n    global_hardware_mesh = bounds_from_last_device(last_device)\n    mesh_ndim = len(global_hardware_mesh)\n    local_hardware_mesh = bounds_from_last_device(last_input_local_devices)\n    mesh_err = (\n        f'each dimension of the model parallel submesh {model_parallel_submesh} '\n        'must be a factor of the corresponding dimension of the global device '\n        f'mesh {global_hardware_mesh}')\n    assert not any(\n        g % m\n        for g, m in zip(global_hardware_mesh, model_parallel_submesh)), mesh_err\n    assert not any(\n        g % l for g, l in zip(global_hardware_mesh, local_hardware_mesh))\n    devices = np.empty(global_hardware_mesh, dtype=object)\n    for device in input_devices:\n        device_coords = get_coords(device)\n        devices[device_coords] = device\n    tile_by_host = tile_by_host_if_needed\n    if len(global_hardware_mesh) == 4:\n        # enable contiguous local chunks without host tiling by making Z major\n        global_hardware_mesh = typing.cast(Tuple[int, int, int, int],\n                                           global_hardware_mesh)\n        model_parallel_submesh = typing.cast(Tuple[int, int, int, int],\n                                             model_parallel_submesh)\n        gx, gy, gz, gc = global_hardware_mesh\n        mx, my, mz, mc = model_parallel_submesh\n        if (mx == gx &gt; 1 and my == mz == 1) or (mx == 1 and my == gy &gt; 1 and\n                                                mz == gz &gt; 1):\n            logging.info('ensuring YZ plane has a Z-major device order')\n            # YZ should be ZY\n            assert mc == gc, (mc, gc)\n            global_hardware_mesh = gx, gz, gy, gc\n            model_parallel_submesh = mx, mz, my, mc\n            devices = devices.swapaxes(1, 2)\n            tile_by_host = False\n        if (my == gy &gt; 1 and mx == mz == 1) or (my == 1 and mx == gx &gt; 1 and\n                                                mz == gz &gt; 1):\n            logging.info('ensuring XZ plane has a Z-major device order')\n            # XZ should be ZX\n            assert mc == gc, (mc, gc)\n            global_hardware_mesh = gz, gy, gx, gc\n            model_parallel_submesh = mz, my, mx, mc\n            devices = devices.swapaxes(0, 2)\n            tile_by_host = False\n    if tile_by_host:\n        logging.warning(\n            'Tiling device assignment mesh by hosts, which may lead to '\n            'reduced XLA collective performance. To avoid this, modify '\n            'the model parallel submesh or run with more tasks per host.')\n        tile_err = (\n            'to tile the mesh by hosts, each dimension of the model parallel '\n            'submesh must be either a factor or a multiple of the corresponding '\n            'dimension of the per-host submesh')\n\n        def dh_dd_mh_md(g: int, m: int, l: int) -&gt; Tuple[int, int, int, int]:\n            \"\"\"Split a global mesh dimension into four tiling components.\n\n            Args:\n              g: global mesh bounds dimension size\n              m: model-parallel submesh bounds dimension size\n              l: local submesh bounds dimension size\n\n            Returns:\n              The resulting tuple divides the dimension into the hosts component of\n              the data-parallel submesh, the devices component of the data-parallel\n              submesh, the hosts component of the model-parallel submesh, and the\n              devices component of the model-parallel submesh.\n            \"\"\"\n            d = g // m\n            if m &gt;= l:\n                assert not m % l, tile_err\n                return d, 1, m // l, l\n            else:\n                assert not l % m, tile_err\n                return d // (l // m), l // m, 1, m\n\n        dh_dd_mh_md_tups = map(dh_dd_mh_md, global_hardware_mesh,\n                               model_parallel_submesh, local_hardware_mesh)\n        devices = devices.reshape(*(s for t in dh_dd_mh_md_tups for s in t))\n        devices = devices.transpose(*(4 * i for i in range(mesh_ndim)),\n                                    *(4 * i + 1 for i in range(mesh_ndim)),\n                                    *(4 * i + 2 for i in range(mesh_ndim)),\n                                    *(4 * i + 3 for i in range(mesh_ndim)))\n    else:\n        model_data_tups = [\n            (g // m, m)\n            for g, m in zip(global_hardware_mesh, model_parallel_submesh)\n        ]\n        devices = devices.reshape(*(s for t in model_data_tups for s in t))\n        devices = devices.transpose(*(2 * i for i in range(mesh_ndim)),\n                                    *(2 * i + 1 for i in range(mesh_ndim)))\n    # reshape to (data, model)\n    devices = devices.reshape(-1, np.prod(model_parallel_submesh))\n    global_mesh = Mesh(devices, ['data', 'model'])\n    logging.info('global_mesh axis_names: %s', global_mesh.axis_names)\n    logging.info('global_mesh devices: %s', global_mesh.devices)\n    logging.info('global_mesh devices shape: %s', global_mesh.devices.shape)\n    return global_mesh\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.global_mesh_defined","title":"<code>global_mesh_defined()</code>","text":"<p>Checks if global xmap/pjit mesh resource environment is defined.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def global_mesh_defined():\n    \"\"\"Checks if global xmap/pjit mesh resource environment is defined.\"\"\"\n    maps_env = jax.experimental.maps.thread_resources.env\n    return maps_env.physical_mesh.devices.shape != ()  # pylint: disable=g-explicit-bool-comparison\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.standard_logical_axis_rules","title":"<code>standard_logical_axis_rules(activation_partitioning_dims=1, parameter_partitioning_dims=1, additional_rules=None)</code>","text":"<p>Default sharding rules for T5X model in terms of logical axis names.</p> <p>Args:   activation_partitioning_dims: enables 2-D activation sharding when set to 2.   parameter_partitioning_dims: enables 2-D parameter sharding when set to 2.   additional_rules: additional rules (a sequence of tuples) that will be     appended to the standard rules.</p> <p>Returns:   Sequence of logical axis rules</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def standard_logical_axis_rules(\n        activation_partitioning_dims: int = 1,\n        parameter_partitioning_dims: int = 1,\n        additional_rules: Optional[LogicalAxisRules] = None) -&gt; LogicalAxisRules:\n    \"\"\"Default sharding rules for T5X model in terms of logical axis names.\n\n    Args:\n      activation_partitioning_dims: enables 2-D activation sharding when set to 2.\n      parameter_partitioning_dims: enables 2-D parameter sharding when set to 2.\n      additional_rules: additional rules (a sequence of tuples) that will be\n        appended to the standard rules.\n\n    Returns:\n      Sequence of logical axis rules\n    \"\"\"\n    logging.info(\n        '`activation_partitioning_dims` = %d, `parameter_partitioning_dims` = %d',\n        activation_partitioning_dims, parameter_partitioning_dims)\n\n    if activation_partitioning_dims == 1 and parameter_partitioning_dims == 1:\n        rules = [\n            ('batch', 'data'),\n            ('vocab', 'model'),\n            ('embed', None),\n            ('mlp', 'model'),\n            ('heads', 'model'),\n            ('kv', None),\n            ('joined_kv', 'model'),  # joined heads+kv dim in 2D attn param layouts\n        ]\n    elif activation_partitioning_dims == 2 and parameter_partitioning_dims == 1:\n        rules = [\n            ('batch', 'data'),\n            ('vocab', 'model'),\n            ('mlp', 'model'),\n            ('heads', 'model'),\n            ('kv', None),\n            ('joined_kv', 'model'),\n            ('embed', 'model'),\n        ]\n    elif activation_partitioning_dims == 1 and parameter_partitioning_dims == 2:\n        rules = [\n            ('batch', 'data'),\n            ('vocab', 'model'),\n            ('mlp', 'model'),\n            ('heads', 'model'),\n            ('kv', None),\n            ('joined_kv', 'model'),\n            ('embed', 'data'),\n        ]\n    elif activation_partitioning_dims == 2 and parameter_partitioning_dims == 2:\n        rules = [\n            ('batch', 'data'),\n            ('vocab', 'model'),\n            ('mlp', 'model'),\n            ('heads', 'model'),\n            ('kv', None),\n            ('joined_kv', 'model'),\n            ('embed', 'model'),\n            ('embed', 'data'),\n        ]\n    else:\n        raise ValueError(\n            f'`activation_partitioning_dims` = {activation_partitioning_dims} '\n            f'`parameter_partitioning_dims` = {parameter_partitioning_dims} '\n            'is not supported.')\n\n    # Add the common rules for the replicated logical axes names.\n    replicated_rules = [\n        ('relpos_buckets', None),\n        ('abspos_buckets', None),\n        ('length', None),\n        ('layers', None),\n        ('stack', None),\n        ('mlp_activations', None),\n    ]\n    rules.extend(replicated_rules)\n\n    if additional_rules:\n        rules.extend(additional_rules)\n\n    return rules\n</code></pre>"},{"location":"generated-partition_utils-t5x_partitioning/#src.fjformer.partition_utils.t5x_partitioning.with_sharding_constraint","title":"<code>with_sharding_constraint(x, axis_resources)</code>","text":"<p>Wrapper for lax.with_sharding_constraint, no-op on cpu or outside pjit.</p> Source code in <code>src/fjformer/partition_utils/t5x_partitioning.py</code> <pre><code>def with_sharding_constraint(x, axis_resources):\n    \"\"\"Wrapper for lax.with_sharding_constraint, no-op on cpu or outside pjit.\"\"\"\n    if jax.devices()[0].platform == 'cpu' or not global_mesh_defined():\n        return x\n    else:\n        return jax.lax.with_sharding_constraint(x, axis_resources)\n</code></pre>"},{"location":"generated-utils/","title":"utils","text":""},{"location":"generated-utils/#src.fjformer.utils.GenerateRNG","title":"<code>GenerateRNG</code>","text":"Source code in <code>src/fjformer/utils.py</code> <pre><code>class GenerateRNG:\n    def __init__(self, seed: int = 0):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the initial state of the object, which in this case includes a seed and a random number generator.\n        The seed can be set by passing an argument to __init__, but if no argument is passed it defaults to 0.\n\n        :param self: Represent the instance of the class\n        :param seed: int: Set the seed for the random number generator\n        :return: The object itself\n\n        \"\"\"\n        self.seed = seed\n        self._rng = jax.random.PRNGKey(seed)\n\n    def __next__(self):\n        \"\"\"\n        The __next__ function is called by the for loop to get the next value.\n        It uses a while True loop so that it can return an infinite number of values.\n        The function splits the random number generator into two parts, one part\n        is used to generate a key and then returned, and the other part becomes\n        the new random number generator.\n\n        :param self: Represent the instance of the class\n        :return: A random number\n\n        \"\"\"\n        while True:\n            self._rng, ke = jax.random.split(self._rng, 2)\n            return ke\n\n    @property\n    def rng(self):\n        return next(self)\n</code></pre>"},{"location":"generated-utils/#src.fjformer.utils.GenerateRNG.__init__","title":"<code>__init__(seed=0)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the initial state of the object, which in this case includes a seed and a random number generator. The seed can be set by passing an argument to init, but if no argument is passed it defaults to 0.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>seed</code> <code>int</code> <p>int: Set the seed for the random number generator</p> <code>0</code> <p>Returns:</p> Type Description <p>The object itself</p> Source code in <code>src/fjformer/utils.py</code> <pre><code>def __init__(self, seed: int = 0):\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the initial state of the object, which in this case includes a seed and a random number generator.\n    The seed can be set by passing an argument to __init__, but if no argument is passed it defaults to 0.\n\n    :param self: Represent the instance of the class\n    :param seed: int: Set the seed for the random number generator\n    :return: The object itself\n\n    \"\"\"\n    self.seed = seed\n    self._rng = jax.random.PRNGKey(seed)\n</code></pre>"},{"location":"generated-utils/#src.fjformer.utils.GenerateRNG.__next__","title":"<code>__next__()</code>","text":"<p>The next function is called by the for loop to get the next value. It uses a while True loop so that it can return an infinite number of values. The function splits the random number generator into two parts, one part is used to generate a key and then returned, and the other part becomes the new random number generator.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A random number</p> Source code in <code>src/fjformer/utils.py</code> <pre><code>def __next__(self):\n    \"\"\"\n    The __next__ function is called by the for loop to get the next value.\n    It uses a while True loop so that it can return an infinite number of values.\n    The function splits the random number generator into two parts, one part\n    is used to generate a key and then returned, and the other part becomes\n    the new random number generator.\n\n    :param self: Represent the instance of the class\n    :return: A random number\n\n    \"\"\"\n    while True:\n        self._rng, ke = jax.random.split(self._rng, 2)\n        return ke\n</code></pre>"},{"location":"generated-utils/#src.fjformer.utils.JaxRNG","title":"<code>JaxRNG</code>","text":"<p>             Bases: <code>object</code></p> Source code in <code>src/fjformer/utils.py</code> <pre><code>class JaxRNG(object):\n    @classmethod\n    def from_seed(cls, seed):\n        \"\"\"\n            The from_seed function is a class method that takes a seed and returns an instance of the class.\n            This allows us to create multiple instances of the same random number generator with different seeds,\n            which can be useful for debugging or reproducibility.\n\n            :param cls: Pass the class of the object that is being created\n            :param seed: Initialize the random number generator\n            :return: An instance of the class\n\n            \"\"\"\n\n        return cls(jax.random.PRNGKey(seed))\n\n    def __init__(self, rng):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the random number generator, which will be used to generate\n        random numbers for initializing weights and biases.\n\n        :param self: Represent the instance of the class\n        :param rng: Generate random numbers\n        :return: The object itself\n\n        \"\"\"\n        self.rng = rng\n\n    def __call__(self, keys=None):\n        \"\"\"\n        The __call__ function is a special function in Python that allows an object to be called like a function.\n\n        :param self: Refer to the object itself\n        :param keys: Split the random number generator into multiple parts\n        :return: A random number generator\n\n        \"\"\"\n        if keys is None:\n            self.rng, split_rng = jax.random.split(self.rng)\n            return split_rng\n        elif isinstance(keys, int):\n            split_rngs = jax.random.split(self.rng, num=keys + 1)\n            self.rng = split_rngs[0]\n            return tuple(split_rngs[1:])\n        else:\n            split_rngs = jax.random.split(self.rng, num=len(keys) + 1)\n            self.rng = split_rngs[0]\n            return {key: val for key, val in zip(keys, split_rngs[1:])}\n</code></pre>"},{"location":"generated-utils/#src.fjformer.utils.JaxRNG.__call__","title":"<code>__call__(keys=None)</code>","text":"<p>The call function is a special function in Python that allows an object to be called like a function.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>keys</code> <p>Split the random number generator into multiple parts</p> <code>None</code> <p>Returns:</p> Type Description <p>A random number generator</p> Source code in <code>src/fjformer/utils.py</code> <pre><code>def __call__(self, keys=None):\n    \"\"\"\n    The __call__ function is a special function in Python that allows an object to be called like a function.\n\n    :param self: Refer to the object itself\n    :param keys: Split the random number generator into multiple parts\n    :return: A random number generator\n\n    \"\"\"\n    if keys is None:\n        self.rng, split_rng = jax.random.split(self.rng)\n        return split_rng\n    elif isinstance(keys, int):\n        split_rngs = jax.random.split(self.rng, num=keys + 1)\n        self.rng = split_rngs[0]\n        return tuple(split_rngs[1:])\n    else:\n        split_rngs = jax.random.split(self.rng, num=len(keys) + 1)\n        self.rng = split_rngs[0]\n        return {key: val for key, val in zip(keys, split_rngs[1:])}\n</code></pre>"},{"location":"generated-utils/#src.fjformer.utils.JaxRNG.__init__","title":"<code>__init__(rng)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the random number generator, which will be used to generate random numbers for initializing weights and biases.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>rng</code> <p>Generate random numbers</p> required <p>Returns:</p> Type Description <p>The object itself</p> Source code in <code>src/fjformer/utils.py</code> <pre><code>def __init__(self, rng):\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the random number generator, which will be used to generate\n    random numbers for initializing weights and biases.\n\n    :param self: Represent the instance of the class\n    :param rng: Generate random numbers\n    :return: The object itself\n\n    \"\"\"\n    self.rng = rng\n</code></pre>"},{"location":"generated-utils/#src.fjformer.utils.JaxRNG.from_seed","title":"<code>from_seed(seed)</code>  <code>classmethod</code>","text":"<p>The from_seed function is a class method that takes a seed and returns an instance of the class. This allows us to create multiple instances of the same random number generator with different seeds, which can be useful for debugging or reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Pass the class of the object that is being created</p> required <code>seed</code> <p>Initialize the random number generator</p> required <p>Returns:</p> Type Description <p>An instance of the class</p> Source code in <code>src/fjformer/utils.py</code> <pre><code>@classmethod\ndef from_seed(cls, seed):\n    \"\"\"\n        The from_seed function is a class method that takes a seed and returns an instance of the class.\n        This allows us to create multiple instances of the same random number generator with different seeds,\n        which can be useful for debugging or reproducibility.\n\n        :param cls: Pass the class of the object that is being created\n        :param seed: Initialize the random number generator\n        :return: An instance of the class\n\n        \"\"\"\n\n    return cls(jax.random.PRNGKey(seed))\n</code></pre>"},{"location":"generated-utils/#src.fjformer.utils.count_num_params","title":"<code>count_num_params(_p)</code>","text":"<p>The count_num_params function is a helper function that counts the number of parameters in a model. It takes as input an unfrozen parameter dictionary, and returns the total number of parameters.</p> <p>Parameters:</p> Name Type Description Default <code>_p</code> <p>Count the number of parameters in a model</p> required <p>Returns:</p> Type Description <p>The number of parameters in the model</p> Source code in <code>src/fjformer/utils.py</code> <pre><code>def count_num_params(_p):\n    \"\"\"\n    The count_num_params function is a helper function that counts the number of parameters in a model.\n    It takes as input an unfrozen parameter dictionary, and returns the total number of parameters.\n\n\n    :param _p: Count the number of parameters in a model\n    :return: The number of parameters in the model\n\n    \"\"\"\n    return sum(i.size for i in jax.tree_util.tree_flatten(flax.core.unfreeze(_p))[0])\n</code></pre>"},{"location":"generated-utils/#src.fjformer.utils.count_params","title":"<code>count_params(_p)</code>","text":"<p>The count_params function takes in a Flax model and prints out the number of parameters it contains.     Args:         _p (Flax Params]): A Flax model to count the number of parameters for.</p> <p>Parameters:</p> Name Type Description Default <code>_p</code> <p>Count the number of parameters in a model</p> required <p>Returns:</p> Type Description <p>The number of parameters in a model</p> Source code in <code>src/fjformer/utils.py</code> <pre><code>def count_params(_p):\n    \"\"\"\n    The count_params function takes in a Flax model and prints out the number of parameters it contains.\n        Args:\n            _p (Flax Params]): A Flax model to count the number of parameters for.\n\n    :param _p: Count the number of parameters in a model\n    :return: The number of parameters in a model\n\n    \"\"\"\n    print('\\033[1;31mModel Contain : ', count_num_params(_p) / 1e9, ' Billion Parameters')\n</code></pre>"},{"location":"generated-utils/#src.fjformer.utils.init_rng","title":"<code>init_rng(seed)</code>","text":"<p>The init_rng function initializes the global random number generator.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <p>Initialize the random number generator</p> required <p>Returns:</p> Type Description <p>A random number generator</p> Source code in <code>src/fjformer/utils.py</code> <pre><code>def init_rng(seed):\n    \"\"\"\n    The init_rng function initializes the global random number generator.\n\n    :param seed: Initialize the random number generator\n    :return: A random number generator\n\n    \"\"\"\n    global jax_utils_rng\n    jax_utils_rng = JaxRNG.from_seed(seed)\n</code></pre>"},{"location":"generated-utils/#src.fjformer.utils.is_torch_available","title":"<code>is_torch_available()</code>","text":"<p>The is_torch_available function checks if PyTorch is installed.</p> <p>Returns:</p> Type Description <p>True if the torch module is installed</p> Source code in <code>src/fjformer/utils.py</code> <pre><code>def is_torch_available():\n    \"\"\"\n    The is_torch_available function checks if PyTorch is installed.\n\n    :return: True if the torch module is installed\n\n    \"\"\"\n    return True if importlib.util.find_spec('torch') is not None else False\n</code></pre>"},{"location":"generated-utils/#src.fjformer.utils.next_rng","title":"<code>next_rng(*args, **kwargs)</code>","text":"<p>The next_rng function is a wrapper around jax.random.PRNGKey, which is used to generate random numbers in JAX. The next_rng function generates a new PRNGKey from the previous one, and updates the global variable jax_utils_rng with this new key.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Pass a variable number of arguments to the function</p> <code>()</code> <code>**kwargs</code> <p>Pass in a dictionary of parameters</p> <code>{}</code> <p>Returns:</p> Type Description <p>A random number generator</p> Source code in <code>src/fjformer/utils.py</code> <pre><code>def next_rng(*args, **kwargs):\n    \"\"\"\n    The next_rng function is a wrapper around jax.random.PRNGKey, which\n    is used to generate random numbers in JAX. The next_rng function\n    generates a new PRNGKey from the previous one, and updates the global\n    variable jax_utils_rng with this new key.\n\n    :param *args: Pass a variable number of arguments to the function\n    :param **kwargs: Pass in a dictionary of parameters\n    :return: A random number generator\n\n    \"\"\"\n    global jax_utils_rng\n    return jax_utils_rng(*args, **kwargs)\n</code></pre>"},{"location":"generated-xrapture-implicit_array/","title":"xrapture.implicit_array","text":""},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.ArrayValue","title":"<code>ArrayValue</code>","text":"<p>Helper class that provides a standard way to create an ABC using inheritance.</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>class ArrayValue(metaclass=ABCMeta):\n    \"\"\"Helper class that provides a standard way to create an ABC using\n    inheritance.\n    \"\"\"\n    __slots__ = ()\n    shape = None\n    e_num_val = None\n    is_registered_by_pJit = False\n</code></pre>"},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.Complement","title":"<code>Complement</code>","text":"<p>Relative complement I.e. Complement[A, B] = A - B</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>@parametric\nclass Complement(metaclass=_ComplementMeta):\n    \"\"\"\n    Relative complement\n    I.e. Complement[A, B] = A - B\n    \"\"\"\n\n    @classmethod\n    @dispatch\n    def __init_type_parameter__(\n            cls,\n            a: Optional[Any],\n            b: Optional[Any],\n    ):\n        return a, b\n\n    @classmethod\n    @dispatch\n    def __le_type_parameter__(\n            cls,\n            left: Tuple[Optional[Any], Optional[Any]],\n            right: Tuple[Optional[Any], Optional[Any]],\n    ):\n        a_left, b_left = left\n        a_right, b_right = right\n\n        return issubclass(a_left, a_right) and issubclass(b_right, b_left)\n</code></pre>"},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.ImplicitArray","title":"<code>ImplicitArray</code>  <code>dataclass</code>","text":"<p>             Bases: <code>_ImplicitArrayBase</code></p> <p>Abstract class for representing an abstract array of a given shape/dtype without actually instantiating it. Subclasses must implement the materialize method, which defines the relationship between the implicit array and the value it represents. Subclasses are valid arguments to functions decorated with qax.use_implicit_args.</p> <p>All subclasses are automatically registered as pytrees using jax.tree_util.register_pytree_with_keys_class. Any dataclass attributes added will be included as children, unless they are decorated with qax.aux_field in which case they are passed as auxiliary data during flattening.</p> <p>The represented shape and dtype may be defined in any of the following ways:     - Explicitly passing shape/dtype keyword arguments at initialization     - Overriding the default_shape/default_dtype class variables     - Overriding the compute_shape/compute_dtype methods, which are called during post_init     - Overriding post_init and manually setting shape/dtype before calling super().post_init     - None of the above, in which case an shape/dtype will be inferred by by running jax.eval_shape()       on the subclass\"s materialize method.</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>@dataclass\nclass ImplicitArray(_ImplicitArrayBase):\n    \"\"\"\n    Abstract class for representing an abstract array of a given shape/dtype without actually instantiating it.\n    Subclasses must implement the materialize method, which defines the relationship between the implicit array\n    and the value it represents. Subclasses are valid arguments to functions decorated with qax.use_implicit_args.\n\n    All subclasses are automatically registered as pytrees using jax.tree_util.register_pytree_with_keys_class.\n    Any dataclass attributes added will be included as children, unless they are decorated with qax.aux_field\n    in which case they are passed as auxiliary data during flattening.\n\n    The represented shape and dtype may be defined in any of the following ways:\n        - Explicitly passing shape/dtype keyword arguments at initialization\n        - Overriding the default_shape/default_dtype class variables\n        - Overriding the compute_shape/compute_dtype methods, which are called during __post_init__\n        - Overriding __post_init__ and manually setting shape/dtype before calling super().__post_init__\n        - None of the above, in which case an shape/dtype will be inferred by by running jax.eval_shape()\n          on the subclass\"s materialize method.\n    \"\"\"\n\n    shape = _AvalDescriptor()\n    dtype = _AvalDescriptor()\n\n    def __post_init__(self):\n        try:\n            aval = _get_materialization_aval(self)\n        except UninitializedAval:\n            # Materialization depends on currently uninitialized shape/dtype\n            aval = None\n\n        shape = None\n        try:\n            shape = self.shape\n        except UninitializedAval as e:\n            shape = self.shape = self.compute_shape()\n\n        if aval is not None:\n            if shape is None:\n                self.shape = aval.shape\n            elif shape != aval.shape:\n                warnings.warn(f\"ImplicitArray shape {shape} does not match materialization shape {aval.shape}\")\n        elif shape is None:\n            raise UninitializedAval(\"shape\")\n\n        dtype = None\n        try:\n            dtype = self.dtype\n        except UninitializedAval as e:\n            dtype = self.dtype = self.compute_dtype()\n\n        if dtype is None and aval is None:\n            # We have a shape but not a dtype, try once again to infer the dtype\n            aval = _get_materialization_aval(self)\n\n        if aval is not None:\n            if dtype is None:\n                self.dtype = aval.dtype\n            elif dtype != aval.dtype:\n                warnings.warn(f\"ImplicitArray dtype {dtype} does not match materialization dtype {aval.dtype}\")\n        elif dtype is None:\n            raise UninitializedAval(\"dtype\")\n\n    def compute_shape(self):\n        \"\"\"\n        Override this method if the subclass instance\"s shape should be computed based on its other properties.\n        Returns: shape\n        \"\"\"\n        return self.default_shape\n\n    def compute_dtype(self):\n        \"\"\"\n        Override this method if the subclass instance\"s dtype should be computed based on its other properties.\n        Returns: dtype\n        \"\"\"\n        return self.default_dtype\n\n    @property\n    def aval(self):\n        return core.ShapedArray(self.shape, self.dtype)\n\n    @classmethod\n    def default_handler(cls, primitive, *args, params=None):\n        if params is None:\n            params = {}\n        return materialize_handler(primitive, *args, params=params)\n\n    @abstractmethod\n    def materialize(self):\n        pass\n\n    def tree_flatten_with_keys(self):\n        children = []\n        aux_data = []\n        for name, is_aux in _get_names_and_aux(self):\n            try:\n                value = getattr(self, name)\n            except UninitializedAval:\n                if not _aval_discovery.get():\n                    raise\n                value = None\n            if is_aux:\n                aux_data.append(value)\n            else:\n                children.append((name, value))\n\n        return children, aux_data\n\n    @classmethod\n    def tree_unflatten(cls, aux_data, children):\n        child_it = iter(children)\n        aux_it = iter(aux_data)\n        obj = cls.__new__(cls)\n        for name, is_aux in _get_names_and_aux(cls):\n            value = next(aux_it if is_aux else child_it)\n            setattr(obj, name, value)\n\n        return obj\n\n    def handle_primitive(self, primitive, *args, params):\n        handler = lu.wrap_init(partial(get_primitive_handler(primitive), primitive))\n        use_params = params\n\n        if len(args) == 2 and self.commute_ops:\n            args, use_params = _maybe_swap_args(primitive.name, args, use_params)\n\n        # maybe_kwargs = {\"params\": params} if params else {}\n        flat_args, in_tree = flatten_one_implicit_layer((args, params))\n        flat_handler, out_tree = flatten_fun(handler, in_tree)\n\n        result = use_implicit_args(flat_handler.call_wrapped)(*flat_args)\n        return jax.tree_util.tree_unflatten(out_tree(), result)\n\n    def __init_subclass__(cls, commute_ops=True, **kwargs):\n        super().__init_subclass__(**kwargs)\n\n        if not is_dataclass(cls):\n            raise TypeError(f\"{cls.__name__} must be a dataclass\")\n        core.pytype_aval_mappings[cls] = lambda x: x.aval\n        register_pytree_with_keys_class(cls)\n        return cls\n</code></pre>"},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.ImplicitArray.compute_dtype","title":"<code>compute_dtype()</code>","text":"<p>Override this method if the subclass instance\"s dtype should be computed based on its other properties. Returns: dtype</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>def compute_dtype(self):\n    \"\"\"\n    Override this method if the subclass instance\"s dtype should be computed based on its other properties.\n    Returns: dtype\n    \"\"\"\n    return self.default_dtype\n</code></pre>"},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.ImplicitArray.compute_shape","title":"<code>compute_shape()</code>","text":"<p>Override this method if the subclass instance\"s shape should be computed based on its other properties. Returns: shape</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>def compute_shape(self):\n    \"\"\"\n    Override this method if the subclass instance\"s shape should be computed based on its other properties.\n    Returns: shape\n    \"\"\"\n    return self.default_shape\n</code></pre>"},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.apply_updates","title":"<code>apply_updates(params, updates)</code>","text":"<p>Like optax.apply_updates, but updates can be SymbolicConstant instances</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>def apply_updates(params: optax.Params, updates: optax.Updates) -&gt; optax.Params:\n    \"\"\"\n    Like optax.apply_updates, but updates can be SymbolicConstant instances\n    \"\"\"\n    updates_flat, update_struct = tree_util.tree_flatten(updates, is_leaf=lambda x: isinstance(x, SymbolicConstant))\n    semi_flat_params = update_struct.flatten_up_to(params)\n\n    updated_flat = use_implicit_args(optax.apply_updates)(semi_flat_params, updates_flat)\n    updated = update_struct.unflatten(updated_flat)\n    return updated\n</code></pre>"},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.freeze_subtrees","title":"<code>freeze_subtrees(optimizer, label_fn, use_scalar_zeros=False)</code>","text":"<p>Utility which wraps an optimizer such that subtrees specified by label_fn will receive zeros as updates. Subtrees to be frozen should be labeled with \"freeze\" and all other subtrees should be labeled with \"train\"</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>def freeze_subtrees(optimizer: optax.GradientTransformation, label_fn, use_scalar_zeros=False):\n    \"\"\"\n    Utility which wraps an optimizer such that subtrees specified by\n    label_fn will receive zeros as updates.\n    Subtrees to be frozen should be labeled with \"freeze\"\n    and all other subtrees should be labeled with \"train\"\n    \"\"\"\n    multi_transformed_optimizer = optax.multi_transform(\n        {\n            'freeze': set_to_zero_scalar() if use_scalar_zeros else optax.set_to_zero(),\n            'train': optimizer\n        },\n        label_fn\n    )\n\n    def new_update(grads, opt_state, params):\n        def map_float0(param, grad):\n            if grad.dtype == float0:\n                return jnp.zeros((), param.dtype) if use_scalar_zeros else jnp.zeros_like(param)\n            return grad\n\n        fixed_grads = jax.tree_map(map_float0, params, grads)\n        return multi_transformed_optimizer.update(fixed_grads, opt_state, params)\n\n    return optax.GradientTransformation(\n        multi_transformed_optimizer.init,\n        new_update\n    )\n</code></pre>"},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.get_common_prefix_transforms","title":"<code>get_common_prefix_transforms(trees)</code>","text":"<p>Given an iterable of pytrees which have the same structure after all ImplicitArray instances are materialized, return a list of callables which will transform each tree into the largest common structure obtainable via materialization of ImplicitArrays.</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>def get_common_prefix_transforms(trees):\n    \"\"\"\n    Given an iterable of pytrees which have the same structure after all\n    ImplicitArray instances are materialized, return a list of callables\n    which will transform each tree into the largest common structure\n    obtainable via materialization of ImplicitArrays.\n    \"\"\"\n    if len(trees) &lt;= 1:\n        return [lambda x: x for _ in trees]\n\n    all_leaves, structures = zip(*(tree_flatten_with_implicit(tree) for tree in trees))\n    post_materialization_avals = [core.get_aval(leaf) for leaf in all_leaves[0]]\n    for i, (leaves, structure) in enumerate(zip(all_leaves[1:], structures[1:]), 1):\n        if structure != structures[0]:\n            raise ValueError('Trees do not have the same structure after materialization')\n\n        for leaf, expected_aval in zip(leaves, post_materialization_avals):\n            aval = core.get_aval(leaf)\n            if not (aval.shape == expected_aval.shape and aval.dtype == expected_aval.dtype):\n                raise ValueError(\n                    f'Trees do not have the same avals after materialization. Tree 0: {expected_aval}, Tree {i}: {aval}'\n                )\n\n    # Stack will contain tuples of (path, nodes)\n    # path = a sequence of integers specifying which child\n    # was taken at each _flatten_one_implicit_layer call\n    # or the first flatten_with_implicit call\n    # nodes = one node from each tree\n    stack = []\n\n    all_leaves = []\n    for tree in trees:\n        all_leaves.append(tree_leaves_with_implicit(tree))\n\n    for i, nodes in enumerate(zip(*all_leaves)):\n        stack.append(((i,), nodes))\n\n    materialization_paths = set()\n    while stack:\n        path_prefix, nodes = stack.pop()\n        if not any(isinstance(node, ImplicitArray) for node in nodes):\n            continue\n\n        all_leaves, all_structures = zip(*(\n            flatten_one_implicit_layer(node) for node in nodes\n        ))\n        node_structures = set(all_structures)\n        if len(node_structures) &gt; 1:\n            materialization_paths.add(path_prefix)\n            continue\n\n        aval_diff = False\n        for leaves in zip(*all_leaves):\n            first_aval = core.get_aval(leaves[0])\n            shape = first_aval.shape\n            dtype = first_aval.dtype\n            for leaf in leaves[1:]:\n                aval = core.get_aval(leaf)\n                if not (aval.shape == shape and aval.dtype == dtype):\n                    materialization_paths.add(path_prefix)\n                    aval_diff = True\n            if aval_diff:\n                break\n\n        if aval_diff:\n            continue\n\n        for i, leaf_group in enumerate(zip(*all_leaves)):\n            stack.append((path_prefix + (i,), leaf_group))\n\n    return [_get_pruning_transform(tree, materialization_paths) for tree in trees]\n</code></pre>"},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.materialize_nested","title":"<code>materialize_nested(implicit_arr, full=False)</code>","text":"<p>Materialize an ImplicitArray instance, handling the case where implicit_arr.materialize() involves further ImplicitArray instances. Arguments:     implicit_arr: An ImplicitArray instance     full: If True, repeatedly materialize until the result is a concrete array Returns:     The materialized array</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>def materialize_nested(implicit_arr, full=False):\n    \"\"\"\n    Materialize an ImplicitArray instance, handling the case where implicit_arr.materialize()\n    involves further ImplicitArray instances.\n    Arguments:\n        implicit_arr: An ImplicitArray instance\n        full: If True, repeatedly materialize until the result is a concrete array\n    Returns:\n        The materialized array\n    \"\"\"\n    while isinstance(implicit_arr, ImplicitArray):\n        wrapped = lu.wrap_init(type(implicit_arr).materialize)\n        flat, in_tree = flatten_one_implicit_layer((implicit_arr,))\n        flat_fn, out_tree = flatten_fun_nokwargs(wrapped, in_tree)\n        out_flat = use_implicit_args(flat_fn.call_wrapped)(*flat)\n        implicit_arr = jax.tree_util.tree_unflatten(out_tree(), out_flat)\n\n        if not full:\n            break\n\n    return implicit_arr\n</code></pre>"},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.set_to_zero_scalar","title":"<code>set_to_zero_scalar()</code>","text":"<p>Returns a gradient transformation that sets all gradients to 0 in order to make downstream constant folding cheaper.</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>def set_to_zero_scalar() -&gt; optax.GradientTransformation:\n    \"\"\"\n    Returns a gradient transformation that sets all gradients to 0 in order to\n    make downstream constant folding cheaper.\n    \"\"\"\n\n    def init_fn(params):\n        del params\n        return optax.EmptyState()\n\n    def update_fn(updates, state, params=None):\n        return jax.tree_map(lambda x: jnp.zeros((), x.dtype), updates), state\n\n    return optax.GradientTransformation(init_fn, update_fn)\n</code></pre>"},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.use_implicit_args","title":"<code>use_implicit_args(f)</code>","text":"<p>Decorator which allows a function to accept arguments which subclass ImplicitArray, possibly including further ImplicitArray instances as children. Any number of arguments (including 0) may be ImplicitArrays.</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>def use_implicit_args(f):\n    \"\"\"\n    Decorator which allows a function to accept arguments which subclass ImplicitArray, possibly\n    including further ImplicitArray instances as children.\n    Any number of arguments (including 0) may be ImplicitArrays.\n    \"\"\"\n\n    @wraps(f)\n    def implicit_f(*args, **kwargs):\n        flat_args, in_tree = tree_flatten_with_implicit((args, kwargs))\n        f_flat, out_tree = flatten_fun(lu.wrap_init(f), in_tree)\n        f_wrapped = _with_implicit_flat(f_flat)\n        outs_flat = f_wrapped.call_wrapped(*flat_args)\n        return out_tree().unflatten(outs_flat)\n\n    return implicit_f\n</code></pre>"},{"location":"generated-xrapture-implicit_array/#src.fjformer.xrapture.implicit_array.vmap_all_but_one","title":"<code>vmap_all_but_one(f, axis, out_ndim=0)</code>","text":"<p>Repeatedly calls vmap to map over all axes except for <code>axis.</code> All args will be mapped on the same dimensions.</p> Source code in <code>src/fjformer/xrapture/implicit_array.py</code> <pre><code>def vmap_all_but_one(f, axis, out_ndim=0):\n    \"\"\"\n    Repeatedly calls vmap to map over all axes except for `axis.`\n    All args will be mapped on the same dimensions.\n    \"\"\"\n\n    @wraps(f)\n    def inner(*args):\n        n_dim = args[0].ndim\n        if axis &gt;= n_dim:\n            raise ValueError(f'Axis {axis} is out of bounds for array of dimension {n_dim}')\n        fn = f\n        vmap_dim = 1\n        out_dim = out_ndim\n        for i in reversed(range(n_dim)):\n            if i == axis:\n                vmap_dim = 0\n                out_dim = 0\n            else:\n                fn = jax.vmap(fn, vmap_dim, out_dim)\n        return fn(*args)\n\n    return inner\n</code></pre>"},{"location":"generated-xrapture-tracer/","title":"xrapture.tracer","text":""},{"location":"generated-xrapture-tracer/#src.fjformer.xrapture.tracer.ImplicitArrayTrace","title":"<code>ImplicitArrayTrace</code>","text":"<p>             Bases: <code>Trace</code></p> Source code in <code>src/fjformer/xrapture/tracer.py</code> <pre><code>class ImplicitArrayTrace(Trace):\n    pure = lift = lambda self, val: ImplicitArrayTracer(self, val)\n\n    def process_primitive(self, primitive, tracers, params):\n\n        \"\"\"\n        The process_primitive function is called by the tracer when it encounters a primitive.\n        The function should return a list of Tracers, which will be used to replace the original\n        Tracers in the trace. The process_primitive function can also modify params, which are\n        the parameters passed to the primitive.\n\n        :param self: Access the class attributes\n        :param primitive: Identify the primitive operation\n        :param tracers: Trace the value of each input to a primitive\n        :param params: Pass in the parameters of the function\n        :return: The primitive, tracers and params\n\n        \"\"\"\n        vals = [t.value for t in tracers]\n        n_implicit = sum(isinstance(v, ImplicitArray) for v in vals)\n        assert 1 &lt;= n_implicit &lt;= 2\n        if n_implicit == 2:\n            warnings.warn(f'Encountered op {primitive.name} with two implicit inputs so second will be materialized.')\n            vals[1] = vals[1].materialize()\n</code></pre>"},{"location":"generated-xrapture-tracer/#src.fjformer.xrapture.tracer.ImplicitArrayTrace.process_primitive","title":"<code>process_primitive(primitive, tracers, params)</code>","text":"<p>The process_primitive function is called by the tracer when it encounters a primitive. The function should return a list of Tracers, which will be used to replace the original Tracers in the trace. The process_primitive function can also modify params, which are the parameters passed to the primitive.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the class attributes</p> required <code>primitive</code> <p>Identify the primitive operation</p> required <code>tracers</code> <p>Trace the value of each input to a primitive</p> required <code>params</code> <p>Pass in the parameters of the function</p> required <p>Returns:</p> Type Description <p>The primitive, tracers and params</p> Source code in <code>src/fjformer/xrapture/tracer.py</code> <pre><code>def process_primitive(self, primitive, tracers, params):\n\n    \"\"\"\n    The process_primitive function is called by the tracer when it encounters a primitive.\n    The function should return a list of Tracers, which will be used to replace the original\n    Tracers in the trace. The process_primitive function can also modify params, which are\n    the parameters passed to the primitive.\n\n    :param self: Access the class attributes\n    :param primitive: Identify the primitive operation\n    :param tracers: Trace the value of each input to a primitive\n    :param params: Pass in the parameters of the function\n    :return: The primitive, tracers and params\n\n    \"\"\"\n    vals = [t.value for t in tracers]\n    n_implicit = sum(isinstance(v, ImplicitArray) for v in vals)\n    assert 1 &lt;= n_implicit &lt;= 2\n    if n_implicit == 2:\n        warnings.warn(f'Encountered op {primitive.name} with two implicit inputs so second will be materialized.')\n        vals[1] = vals[1].materialize()\n</code></pre>"},{"location":"generated-xrapture-tracer/#src.fjformer.xrapture.tracer.ImplicitArrayTracer","title":"<code>ImplicitArrayTracer</code>","text":"<p>             Bases: <code>Tracer</code></p> Source code in <code>src/fjformer/xrapture/tracer.py</code> <pre><code>class ImplicitArrayTracer(Tracer):\n    def __init__(self, trace, value):\n\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the object with all of its properties and methods.\n        The self parameter refers to the instance of the object itself.\n\n        :param self: Refer to the instance of the class\n        :param trace: Store the traceback object, which is used to print out a stack trace\n        :param value: Store the value of the exception\n        :return: The value of the class\n        \"\"\"\n        super().__init__(trace)\n        self.value = value\n\n    @property\n    def aval(self):\n\n        \"\"\"\n        The aval function is used to determine the shape and dtype of a value.\n\n        :param self: Refer to the object itself\n        :return: The aval of the value\n\n        \"\"\"\n        if isinstance(self.value, ImplicitArray):\n            return jax.ShapedArray(self.value.shape, self.value.dtype)\n        return get_aval(self.value)\n\n    def full_lower(self):\n\n        \"\"\"\n        The full_lower function is used to convert an expression into a form that can be\n           evaluated by the SymPy lambdify function.  The full_lower function will recursively\n           descend through the expression tree and replace any instances of ImplicitArray with\n           their value attribute.  This allows for expressions like:\n\n        :param self: Refer to the current object\n        :return: An implicitarray object\n        \"\"\"\n        if isinstance(self.value, ImplicitArray):\n            return self\n\n        return full_lower(self.value)\n</code></pre>"},{"location":"generated-xrapture-tracer/#src.fjformer.xrapture.tracer.ImplicitArrayTracer.aval","title":"<code>aval</code>  <code>property</code>","text":"<p>The aval function is used to determine the shape and dtype of a value.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <p>Returns:</p> Type Description <p>The aval of the value</p>"},{"location":"generated-xrapture-tracer/#src.fjformer.xrapture.tracer.ImplicitArrayTracer.__init__","title":"<code>__init__(trace, value)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the object with all of its properties and methods. The self parameter refers to the instance of the object itself.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <code>trace</code> <p>Store the traceback object, which is used to print out a stack trace</p> required <code>value</code> <p>Store the value of the exception</p> required <p>Returns:</p> Type Description <p>The value of the class</p> Source code in <code>src/fjformer/xrapture/tracer.py</code> <pre><code>def __init__(self, trace, value):\n\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the object with all of its properties and methods.\n    The self parameter refers to the instance of the object itself.\n\n    :param self: Refer to the instance of the class\n    :param trace: Store the traceback object, which is used to print out a stack trace\n    :param value: Store the value of the exception\n    :return: The value of the class\n    \"\"\"\n    super().__init__(trace)\n    self.value = value\n</code></pre>"},{"location":"generated-xrapture-tracer/#src.fjformer.xrapture.tracer.ImplicitArrayTracer.full_lower","title":"<code>full_lower()</code>","text":"<p>The full_lower function is used to convert an expression into a form that can be    evaluated by the SymPy lambdify function.  The full_lower function will recursively    descend through the expression tree and replace any instances of ImplicitArray with    their value attribute.  This allows for expressions like:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current object</p> required <p>Returns:</p> Type Description <p>An implicitarray object</p> Source code in <code>src/fjformer/xrapture/tracer.py</code> <pre><code>def full_lower(self):\n\n    \"\"\"\n    The full_lower function is used to convert an expression into a form that can be\n       evaluated by the SymPy lambdify function.  The full_lower function will recursively\n       descend through the expression tree and replace any instances of ImplicitArray with\n       their value attribute.  This allows for expressions like:\n\n    :param self: Refer to the current object\n    :return: An implicitarray object\n    \"\"\"\n    if isinstance(self.value, ImplicitArray):\n        return self\n\n    return full_lower(self.value)\n</code></pre>"},{"location":"generated-xrapture-xrapture/","title":"xrapture.xrapture","text":""},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.LoraWeight","title":"<code>LoraWeight</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ImplicitArray</code></p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>@dataclass\nclass LoraWeight(ImplicitArray):\n    w: ArrayValue  # M x N\n    a: ArrayValue  # k x N\n    b: ArrayValue  # M x k\n\n    alpha: float = aux_field(default=1.)\n\n    def __post_init__(self):\n        \"\"\"\n        The __post_init__ function is called after the __init__ function.\n        It allows us to check that the shapes of our parameters are correct, and if not, raise an error.\n\n        :param self: Represent the instance of the class\n        :return: The output of the super()\n\n        \"\"\"\n        super().__post_init__()\n        assert self.a.shape[-2] == self.b.shape[-1], \"A and B Array should be like a[K,N] B[M,K]\"\n        assert self.w.shape[-2] == self.b.shape[-2], \"A and B Array should be like W[M,N] B[M,K]\"\n        assert self.w.shape[-1] == self.a.shape[-1], \"A and B Array should be like W[M,N] A[K,N]\"\n\n    def materialize(self):\n        \"\"\"\n        The materialize function is used to create a new matrix from the parameters of the factorization.\n\n        :param self: Access the attributes and methods of a class\n        :return: The materialized vector\n\n        \"\"\"\n        return (self.w + self.get_scale() * self.b @ self.a).astype(self.w.dtype)\n\n    def get_scale(self):\n        \"\"\"\n        The get_scale function returns the scale of the model.\n        The scale is defined as alpha / number of columns in b.\n\n\n        :param self: Represent the instance of the class\n        :return: The scale of the model\n\n        \"\"\"\n        return self.alpha / self.b.shape[1]\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.LoraWeight.__post_init__","title":"<code>__post_init__()</code>","text":"<p>The post_init function is called after the init function. It allows us to check that the shapes of our parameters are correct, and if not, raise an error.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>The output of the super()</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    The __post_init__ function is called after the __init__ function.\n    It allows us to check that the shapes of our parameters are correct, and if not, raise an error.\n\n    :param self: Represent the instance of the class\n    :return: The output of the super()\n\n    \"\"\"\n    super().__post_init__()\n    assert self.a.shape[-2] == self.b.shape[-1], \"A and B Array should be like a[K,N] B[M,K]\"\n    assert self.w.shape[-2] == self.b.shape[-2], \"A and B Array should be like W[M,N] B[M,K]\"\n    assert self.w.shape[-1] == self.a.shape[-1], \"A and B Array should be like W[M,N] A[K,N]\"\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.LoraWeight.get_scale","title":"<code>get_scale()</code>","text":"<p>The get_scale function returns the scale of the model. The scale is defined as alpha / number of columns in b.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>The scale of the model</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>def get_scale(self):\n    \"\"\"\n    The get_scale function returns the scale of the model.\n    The scale is defined as alpha / number of columns in b.\n\n\n    :param self: Represent the instance of the class\n    :return: The scale of the model\n\n    \"\"\"\n    return self.alpha / self.b.shape[1]\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.LoraWeight.materialize","title":"<code>materialize()</code>","text":"<p>The materialize function is used to create a new matrix from the parameters of the factorization.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the attributes and methods of a class</p> required <p>Returns:</p> Type Description <p>The materialized vector</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>def materialize(self):\n    \"\"\"\n    The materialize function is used to create a new matrix from the parameters of the factorization.\n\n    :param self: Access the attributes and methods of a class\n    :return: The materialized vector\n\n    \"\"\"\n    return (self.w + self.get_scale() * self.b @ self.a).astype(self.w.dtype)\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.XRapTure","title":"<code>XRapTure</code>","text":"Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>class XRapTure:\n    def __init__(\n            self,\n            config: XRapTureConfig\n    ):\n        self.config = config\n\n    @staticmethod\n    def merge_parameters(\n            lora_parameters,\n            destructive=True,\n    ):\n\n        \"\"\"    \n        The merge_parameters function is used to convert a LoraWeight into an array.\n\n        :param lora_parameters: Pass in the parameters of the model\n        :param destructive: Determine whether to delete the original parameters or not\n        :param : Determine if the function is destructive or not\n        :return: The parameters of the model\n\n        \"\"\"\n\n        def _ensure_delete(val):\n            if not isinstance(val, jax.Array) or val.is_deleted():\n                return\n            try:\n                val.device_buffer.delete()\n            except ValueError:\n                val.device_buffers.delete()\n\n        materialize = jax.jit(materialize_nested, donate_argnums=0 if destructive else ())\n\n        def map_fn(param):\n            if isinstance(param, LoraWeight):\n                result = materialize(param)\n                if destructive:\n                    jax.tree_map(_ensure_delete, param)\n                return result\n            return param\n\n        return tree_map_with_implicit(map_fn, lora_parameters)\n\n    def base_decision_function(\n            self,\n            path: list[jax.tree_util.DictKey],\n            params: Optional[Union[dict, jax.tree_util.PyTreeDef]] = None\n    ):\n\n        \"\"\"    \n        The base_decision_function function is used to determine which parameters are frozen,\n        which are fine-tuned with LoRA, and which are fully fine-tuned. The function takes in a path\n        to the parameter (e.g., &amp;quot;model/dense_layer/kernel&amp;quot;) and returns an integer indicating how \n        the parameter should be treated:\n\n        :param self: Refer to the object itself\n        :param path: list[jax.tree_util.DictKey]: Determine the path of the parameter in question\n        :param params: dict | jax.tree_util.PyTreeDef | None: Specify the parameters of the model\n        :return: The following:\n\n        \"\"\"\n        if self.config.fully_fine_tune_parameters is not None:\n            for param_name in self.config.fully_fine_tune_parameters:\n                if jax.tree_util.DictKey(key=param_name) in path:\n                    if self.config.verbose:\n                        print(\n                            termcolor.colored(\n                                f\"Parameter\"\n                                f\" {'/'.join(str(n.key) for n in path)} \"\n                                f\"Selected for Fully Fine-Tune.\",\n                                color=\"cyan\",\n                                force_color=True\n                            )\n                        )\n                    return LORA_FULL\n\n        if self.config.lora_fine_tune_parameters is not None:\n            for param_name in self.config.lora_fine_tune_parameters:\n                if jax.tree_util.DictKey(key=param_name) in path:\n                    if self.config.verbose:\n                        print(\n                            termcolor.colored(\n                                f\"Parameter\"\n                                f\" {'/'.join(str(n.key) for n in path)} \"\n                                f\"Selected for LoRA Fine-Tune with {self.config.lora_dim} dimensions.\",\n                                color=\"cyan\",\n                                force_color=True\n                            )\n                        )\n                    return self.config.lora_dim\n\n        return LORA_FREEZE\n\n    def make_lora_specs(\n            self,\n            parameters: Union[dict, flax.core.FrozenDict],\n            decision_fn: Optional[Callable] = None,\n            tune_vectors: bool = False,\n    ):\n\n        \"\"\"    \n        The make_lora_specs function is used to create a dictionary of LORA specs for the parameters\n        of a model. The function takes in two arguments:\n\n        :param self: Allow the function to access other attributes and methods of the class\n        :param parameters: dict | flax.core.FrozenDict: Specify the parameters to be tuned\n        :param decision_fn: Optional[Callable]: Decide whether to freeze or unfreeze a parameter\n        :param tune_vectors: bool: Determine if the vectors should be tuned or not\n        :param : Decide whether to freeze the parameter or not\n        :return: A dictionary of the same shape as the input parameters,\n\n        \"\"\"\n        decision_fn = decision_fn if decision_fn is not None else self.base_decision_function\n\n        if decision_fn is None:\n            def decision_fn(*args):\n                return LORA_FREEZE\n\n        def full_fn(path, arr):\n            if len(arr.shape) &lt; 2:\n                return LORA_FULL if tune_vectors else LORA_FREEZE\n\n            value = decision_fn(path, arr)\n            return value\n\n        return jax.tree_util.tree_map_with_path(full_fn, parameters, is_leaf=None)\n\n    @staticmethod\n    def init_lora_parameters(\n            param_tree,\n            lora_spec,\n            dtype: jnp.dtype = jnp.float32,\n            rng: jax.random.PRNGKey = jax.random.PRNGKey(0),\n            stddev: float = 0.01,\n            alpha: float = 1.,\n            is_leaf: bool = None\n    ):\n\n        \"\"\"\n        The init_lora_parameters function takes in a parameter tree, the lora_spec, and some other parameters.\n        It then iterates through the parameter tree using jax.tree_util.tree_map_with_path to get each path and value of \n        the parameter tree (which is just a nested dictionary). It then checks if that value is either LORA_FREEZE or\n        LORA_FULL (these are constants defined above). If it's one of those two values, it returns the original parameter as-is; \n        otherwise it creates a new LoraWeight object with random values for b\n\n        :param param_tree: Specify the parameters of a neural network\n        :param lora_spec: Determine how many parameters to tune\n        :param dtype: jnp.dtype: Specify the data type of the parameters\n        :param rng: jax.random.PRNGKey: Generate random numbers\n        :param stddev: float: Initialize the weights of the network\n        :param alpha: float: Control the amount of regularization\n        :param is_leaf: bool: Specify whether a node is a leaf or not\n        :return: A tree of loraweight objects\n\n        \"\"\"\n\n        def iter_keys(key):\n            while True:\n                key, out_key = jax.random.split(key)\n                yield out_key\n\n        key_it = iter_keys(rng)\n\n        def get_param(path, param, spec_val):\n            if spec_val in (LORA_FREEZE, LORA_FULL):\n                return param\n\n            if len(param.shape) == 1:\n                raise ValueError(\n                    f\"Vectors must either be frozen or fully tuned, but got \"\n                    f\"lora_spec value {lora_spec} for param with path {path}\"\n                )\n\n            if len(param.shape) == 2:\n                b_dim, a_dim = param.shape\n\n                b = jnp.zeros((b_dim, spec_val), dtype=dtype)\n                a = jax.random.normal(next(key_it), (spec_val, a_dim), dtype=dtype) * stddev\n                return LoraWeight(w=param, a=a, b=b, alpha=alpha)\n\n            # conv case\n            *window_shape, in_channels, out_channels = param.shape\n\n            a = jnp.zeros((\n                *(1 for _ in range(len(window_shape))),\n                spec_val,\n                out_channels\n            ), dtype=param.dtype)\n            b = jax.random.normal(\n                rng, (\n                    *window_shape,\n                    in_channels,\n                    spec_val\n                ), dtype=param.dtype\n            ) * stddev\n            return LoraWeight(\n                param,\n                a,\n                b,\n                alpha=alpha\n            )\n\n        return jax.tree_util.tree_map_with_path(\n            get_param,\n            param_tree,\n            lora_spec,\n            is_leaf=is_leaf\n        )\n\n    @staticmethod\n    def wrap_tx(\n            tx: optax.GradientTransformation,\n            lora_spec,\n            scalar_frozen_grads=False\n    ):\n\n        \"\"\"    \n        The wrap_tx function takes a gradient transformation and wraps it in two\n        freeze transformations. The first freezes all parameters that are marked as\n        LORA_FREEZE, which is the default for LoraWeight objects. The second freezes\n        the weights of all LoraWeight objects, regardless of their freeze status.\n\n        :param tx: optax.GradientTransformation: Pass in the optimizer\n        :param lora_spec: Specify which parameters we want to freeze\n        :param scalar_frozen_grads: Determine whether to use scalar zeros or array zeros\n        :return: A transformed version of the optimizer\n\n        \"\"\"\n        full_freeze_labels = jax.tree_map(\n            lambda x: \"freeze\" if x == LORA_FREEZE else \"train\",\n            lora_spec\n        )\n        optimizer_with_full_freeze = freeze_subtrees(\n            tx,\n            full_freeze_labels,\n            use_scalar_zeros=scalar_frozen_grads\n        )\n\n        return freeze_keys(\n            optimizer_with_full_freeze,\n            LoraWeight,\n            \"w\",\n            use_scalar_zeros=scalar_frozen_grads\n        )\n\n    def apply_lora(\n            self,\n            module: Union[Any, flax.linen.Module],\n            parameters: Union[dict, flax.core.FrozenDict],\n            tx: optax.GradientTransformation,\n            decision_fn: Optional[Callable] = None,\n            tune_vectors: bool = False,\n            rng: jax.random.PRNGKey = jax.random.PRNGKey(0),\n            stddev: float = 0.01,\n            alpha: float = 1.,\n            is_leaf: bool = None\n    ) -&gt; XRapTureModule:\n\n        \"\"\"\n        The apply_lora function is a wrapper for the XRapTureModule class.\n        It takes in a module, parameters, and an optimizer (tx) and returns an instance of the XRapTureModule class.\n        The apply_lora function also allows you to specify whether you want to tune vectors as well as\n        whether you want to use a decision function when tuning your parameters. The default behavior is that\n        vectors are tuned using LORA while scalars are tuned using SGD.\n\n        :param self: Access the attributes of the class\n        :param module: Any | flax.linen.Module: Specify the model that is being trained\n        :param parameters: dict | flax.core.FrozenDict: Define the parameters of the model\n        :param tx: optax.GradientTransformation: Specify the optimizer\n        :param decision_fn: Optional[Callable]: Decide whether to apply lora to a parameter or not\n        :param tune_vectors: bool: Determine whether to tune the vectors or not\n        :param rng: jax.random.PRNGKey: Set the random seed for the initialisation of parameters\n        :param stddev: float: Set the standard deviation of the initial weights\n        :param alpha: float: Control the variance of the gaussian distribution used to initialize\n        :param is_leaf: bool: Determine if the node is a leaf or not\n        :return: A XRaptureModule object\n        \"\"\"\n        lora_spec = self.make_lora_specs(\n            parameters=parameters,\n            tune_vectors=tune_vectors,\n            decision_fn=decision_fn\n        )\n\n        lora_parameters = self.init_lora_parameters(\n            param_tree=parameters,\n            dtype=self.config.dtype,\n            lora_spec=lora_spec,\n            rng=rng,\n            stddev=stddev,\n            alpha=alpha,\n            is_leaf=is_leaf\n        )\n        tx = self.wrap_tx(\n            tx=tx,\n            lora_spec=lora_spec\n        )\n        opt_state = tx.init(lora_parameters)\n        lora_model = use_implicit_args(\n            module\n        )\n\n        return XRapTureModule(\n            lora_opt_state=opt_state,\n            lora_module=lora_model,\n            lora_specs=lora_spec,\n            lora_parameters=lora_parameters,\n            lora_tx=tx\n        )\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.XRapTure.apply_lora","title":"<code>apply_lora(module, parameters, tx, decision_fn=None, tune_vectors=False, rng=jax.random.PRNGKey(0), stddev=0.01, alpha=1.0, is_leaf=None)</code>","text":"<p>The apply_lora function is a wrapper for the XRapTureModule class. It takes in a module, parameters, and an optimizer (tx) and returns an instance of the XRapTureModule class. The apply_lora function also allows you to specify whether you want to tune vectors as well as whether you want to use a decision function when tuning your parameters. The default behavior is that vectors are tuned using LORA while scalars are tuned using SGD.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the attributes of the class</p> required <code>module</code> <code>Union[Any, Module]</code> <p>Any | flax.linen.Module: Specify the model that is being trained</p> required <code>parameters</code> <code>Union[dict, FrozenDict]</code> <p>dict | flax.core.FrozenDict: Define the parameters of the model</p> required <code>tx</code> <code>GradientTransformation</code> <p>optax.GradientTransformation: Specify the optimizer</p> required <code>decision_fn</code> <code>Optional[Callable]</code> <p>Optional[Callable]: Decide whether to apply lora to a parameter or not</p> <code>None</code> <code>tune_vectors</code> <code>bool</code> <p>bool: Determine whether to tune the vectors or not</p> <code>False</code> <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Set the random seed for the initialisation of parameters</p> <code>PRNGKey(0)</code> <code>stddev</code> <code>float</code> <p>float: Set the standard deviation of the initial weights</p> <code>0.01</code> <code>alpha</code> <code>float</code> <p>float: Control the variance of the gaussian distribution used to initialize</p> <code>1.0</code> <code>is_leaf</code> <code>bool</code> <p>bool: Determine if the node is a leaf or not</p> <code>None</code> <p>Returns:</p> Type Description <code>XRapTureModule</code> <p>A XRaptureModule object</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>def apply_lora(\n        self,\n        module: Union[Any, flax.linen.Module],\n        parameters: Union[dict, flax.core.FrozenDict],\n        tx: optax.GradientTransformation,\n        decision_fn: Optional[Callable] = None,\n        tune_vectors: bool = False,\n        rng: jax.random.PRNGKey = jax.random.PRNGKey(0),\n        stddev: float = 0.01,\n        alpha: float = 1.,\n        is_leaf: bool = None\n) -&gt; XRapTureModule:\n\n    \"\"\"\n    The apply_lora function is a wrapper for the XRapTureModule class.\n    It takes in a module, parameters, and an optimizer (tx) and returns an instance of the XRapTureModule class.\n    The apply_lora function also allows you to specify whether you want to tune vectors as well as\n    whether you want to use a decision function when tuning your parameters. The default behavior is that\n    vectors are tuned using LORA while scalars are tuned using SGD.\n\n    :param self: Access the attributes of the class\n    :param module: Any | flax.linen.Module: Specify the model that is being trained\n    :param parameters: dict | flax.core.FrozenDict: Define the parameters of the model\n    :param tx: optax.GradientTransformation: Specify the optimizer\n    :param decision_fn: Optional[Callable]: Decide whether to apply lora to a parameter or not\n    :param tune_vectors: bool: Determine whether to tune the vectors or not\n    :param rng: jax.random.PRNGKey: Set the random seed for the initialisation of parameters\n    :param stddev: float: Set the standard deviation of the initial weights\n    :param alpha: float: Control the variance of the gaussian distribution used to initialize\n    :param is_leaf: bool: Determine if the node is a leaf or not\n    :return: A XRaptureModule object\n    \"\"\"\n    lora_spec = self.make_lora_specs(\n        parameters=parameters,\n        tune_vectors=tune_vectors,\n        decision_fn=decision_fn\n    )\n\n    lora_parameters = self.init_lora_parameters(\n        param_tree=parameters,\n        dtype=self.config.dtype,\n        lora_spec=lora_spec,\n        rng=rng,\n        stddev=stddev,\n        alpha=alpha,\n        is_leaf=is_leaf\n    )\n    tx = self.wrap_tx(\n        tx=tx,\n        lora_spec=lora_spec\n    )\n    opt_state = tx.init(lora_parameters)\n    lora_model = use_implicit_args(\n        module\n    )\n\n    return XRapTureModule(\n        lora_opt_state=opt_state,\n        lora_module=lora_model,\n        lora_specs=lora_spec,\n        lora_parameters=lora_parameters,\n        lora_tx=tx\n    )\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.XRapTure.base_decision_function","title":"<code>base_decision_function(path, params=None)</code>","text":"<p>The base_decision_function function is used to determine which parameters are frozen, which are fine-tuned with LoRA, and which are fully fine-tuned. The function takes in a path to the parameter (e.g., \"model/dense_layer/kernel\") and returns an integer indicating how  the parameter should be treated:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>path</code> <code>list[DictKey]</code> <p>list[jax.tree_util.DictKey]: Determine the path of the parameter in question</p> required <code>params</code> <code>Optional[Union[dict, PyTreeDef]]</code> <p>dict | jax.tree_util.PyTreeDef | None: Specify the parameters of the model</p> <code>None</code> <p>Returns:</p> Type Description <p>The following:</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>def base_decision_function(\n        self,\n        path: list[jax.tree_util.DictKey],\n        params: Optional[Union[dict, jax.tree_util.PyTreeDef]] = None\n):\n\n    \"\"\"    \n    The base_decision_function function is used to determine which parameters are frozen,\n    which are fine-tuned with LoRA, and which are fully fine-tuned. The function takes in a path\n    to the parameter (e.g., &amp;quot;model/dense_layer/kernel&amp;quot;) and returns an integer indicating how \n    the parameter should be treated:\n\n    :param self: Refer to the object itself\n    :param path: list[jax.tree_util.DictKey]: Determine the path of the parameter in question\n    :param params: dict | jax.tree_util.PyTreeDef | None: Specify the parameters of the model\n    :return: The following:\n\n    \"\"\"\n    if self.config.fully_fine_tune_parameters is not None:\n        for param_name in self.config.fully_fine_tune_parameters:\n            if jax.tree_util.DictKey(key=param_name) in path:\n                if self.config.verbose:\n                    print(\n                        termcolor.colored(\n                            f\"Parameter\"\n                            f\" {'/'.join(str(n.key) for n in path)} \"\n                            f\"Selected for Fully Fine-Tune.\",\n                            color=\"cyan\",\n                            force_color=True\n                        )\n                    )\n                return LORA_FULL\n\n    if self.config.lora_fine_tune_parameters is not None:\n        for param_name in self.config.lora_fine_tune_parameters:\n            if jax.tree_util.DictKey(key=param_name) in path:\n                if self.config.verbose:\n                    print(\n                        termcolor.colored(\n                            f\"Parameter\"\n                            f\" {'/'.join(str(n.key) for n in path)} \"\n                            f\"Selected for LoRA Fine-Tune with {self.config.lora_dim} dimensions.\",\n                            color=\"cyan\",\n                            force_color=True\n                        )\n                    )\n                return self.config.lora_dim\n\n    return LORA_FREEZE\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.XRapTure.init_lora_parameters","title":"<code>init_lora_parameters(param_tree, lora_spec, dtype=jnp.float32, rng=jax.random.PRNGKey(0), stddev=0.01, alpha=1.0, is_leaf=None)</code>  <code>staticmethod</code>","text":"<p>The init_lora_parameters function takes in a parameter tree, the lora_spec, and some other parameters. It then iterates through the parameter tree using jax.tree_util.tree_map_with_path to get each path and value of  the parameter tree (which is just a nested dictionary). It then checks if that value is either LORA_FREEZE or LORA_FULL (these are constants defined above). If it's one of those two values, it returns the original parameter as-is;  otherwise it creates a new LoraWeight object with random values for b</p> <p>Parameters:</p> Name Type Description Default <code>param_tree</code> <p>Specify the parameters of a neural network</p> required <code>lora_spec</code> <p>Determine how many parameters to tune</p> required <code>dtype</code> <code>dtype</code> <p>jnp.dtype: Specify the data type of the parameters</p> <code>float32</code> <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Generate random numbers</p> <code>PRNGKey(0)</code> <code>stddev</code> <code>float</code> <p>float: Initialize the weights of the network</p> <code>0.01</code> <code>alpha</code> <code>float</code> <p>float: Control the amount of regularization</p> <code>1.0</code> <code>is_leaf</code> <code>bool</code> <p>bool: Specify whether a node is a leaf or not</p> <code>None</code> <p>Returns:</p> Type Description <p>A tree of loraweight objects</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>@staticmethod\ndef init_lora_parameters(\n        param_tree,\n        lora_spec,\n        dtype: jnp.dtype = jnp.float32,\n        rng: jax.random.PRNGKey = jax.random.PRNGKey(0),\n        stddev: float = 0.01,\n        alpha: float = 1.,\n        is_leaf: bool = None\n):\n\n    \"\"\"\n    The init_lora_parameters function takes in a parameter tree, the lora_spec, and some other parameters.\n    It then iterates through the parameter tree using jax.tree_util.tree_map_with_path to get each path and value of \n    the parameter tree (which is just a nested dictionary). It then checks if that value is either LORA_FREEZE or\n    LORA_FULL (these are constants defined above). If it's one of those two values, it returns the original parameter as-is; \n    otherwise it creates a new LoraWeight object with random values for b\n\n    :param param_tree: Specify the parameters of a neural network\n    :param lora_spec: Determine how many parameters to tune\n    :param dtype: jnp.dtype: Specify the data type of the parameters\n    :param rng: jax.random.PRNGKey: Generate random numbers\n    :param stddev: float: Initialize the weights of the network\n    :param alpha: float: Control the amount of regularization\n    :param is_leaf: bool: Specify whether a node is a leaf or not\n    :return: A tree of loraweight objects\n\n    \"\"\"\n\n    def iter_keys(key):\n        while True:\n            key, out_key = jax.random.split(key)\n            yield out_key\n\n    key_it = iter_keys(rng)\n\n    def get_param(path, param, spec_val):\n        if spec_val in (LORA_FREEZE, LORA_FULL):\n            return param\n\n        if len(param.shape) == 1:\n            raise ValueError(\n                f\"Vectors must either be frozen or fully tuned, but got \"\n                f\"lora_spec value {lora_spec} for param with path {path}\"\n            )\n\n        if len(param.shape) == 2:\n            b_dim, a_dim = param.shape\n\n            b = jnp.zeros((b_dim, spec_val), dtype=dtype)\n            a = jax.random.normal(next(key_it), (spec_val, a_dim), dtype=dtype) * stddev\n            return LoraWeight(w=param, a=a, b=b, alpha=alpha)\n\n        # conv case\n        *window_shape, in_channels, out_channels = param.shape\n\n        a = jnp.zeros((\n            *(1 for _ in range(len(window_shape))),\n            spec_val,\n            out_channels\n        ), dtype=param.dtype)\n        b = jax.random.normal(\n            rng, (\n                *window_shape,\n                in_channels,\n                spec_val\n            ), dtype=param.dtype\n        ) * stddev\n        return LoraWeight(\n            param,\n            a,\n            b,\n            alpha=alpha\n        )\n\n    return jax.tree_util.tree_map_with_path(\n        get_param,\n        param_tree,\n        lora_spec,\n        is_leaf=is_leaf\n    )\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.XRapTure.make_lora_specs","title":"<code>make_lora_specs(parameters, decision_fn=None, tune_vectors=False)</code>","text":"<p>The make_lora_specs function is used to create a dictionary of LORA specs for the parameters of a model. The function takes in two arguments:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Allow the function to access other attributes and methods of the class</p> required <code>parameters</code> <code>Union[dict, FrozenDict]</code> <p>dict | flax.core.FrozenDict: Specify the parameters to be tuned</p> required <code>decision_fn</code> <code>Optional[Callable]</code> <p>Optional[Callable]: Decide whether to freeze or unfreeze a parameter</p> <code>None</code> <code>tune_vectors</code> <code>bool</code> <p>bool: Determine if the vectors should be tuned or not</p> <code>False</code> <code></code> <p>Decide whether to freeze the parameter or not</p> required <p>Returns:</p> Type Description <p>A dictionary of the same shape as the input parameters,</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>def make_lora_specs(\n        self,\n        parameters: Union[dict, flax.core.FrozenDict],\n        decision_fn: Optional[Callable] = None,\n        tune_vectors: bool = False,\n):\n\n    \"\"\"    \n    The make_lora_specs function is used to create a dictionary of LORA specs for the parameters\n    of a model. The function takes in two arguments:\n\n    :param self: Allow the function to access other attributes and methods of the class\n    :param parameters: dict | flax.core.FrozenDict: Specify the parameters to be tuned\n    :param decision_fn: Optional[Callable]: Decide whether to freeze or unfreeze a parameter\n    :param tune_vectors: bool: Determine if the vectors should be tuned or not\n    :param : Decide whether to freeze the parameter or not\n    :return: A dictionary of the same shape as the input parameters,\n\n    \"\"\"\n    decision_fn = decision_fn if decision_fn is not None else self.base_decision_function\n\n    if decision_fn is None:\n        def decision_fn(*args):\n            return LORA_FREEZE\n\n    def full_fn(path, arr):\n        if len(arr.shape) &lt; 2:\n            return LORA_FULL if tune_vectors else LORA_FREEZE\n\n        value = decision_fn(path, arr)\n        return value\n\n    return jax.tree_util.tree_map_with_path(full_fn, parameters, is_leaf=None)\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.XRapTure.merge_parameters","title":"<code>merge_parameters(lora_parameters, destructive=True)</code>  <code>staticmethod</code>","text":"<p>The merge_parameters function is used to convert a LoraWeight into an array.</p> <p>Parameters:</p> Name Type Description Default <code>lora_parameters</code> <p>Pass in the parameters of the model</p> required <code>destructive</code> <p>Determine whether to delete the original parameters or not</p> <code>True</code> <code></code> <p>Determine if the function is destructive or not</p> required <p>Returns:</p> Type Description <p>The parameters of the model</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>@staticmethod\ndef merge_parameters(\n        lora_parameters,\n        destructive=True,\n):\n\n    \"\"\"    \n    The merge_parameters function is used to convert a LoraWeight into an array.\n\n    :param lora_parameters: Pass in the parameters of the model\n    :param destructive: Determine whether to delete the original parameters or not\n    :param : Determine if the function is destructive or not\n    :return: The parameters of the model\n\n    \"\"\"\n\n    def _ensure_delete(val):\n        if not isinstance(val, jax.Array) or val.is_deleted():\n            return\n        try:\n            val.device_buffer.delete()\n        except ValueError:\n            val.device_buffers.delete()\n\n    materialize = jax.jit(materialize_nested, donate_argnums=0 if destructive else ())\n\n    def map_fn(param):\n        if isinstance(param, LoraWeight):\n            result = materialize(param)\n            if destructive:\n                jax.tree_map(_ensure_delete, param)\n            return result\n        return param\n\n    return tree_map_with_implicit(map_fn, lora_parameters)\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.XRapTure.wrap_tx","title":"<code>wrap_tx(tx, lora_spec, scalar_frozen_grads=False)</code>  <code>staticmethod</code>","text":"<p>The wrap_tx function takes a gradient transformation and wraps it in two freeze transformations. The first freezes all parameters that are marked as LORA_FREEZE, which is the default for LoraWeight objects. The second freezes the weights of all LoraWeight objects, regardless of their freeze status.</p> <p>Parameters:</p> Name Type Description Default <code>tx</code> <code>GradientTransformation</code> <p>optax.GradientTransformation: Pass in the optimizer</p> required <code>lora_spec</code> <p>Specify which parameters we want to freeze</p> required <code>scalar_frozen_grads</code> <p>Determine whether to use scalar zeros or array zeros</p> <code>False</code> <p>Returns:</p> Type Description <p>A transformed version of the optimizer</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>@staticmethod\ndef wrap_tx(\n        tx: optax.GradientTransformation,\n        lora_spec,\n        scalar_frozen_grads=False\n):\n\n    \"\"\"    \n    The wrap_tx function takes a gradient transformation and wraps it in two\n    freeze transformations. The first freezes all parameters that are marked as\n    LORA_FREEZE, which is the default for LoraWeight objects. The second freezes\n    the weights of all LoraWeight objects, regardless of their freeze status.\n\n    :param tx: optax.GradientTransformation: Pass in the optimizer\n    :param lora_spec: Specify which parameters we want to freeze\n    :param scalar_frozen_grads: Determine whether to use scalar zeros or array zeros\n    :return: A transformed version of the optimizer\n\n    \"\"\"\n    full_freeze_labels = jax.tree_map(\n        lambda x: \"freeze\" if x == LORA_FREEZE else \"train\",\n        lora_spec\n    )\n    optimizer_with_full_freeze = freeze_subtrees(\n        tx,\n        full_freeze_labels,\n        use_scalar_zeros=scalar_frozen_grads\n    )\n\n    return freeze_keys(\n        optimizer_with_full_freeze,\n        LoraWeight,\n        \"w\",\n        use_scalar_zeros=scalar_frozen_grads\n    )\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.eval_lora_transpose","title":"<code>eval_lora_transpose(primitive, arg, *, permutation)</code>","text":"<p>The eval_lora_transpose function is used to transpose a LoraWeight object.</p> <p>Parameters:</p> Name Type Description Default <code>primitive</code> <p>Determine which function to use</p> required <code>arg</code> <code>LoraWeight</code> <p>LoraWeight: Specify the type of input that is expected</p> required <code>*</code> <p>Indicate that the permutation parameter is a keyword-only argument</p> required <code>permutation</code> <p>Specify the permutation of the weights</p> required <p>Returns:</p> Type Description <p>A loraweight object with the same</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>@primitive_handler(\"transpose\")\ndef eval_lora_transpose(\n        primitive,\n        arg: LoraWeight,\n        *,\n        permutation\n):\n    \"\"\"\n    The eval_lora_transpose function is used to transpose a LoraWeight object.\n\n    :param primitive: Determine which function to use\n    :param arg: LoraWeight: Specify the type of input that is expected\n    :param *: Indicate that the permutation parameter is a keyword-only argument\n    :param permutation: Specify the permutation of the weights\n    :return: A loraweight object with the same\n\n    \"\"\"\n    if not len(arg.shape) == 2 and permutation == (1, 0):\n        return NotImplemented\n\n    return LoraWeight(\n        w=arg.w.T,\n        a=arg.b.T,\n        b=arg.a.T,\n        alpha=arg.alpha,\n    )\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.handle_conv","title":"<code>handle_conv(primitive, inp, lora, *, dimension_numbers, **params)</code>","text":"<p>The handle_conv function is a helper function that allows us to use LoraWeight objects as inputs to convolutions.</p> <p>Parameters:</p> Name Type Description Default <code>primitive</code> <p>Identify the function that is being called</p> required <code>inp</code> <code>ArrayValue</code> <p>ArrayValue: Specify the input to the convolution</p> required <code>lora</code> <code>LoraWeight</code> <p>LoraWeight: Pass the loraweight object into the function</p> required <code>*</code> <p>Pass in the dimension_numbers parameter</p> required <code>dimension_numbers</code> <p>Specify the convolution</p> required <code>params</code> <p>Pass in the dimension_numbers parameter</p> <code>{}</code> <p>Returns:</p> Type Description <p>The result of the convolution</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>@primitive_handler(\"conv_general_dilated\")\ndef handle_conv(\n        primitive,\n        inp: ArrayValue,\n        lora: LoraWeight,\n        *,\n        dimension_numbers,\n        **params\n):\n    \"\"\"\n    The handle_conv function is a helper function that allows us to use LoraWeight objects as inputs to convolutions.\n\n    :param primitive: Identify the function that is being called\n    :param inp: ArrayValue: Specify the input to the convolution\n    :param lora: LoraWeight: Pass the loraweight object into the function\n    :param *: Pass in the dimension_numbers parameter\n    :param dimension_numbers: Specify the convolution\n    :param params: Pass in the dimension_numbers parameter\n    :return: The result of the convolution\n\n    \"\"\"\n    if isinstance(inp, LoraWeight):\n        warnings.warn(\"Using a LoraWeight as input to a convolution is not supported, so it will be materialized.\")\n        inp = inp.materialize()\n\n    if not dimension_numbers.rhs_spec[:1] != (\n            len(dimension_numbers.rhs_spec) - 1,\n            len(dimension_numbers.rhs_spec) - 2,\n    ):\n        raise ValueError(\"Lorax only supports convolutions with shape (..., in_features, out_features)\")\n\n    params = {**params, \"dimension_numbers\": dimension_numbers}\n    op = partial(jax.lax.conv_general_dilated, **params)\n    orig = op(inp, lora.w)\n\n    lora_product = op(inp, lora.b)\n\n    params[\"window_strides\"] = (1,) * (len(dimension_numbers.rhs_spec) - 2)\n    params[\"padding\"] = \"VALID\"\n    lora_product = jax.lax.conv_general_dilated(\n        lora_product,\n        lora.a * lora.get_scale(),\n        **params\n    )\n\n    return (orig + lora_product).astype(orig.dtype)\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.handle_dot_lhs","title":"<code>handle_dot_lhs(primitive, lora, rhs, *, dimension_numbers, **kwargs)</code>","text":"<p>The handle_dot_lhs function is a JAX primitive that allows us to perform matrix multiplication on LoraWeights. It does this by first performing the matrix multiplication on the underlying weight tensor, and then adding in a second term that accounts for the fact that we are multiplying two low-rank matrices together.</p> <p>Parameters:</p> Name Type Description Default <code>primitive</code> <p>Determine which function to use</p> required <code>lora</code> <code>LoraWeight</code> <p>LoraWeight: Pass the loraweight object to the function</p> required <code>rhs</code> <code>ArrayValue</code> <p>ArrayValue: Pass the right hand side of the dot product</p> required <code>dimension_numbers</code> <p>Determine which dimensions are being contracted</p> required <code>kwargs</code> <p>Pass the dimension_numbers to handle_dot_lhs</p> <code>{}</code> <p>Returns:</p> Type Description <p>The result of the dot product between</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>@primitive_handler(\"dot_general\")\ndef handle_dot_lhs(\n        primitive,\n        lora: LoraWeight,\n        rhs: ArrayValue,\n        *,\n        dimension_numbers,\n        **kwargs\n):\n    \"\"\"\n    The handle_dot_lhs function is a JAX primitive that allows us to perform\n    matrix multiplication on LoraWeights. It does this by first performing the matrix\n    multiplication on the underlying weight tensor, and then adding in a second term\n    that accounts for the fact that we are multiplying two low-rank matrices together.\n\n    :param primitive: Determine which function to use\n    :param lora: LoraWeight: Pass the loraweight object to the function\n    :param rhs: ArrayValue: Pass the right hand side of the dot product\n    :param dimension_numbers: Determine which dimensions are being contracted\n    :param kwargs: Pass the dimension_numbers to handle_dot_lhs\n    :return: The result of the dot product between\n\n    \"\"\"\n    if not _check_dot_dimension_numbers(dimension_numbers):\n        return NotImplemented\n\n    if isinstance(rhs, LoraWeight):\n        rhs = rhs.materialize()\n        warnings.warn(\"Encountered product of two LoraWeights. Materializing the rhs\")\n\n    op = partial(jax.lax.dot_general, **kwargs)\n\n    lhs_contract, = dimension_numbers[0][0]\n\n    first, second = (lora.a, lora.b) if lhs_contract == 1 else (lora.b, lora.a)\n\n    first *= lora.get_scale()\n\n    orig = op(lora.w, rhs, dimension_numbers=dimension_numbers)\n    lora_product = op(first, rhs, dimension_numbers=dimension_numbers)\n\n    second_dimension_numbers = ((lhs_contract,), (0,)), dimension_numbers[1]\n\n    lora_product = op(second, lora_product, dimension_numbers=second_dimension_numbers)\n\n    return (orig + lora_product).astype(orig.dtype)\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.handle_dot_rhs","title":"<code>handle_dot_rhs(primitive, lhs, lora, *, dimension_numbers, **kwargs)</code>","text":"<p>The handle_dot_rhs function is a partial application of the jax.lax.dot_general function, which takes in two arrays and returns their dot product (or matrix multiplication). The  handle_dot_rhs function is used to handle the case where a LoraWeight object appears on the right-hand side of an equation.  The handle_dot_rhs function takes in three arguments: primitive, lhs, and lora (the LoraWeight object). It then checks that  the dimension numbers are correct for this operation using _check_dimension numbers(). If they are not correct it will return NotIm</p> <p>Parameters:</p> Name Type Description Default <code>primitive</code> <p>Identify the function that is being called</p> required <code>lhs</code> <code>Array</code> <p>jax.Array: Store the left hand side of the dot product</p> required <code>lora</code> <code>LoraWeight</code> <p>LoraWeight: Pass the loraweight object to the function</p> required <code>dimension_numbers</code> <p>Specify the dimensions of the input arrays</p> required <code>kwargs</code> <p>Pass the dimension_numbers argument to handle_dot_rhs</p> <code>{}</code> <p>Returns:</p> Type Description <p>The output of the dot product with a loraweight</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>@primitive_handler(\"dot_general\")\ndef handle_dot_rhs(\n        primitive,\n        lhs: jax.Array,\n        lora: LoraWeight,\n        *,\n        dimension_numbers,\n        **kwargs\n):\n    \"\"\"\n    The handle_dot_rhs function is a partial application of the jax.lax.dot_general function,\n    which takes in two arrays and returns their dot product (or matrix multiplication). The \n    handle_dot_rhs function is used to handle the case where a LoraWeight object appears on the right-hand side of an equation. \n    The handle_dot_rhs function takes in three arguments: primitive, lhs, and lora (the LoraWeight object). It then checks that \n    the dimension numbers are correct for this operation using _check_dimension numbers(). If they are not correct it will return NotIm\n\n    :param primitive: Identify the function that is being called\n    :param lhs: jax.Array: Store the left hand side of the dot product\n    :param lora: LoraWeight: Pass the loraweight object to the function\n    :param dimension_numbers: Specify the dimensions of the input arrays\n    :param kwargs: Pass the dimension_numbers argument to handle_dot_rhs\n    :return: The output of the dot product with a loraweight\n\n    \"\"\"\n    if not _check_dot_dimension_numbers(dimension_numbers):\n        return NotImplemented\n    op = partial(jax.lax.dot_general, **kwargs)\n\n    rhs_contract, = dimension_numbers[0][1]\n    first, second = (lora.a, lora.b) if rhs_contract == 1 else (lora.b, lora.a)\n\n    first *= lora.get_scale()\n\n    orig = op(lhs, lora.w, dimension_numbers=dimension_numbers)\n    lora_product = op(lhs, first, dimension_numbers=dimension_numbers)\n\n    second_dimension_numbers = ((lhs.ndim - 1), (rhs_contract,)), dimension_numbers[1]\n\n    lora_product = op(lora_product, second, dimension_numbers=second_dimension_numbers)\n\n    return (orig + lora_product).astype(orig.dtype)\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.handle_gather","title":"<code>handle_gather(primitive, lora, indices, *, dimension_numbers, slice_sizes, **params)</code>","text":"<p>The handle_gather function is a JAX primitive handler that allows us to perform the gather operation on LoraWeight objects. This function is called by JAX when it encounters a gather operation in the computation graph. The function takes as input:</p> <p>Parameters:</p> Name Type Description Default <code>primitive</code> <p>Identify the operation</p> required <code>lora</code> <code>LoraWeight</code> <p>LoraWeight: Pass the loraweight object to the function</p> required <code>indices</code> <code>Array</code> <p>jax.Array: Select the rows of the weight matrix</p> required <code>dimension_numbers</code> <p>Specify the dimension numbers of</p> required <code>slice_sizes</code> <p>Specify the size of each slice</p> required <code>params</code> <p>Pass the dimension_numbers parameter to the gather function</p> <code>{}</code> <p>Returns:</p> Type Description <p>A new loraweight</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>@primitive_handler(\"gather\")\ndef handle_gather(\n        primitive,\n        lora: LoraWeight,\n        indices: jax.Array,\n        *,\n        dimension_numbers,\n        slice_sizes,\n        **params\n):\n    \"\"\"\n    The handle_gather function is a JAX primitive handler that allows us to\n    perform the gather operation on LoraWeight objects. This function is called by\n    JAX when it encounters a gather operation in the computation graph. The function\n    takes as input:\n\n    :param primitive: Identify the operation\n    :param lora: LoraWeight: Pass the loraweight object to the function\n    :param indices: jax.Array: Select the rows of the weight matrix\n    :param dimension_numbers: Specify the dimension numbers of\n    :param slice_sizes: Specify the size of each slice\n    :param params: Pass the dimension_numbers parameter to the gather function\n    :return: A new loraweight\n\n    \"\"\"\n    if dimension_numbers.offset_dims != (len(indices.shape) - 1,):\n        return NotImplemented\n\n    lora_dim = lora.b.shape[-1]\n\n    if slice_sizes != (1, lora.a.shape[1]):\n        return NotImplemented\n\n    params = {**params, \"dimension_numbers\": dimension_numbers}\n\n    orig = jax.lax.gather(lora.w, indices, slice_sizes=slice_sizes, **params)\n\n    new_slice_sizes = (1, lora_dim)\n\n    lora_product = jax.lax.gather(lora.b, indices, slice_sizes=new_slice_sizes, **params)\n    lora_product = lora_product @ (lora.a * lora.get_scale())\n\n    return (orig + lora_product).astype(orig.dtype)\n</code></pre>"},{"location":"generated-xrapture-xrapture/#src.fjformer.xrapture.xrapture.split_lora_params","title":"<code>split_lora_params(params, lora_spec)</code>","text":"<p>Map params to a pytree in which all <code>LoraWeight.w</code> values and all params marked with LORA_FREEZE are replaced with EmptyNode. This is useful for checkpointing just the trainable params.</p> Source code in <code>src/fjformer/xrapture/xrapture.py</code> <pre><code>def split_lora_params(params, lora_spec):\n    \"\"\"\n    Map params to a pytree in which all `LoraWeight.w` values and all params marked with\n    LORA_FREEZE are replaced with EmptyNode. This is useful for checkpointing just\n    the trainable params.\n    \"\"\"\n\n    def node_mapper(node, spec_val):\n        if not isinstance(node, LoraWeight):\n            return node if spec_val != LORA_FREEZE else EmptyNode\n        children, aux = node.tree_flatten_with_keys()\n        idx = next(i for i, (key, _) in enumerate(children) if key == \"w\")\n        children[idx] = (\"w\", EmptyNode)\n\n        return LoraWeight.tree_unflatten(aux, [c for _, c in children])\n\n    return tree_map_with_implicit(node_mapper, params, lora_spec)\n</code></pre>"}]}